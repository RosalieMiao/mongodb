2020-09-22T12:10:13.485+0800 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2020-09-22T12:10:13.497+0800 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] MongoDB starting : pid=3696 port=27019 dbpath=/var/lib/mongodb 64-bit host=iZbp157vvbma2xfbxku74xZ
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] db version v4.2.6
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] git version: 20364840b8f1af16917e4c23c1b5f5efd8b352f8
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] OpenSSL version: OpenSSL 1.1.0g  2 Nov 2017
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] allocator: tcmalloc
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] modules: none
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] build environment:
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten]     distmod: debian92
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten]     distarch: x86_64
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten]     target_arch: x86_64
2020-09-22T12:10:13.497+0800 I  CONTROL  [initandlisten] options: { config: "/etc/mongod.conf", net: { bindIp: "0.0.0.0" }, processManagement: { timeZoneInfo: "/usr/share/zoneinfo" }, replication: { replSetName: "rs_config" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/var/lib/mongodb", journal: { enabled: true } }, systemLog: { destination: "file", logAppend: true, path: "/var/log/mongodb/mongod.log" } }
2020-09-22T12:10:13.498+0800 I  STORAGE  [initandlisten] 
2020-09-22T12:10:13.498+0800 I  STORAGE  [initandlisten] ** WARNING: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine
2020-09-22T12:10:13.498+0800 I  STORAGE  [initandlisten] **          See http://dochub.mongodb.org/core/prodnotes-filesystem
2020-09-22T12:10:13.498+0800 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=487M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2020-09-22T12:10:14.175+0800 I  STORAGE  [initandlisten] WiredTiger message [1600747814:175489][3696:0x7f4668208b00], txn-recover: Set global recovery timestamp: (0, 0)
2020-09-22T12:10:14.189+0800 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2020-09-22T12:10:14.205+0800 I  STORAGE  [initandlisten] Timestamp monitor starting
2020-09-22T12:10:14.213+0800 I  CONTROL  [initandlisten] 
2020-09-22T12:10:14.213+0800 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2020-09-22T12:10:14.213+0800 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2020-09-22T12:10:14.213+0800 I  CONTROL  [initandlisten] 
2020-09-22T12:10:14.214+0800 I  SHARDING [initandlisten] Marking collection local.system.replset as collection version: <unsharded>
2020-09-22T12:10:14.214+0800 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2020-09-22T12:10:14.214+0800 I  SHARDING [initandlisten] Marking collection admin.system.roles as collection version: <unsharded>
2020-09-22T12:10:14.215+0800 I  SHARDING [initandlisten] Marking collection admin.system.version as collection version: <unsharded>
2020-09-22T12:10:14.216+0800 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: e6ebf755-d943-4a14-b38b-aa0a71564608 and options: { capped: true, size: 10485760 }
2020-09-22T12:10:14.230+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2020-09-22T12:10:14.230+0800 I  SHARDING [initandlisten] Marking collection local.startup_log as collection version: <unsharded>
2020-09-22T12:10:14.230+0800 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/var/lib/mongodb/diagnostic.data'
2020-09-22T12:10:14.235+0800 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: ReadConcernMajorityNotAvailableYet: could not get updated shard list from config server :: caused by :: Read concern majority reads are currently not possible.; will retry after 30s
2020-09-22T12:10:14.235+0800 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 35d6d6bb-e300-4820-bb00-f25ac5b9b7a4 and options: {}
2020-09-22T12:10:14.236+0800 I  SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2020-09-22T12:10:14.247+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2020-09-22T12:10:14.247+0800 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: be0c223b-8267-477c-8dbd-9ba56acb0d49 and options: {}
2020-09-22T12:10:14.261+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2020-09-22T12:10:14.261+0800 I  SHARDING [initandlisten] Marking collection local.replset.minvalid as collection version: <unsharded>
2020-09-22T12:10:14.261+0800 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 3036a967-c07a-46a3-8dc5-f22b31f8e552 and options: {}
2020-09-22T12:10:14.278+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2020-09-22T12:10:14.278+0800 I  SHARDING [initandlisten] Marking collection local.replset.election as collection version: <unsharded>
2020-09-22T12:10:14.279+0800 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2020-09-22T12:10:14.279+0800 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2020-09-22T12:10:14.279+0800 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 13240e05-fd40-4f1f-bfd3-fda2f91b2bf5 and options: {}
2020-09-22T12:10:14.292+0800 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2020-09-22T12:10:14.292+0800 I  SHARDING [initandlisten] Marking collection local.system.rollback.id as collection version: <unsharded>
2020-09-22T12:10:14.292+0800 I  REPL     [initandlisten] Initialized the rollback ID to 1
2020-09-22T12:10:14.292+0800 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2020-09-22T12:10:14.294+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database config from version {} to version { uuid: UUID("d8c5dec3-aa68-46bb-88b4-2b53f77e66d9"), lastMod: 0 } took 0 ms
2020-09-22T12:10:14.294+0800 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2020-09-22T12:10:14.294+0800 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2020-09-22T12:10:14.294+0800 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Cannot use non-local read concern until replica set is finished initializing.
2020-09-22T12:10:14.294+0800 I  NETWORK  [listener] Listening on /tmp/mongodb-27019.sock
2020-09-22T12:10:14.294+0800 I  NETWORK  [listener] Listening on 0.0.0.0
2020-09-22T12:10:14.294+0800 I  NETWORK  [listener] waiting for connections on port 27019
2020-09-22T12:10:15.000+0800 I  SHARDING [ftdc] Marking collection local.oplog.rs as collection version: <unsharded>
2020-09-22T12:10:15.603+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11612 #1 (1 connection now open)
2020-09-22T12:10:15.605+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22054 #2 (2 connections now open)
2020-09-22T12:10:15.622+0800 I  NETWORK  [conn2] received client metadata from 211.162.81.126:22054 conn2: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:15.623+0800 I  NETWORK  [conn1] received client metadata from 211.162.81.126:11612 conn1: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:15.697+0800 I  NETWORK  [conn1] end connection 211.162.81.126:11612 (1 connection now open)
2020-09-22T12:10:15.697+0800 I  NETWORK  [conn2] end connection 211.162.81.126:22054 (0 connections now open)
2020-09-22T12:10:16.361+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22056 #3 (1 connection now open)
2020-09-22T12:10:16.361+0800 I  NETWORK  [conn3] received client metadata from 211.162.81.126:22056 conn3: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:16.362+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22055 #4 (2 connections now open)
2020-09-22T12:10:16.363+0800 I  NETWORK  [conn4] received client metadata from 211.162.81.126:22055 conn4: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:16.402+0800 I  REPL     [conn4] replSetInitiate admin command received from client
2020-09-22T12:10:16.403+0800 I  NETWORK  [listener] connection accepted from 120.55.194.98:50264 #5 (3 connections now open)
2020-09-22T12:10:16.403+0800 I  NETWORK  [conn5] end connection 120.55.194.98:50264 (2 connections now open)
2020-09-22T12:10:16.405+0800 I  REPL     [conn4] replSetInitiate config object with 3 members parses ok
2020-09-22T12:10:16.406+0800 I  REPL     [conn4] Scheduling remote command request for initiate quorum check: RemoteCommand 1 -- target:120.55.192.104:27019 db:admin cmd:{ replSetHeartbeat: "rs_config", checkEmpty: true, configVersion: 1, hbv: 1, from: "120.55.194.98:27019", fromId: 0, term: 0 }
2020-09-22T12:10:16.406+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:10:16.406+0800 I  REPL     [conn4] Scheduling remote command request for initiate quorum check: RemoteCommand 2 -- target:112.124.21.191:27019 db:admin cmd:{ replSetHeartbeat: "rs_config", checkEmpty: true, configVersion: 1, hbv: 1, from: "120.55.194.98:27019", fromId: 0, term: 0 }
2020-09-22T12:10:16.406+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:10:16.408+0800 I  REPL     [conn4] ******
2020-09-22T12:10:16.408+0800 I  REPL     [conn4] creating replication oplog of size: 1728MB...
2020-09-22T12:10:16.408+0800 I  STORAGE  [conn4] createCollection: local.oplog.rs with generated UUID: 8dcad042-3b68-4716-a92a-ff3ffb16b669 and options: { capped: true, size: 1812645068.0, autoIndexId: false }
2020-09-22T12:10:16.408+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54220 #11 (3 connections now open)
2020-09-22T12:10:16.409+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47628 #12 (4 connections now open)
2020-09-22T12:10:16.410+0800 I  NETWORK  [conn12] received client metadata from 120.55.192.104:47628 conn12: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:16.410+0800 I  NETWORK  [conn11] received client metadata from 112.124.21.191:54220 conn11: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:16.415+0800 I  STORAGE  [conn4] Starting OplogTruncaterThread local.oplog.rs
2020-09-22T12:10:16.415+0800 I  STORAGE  [conn4] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2020-09-22T12:10:16.415+0800 I  STORAGE  [conn4] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:10:16.415+0800 I  STORAGE  [conn4] WiredTiger record store oplog processing took 0ms
2020-09-22T12:10:16.447+0800 I  REPL     [conn4] ******
2020-09-22T12:10:16.447+0800 I  STORAGE  [conn4] createCollection: local.system.replset with generated UUID: 4f2ec344-4a03-487d-919c-84631ccf2b25 and options: {}
2020-09-22T12:10:16.464+0800 I  INDEX    [conn4] index build: done building index _id_ on ns local.system.replset
2020-09-22T12:10:16.466+0800 I  SHARDING [conn4] Marking collection local.replset.oplogTruncateAfterPoint as collection version: <unsharded>
2020-09-22T12:10:16.467+0800 I  STORAGE  [conn4] createCollection: admin.system.version with provided UUID: 7e4950ec-424a-4bdd-ab0e-79368851545a and options: { uuid: UUID("7e4950ec-424a-4bdd-ab0e-79368851545a") }
2020-09-22T12:10:16.484+0800 I  INDEX    [conn4] index build: done building index _id_ on ns admin.system.version
2020-09-22T12:10:16.485+0800 I  COMMAND  [conn4] setting featureCompatibilityVersion to 4.2
2020-09-22T12:10:16.485+0800 I  NETWORK  [conn4] Skip closing connection for connection # 12
2020-09-22T12:10:16.485+0800 I  NETWORK  [conn4] Skip closing connection for connection # 11
2020-09-22T12:10:16.485+0800 I  NETWORK  [conn4] Skip closing connection for connection # 4
2020-09-22T12:10:16.485+0800 I  NETWORK  [conn4] Skip closing connection for connection # 3
2020-09-22T12:10:16.485+0800 I  REPL     [conn4] New replica set config in use: { _id: "rs_config", version: 1, configsvr: true, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "120.55.194.98:27019", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 3.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "120.55.192.104:27019", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 2.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "112.124.21.191:27019", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 1, electionTimeoutMillis: 1000, catchUpTimeoutMillis: 1000, catchUpTakeoverDelayMillis: 3000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5f697928513fac20f0ca1972') } }
2020-09-22T12:10:16.485+0800 I  REPL     [conn4] This node is 120.55.194.98:27019 in the config
2020-09-22T12:10:16.485+0800 I  REPL     [conn4] transition to STARTUP2 from STARTUP
2020-09-22T12:10:16.485+0800 I  REPL     [conn4] Starting replication storage threads
2020-09-22T12:10:16.486+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state STARTUP
2020-09-22T12:10:16.486+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state STARTUP
2020-09-22T12:10:16.488+0800 I  REPL     [conn4] transition to RECOVERING from STARTUP2
2020-09-22T12:10:16.488+0800 I  REPL     [conn4] Starting replication fetcher thread
2020-09-22T12:10:16.489+0800 I  REPL     [conn4] Starting replication applier thread
2020-09-22T12:10:16.489+0800 I  REPL     [conn4] Starting replication reporter thread
2020-09-22T12:10:16.489+0800 I  REPL     [rsSync-0] Starting oplog application
2020-09-22T12:10:16.489+0800 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2020-09-22T12:10:16.489+0800 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2020-09-22T12:10:16.489+0800 I  REPL     [rsBackgroundSync] waiting for 2 pings from other members before syncing
2020-09-22T12:10:16.911+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47630 #13 (5 connections now open)
2020-09-22T12:10:16.912+0800 I  NETWORK  [conn13] end connection 120.55.192.104:47630 (4 connections now open)
2020-09-22T12:10:16.912+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54222 #14 (5 connections now open)
2020-09-22T12:10:16.913+0800 I  NETWORK  [conn14] end connection 112.124.21.191:54222 (4 connections now open)
2020-09-22T12:10:16.986+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state STARTUP2
2020-09-22T12:10:16.986+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state STARTUP2
2020-09-22T12:10:17.011+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47638 #15 (5 connections now open)
2020-09-22T12:10:17.011+0800 I  NETWORK  [conn15] received client metadata from 120.55.192.104:47638 conn15: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:17.012+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54230 #16 (6 connections now open)
2020-09-22T12:10:17.013+0800 I  NETWORK  [conn16] received client metadata from 112.124.21.191:54230 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:17.013+0800 I  SHARDING [conn15] Marking collection config.transactions as collection version: <unsharded>
2020-09-22T12:10:17.015+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47640 #17 (7 connections now open)
2020-09-22T12:10:17.016+0800 I  NETWORK  [conn17] received client metadata from 120.55.192.104:47640 conn17: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:17.016+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54232 #18 (8 connections now open)
2020-09-22T12:10:17.017+0800 I  NETWORK  [conn18] received client metadata from 112.124.21.191:54232 conn18: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:17.031+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47642 #19 (9 connections now open)
2020-09-22T12:10:17.031+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54234 #20 (10 connections now open)
2020-09-22T12:10:17.032+0800 I  NETWORK  [conn20] received client metadata from 112.124.21.191:54234 conn20: { driver: { name: "MongoDB Internal Client", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:17.032+0800 I  NETWORK  [conn19] received client metadata from 120.55.192.104:47642 conn19: { driver: { name: "MongoDB Internal Client", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:17.033+0800 I  NETWORK  [conn20] end connection 112.124.21.191:54234 (9 connections now open)
2020-09-22T12:10:17.033+0800 I  NETWORK  [conn19] end connection 120.55.192.104:47642 (8 connections now open)
2020-09-22T12:10:17.486+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:10:17.486+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:10:17.517+0800 I  NETWORK  [conn17] end connection 120.55.192.104:47640 (7 connections now open)
2020-09-22T12:10:17.517+0800 I  NETWORK  [conn18] end connection 112.124.21.191:54232 (6 connections now open)
2020-09-22T12:10:17.534+0800 I  ELECTION [replexec-1] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-09-22T12:10:17.534+0800 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 0
2020-09-22T12:10:17.534+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 9 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 0, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747816, 1), t: -1 } }
2020-09-22T12:10:17.534+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 10 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 0, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747816, 1), t: -1 } }
2020-09-22T12:10:17.535+0800 I  ELECTION [replexec-0] VoteRequester(term 0 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 0, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('000000000000000000000000') }, lastCommittedOpTime: Timestamp(0, 0), $clusterTime: { clusterTime: Timestamp(1600747816, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747816, 1) }
2020-09-22T12:10:17.535+0800 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
2020-09-22T12:10:17.536+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 11 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 1, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747816, 1), t: -1 } }
2020-09-22T12:10:17.536+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 12 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 1, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747816, 1), t: -1 } }
2020-09-22T12:10:17.537+0800 I  ELECTION [replexec-1] VoteRequester(term 1) received a yes vote from 120.55.192.104:27019; response message: { term: 1, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('000000000000000000000000') }, lastCommittedOpTime: Timestamp(0, 0), $clusterTime: { clusterTime: Timestamp(1600747816, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747816, 1) }
2020-09-22T12:10:17.537+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 1
2020-09-22T12:10:17.537+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2020-09-22T12:10:17.537+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2020-09-22T12:10:17.538+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2020-09-22T12:10:17.538+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:10:17.538+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:10:17.539+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747816, 1), t: -1 }. My Last Applied: { ts: Timestamp(1600747816, 1), t: -1 }
2020-09-22T12:10:17.539+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-09-22T12:10:17.539+0800 I  REPL     [replexec-0] Stopping replication producer
2020-09-22T12:10:17.539+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 1
2020-09-22T12:10:17.540+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 1
2020-09-22T12:10:17.540+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:10:17.540+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:10:17.540+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:10:17.540+0800 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: 4053617d-d9e7-479b-91cd-0abf68267354 and options: {}
2020-09-22T12:10:17.553+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
2020-09-22T12:10:17.554+0800 I  STORAGE  [rsSync-0] createCollection: config.chunks with provided UUID: a101121e-0946-4ee4-9c3c-7bb6281db167 and options: { uuid: UUID("a101121e-0946-4ee4-9c3c-7bb6281db167") }
2020-09-22T12:10:17.563+0800 I  NETWORK  [conn4] end connection 211.162.81.126:22055 (5 connections now open)
2020-09-22T12:10:17.563+0800 I  NETWORK  [conn3] end connection 211.162.81.126:22056 (4 connections now open)
2020-09-22T12:10:17.565+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.chunks
2020-09-22T12:10:17.565+0800 I  SHARDING [rsSync-0] Marking collection config.chunks as collection version: <unsharded>
2020-09-22T12:10:17.582+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11613 #22 (5 connections now open)
2020-09-22T12:10:17.582+0800 I  NETWORK  [conn22] received client metadata from 211.162.81.126:11613 conn22: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:17.583+0800 I  INDEX    [rsSync-0] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.chunks" } using method: Hybrid
2020-09-22T12:10:17.583+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.583+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.584+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.586+0800 I  INDEX    [rsSync-0] index build: done building index ns_1_min_1 on ns config.chunks
2020-09-22T12:10:17.590+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22057 #23 (6 connections now open)
2020-09-22T12:10:17.591+0800 I  NETWORK  [conn23] received client metadata from 211.162.81.126:22057 conn23: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:17.606+0800 I  INDEX    [rsSync-0] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, shard: 1, min: 1 }, name: "ns_1_shard_1_min_1", ns: "config.chunks" } using method: Hybrid
2020-09-22T12:10:17.606+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.606+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.606+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.609+0800 I  INDEX    [rsSync-0] index build: done building index ns_1_shard_1_min_1 on ns config.chunks
2020-09-22T12:10:17.631+0800 I  INDEX    [rsSync-0] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, lastmod: 1 }, name: "ns_1_lastmod_1", ns: "config.chunks" } using method: Hybrid
2020-09-22T12:10:17.631+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.631+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.632+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.634+0800 I  INDEX    [rsSync-0] index build: done building index ns_1_lastmod_1 on ns config.chunks
2020-09-22T12:10:17.638+0800 I  STORAGE  [rsSync-0] createCollection: config.migrations with provided UUID: 2a18503c-6359-4f56-9f16-29eed137e385 and options: { uuid: UUID("2a18503c-6359-4f56-9f16-29eed137e385") }
2020-09-22T12:10:17.650+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.migrations
2020-09-22T12:10:17.650+0800 I  SHARDING [rsSync-0] Marking collection config.migrations as collection version: <unsharded>
2020-09-22T12:10:17.667+0800 I  INDEX    [rsSync-0] index build: starting on config.migrations properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.migrations" } using method: Hybrid
2020-09-22T12:10:17.667+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.667+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.667+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.670+0800 I  INDEX    [rsSync-0] index build: done building index ns_1_min_1 on ns config.migrations
2020-09-22T12:10:17.673+0800 I  STORAGE  [rsSync-0] createCollection: config.shards with provided UUID: 551d8645-5410-46d3-ac48-3f86cc78c694 and options: { uuid: UUID("551d8645-5410-46d3-ac48-3f86cc78c694") }
2020-09-22T12:10:17.686+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.shards
2020-09-22T12:10:17.687+0800 I  SHARDING [rsSync-0] Marking collection config.shards as collection version: <unsharded>
2020-09-22T12:10:17.706+0800 I  INDEX    [rsSync-0] index build: starting on config.shards properties: { v: 2, unique: true, key: { host: 1 }, name: "host_1", ns: "config.shards" } using method: Hybrid
2020-09-22T12:10:17.706+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.706+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.707+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.712+0800 I  INDEX    [rsSync-0] index build: done building index host_1 on ns config.shards
2020-09-22T12:10:17.719+0800 I  STORAGE  [rsSync-0] createCollection: config.locks with provided UUID: 75fb8f2f-a2b3-49f4-bb93-400680f69951 and options: { uuid: UUID("75fb8f2f-a2b3-49f4-bb93-400680f69951") }
2020-09-22T12:10:17.730+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.locks
2020-09-22T12:10:17.745+0800 I  INDEX    [rsSync-0] index build: starting on config.locks properties: { v: 2, key: { ts: 1 }, name: "ts_1", ns: "config.locks" } using method: Hybrid
2020-09-22T12:10:17.745+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.746+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.746+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.748+0800 I  INDEX    [rsSync-0] index build: done building index ts_1 on ns config.locks
2020-09-22T12:10:17.763+0800 I  INDEX    [rsSync-0] index build: starting on config.locks properties: { v: 2, key: { state: 1, process: 1 }, name: "state_1_process_1", ns: "config.locks" } using method: Hybrid
2020-09-22T12:10:17.763+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.764+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.764+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.767+0800 I  INDEX    [rsSync-0] index build: done building index state_1_process_1 on ns config.locks
2020-09-22T12:10:17.770+0800 I  STORAGE  [rsSync-0] createCollection: config.lockpings with provided UUID: 990ea228-4f92-4ac6-83ef-83a3dd549ffd and options: { uuid: UUID("990ea228-4f92-4ac6-83ef-83a3dd549ffd") }
2020-09-22T12:10:17.782+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.lockpings
2020-09-22T12:10:17.801+0800 I  INDEX    [rsSync-0] index build: starting on config.lockpings properties: { v: 2, key: { ping: 1 }, name: "ping_1", ns: "config.lockpings" } using method: Hybrid
2020-09-22T12:10:17.801+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.802+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.802+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.805+0800 I  INDEX    [rsSync-0] index build: done building index ping_1 on ns config.lockpings
2020-09-22T12:10:17.808+0800 I  STORAGE  [rsSync-0] createCollection: config.tags with provided UUID: 90449d01-a92c-4bc8-b6f6-bf9774d459a2 and options: { uuid: UUID("90449d01-a92c-4bc8-b6f6-bf9774d459a2") }
2020-09-22T12:10:17.810+0800 I  NETWORK  [conn23] end connection 211.162.81.126:22057 (5 connections now open)
2020-09-22T12:10:17.811+0800 I  NETWORK  [conn22] end connection 211.162.81.126:11613 (4 connections now open)
2020-09-22T12:10:17.820+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.tags
2020-09-22T12:10:17.820+0800 I  SHARDING [rsSync-0] Marking collection config.tags as collection version: <unsharded>
2020-09-22T12:10:17.847+0800 I  INDEX    [rsSync-0] index build: starting on config.tags properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.tags" } using method: Hybrid
2020-09-22T12:10:17.847+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.847+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.848+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.850+0800 I  INDEX    [rsSync-0] index build: done building index ns_1_min_1 on ns config.tags
2020-09-22T12:10:17.869+0800 I  INDEX    [rsSync-0] index build: starting on config.tags properties: { v: 2, key: { ns: 1, tag: 1 }, name: "ns_1_tag_1", ns: "config.tags" } using method: Hybrid
2020-09-22T12:10:17.869+0800 I  INDEX    [rsSync-0] build may temporarily use up to 200 megabytes of RAM
2020-09-22T12:10:17.870+0800 I  INDEX    [rsSync-0] index build: collection scan done. scanned 0 total records in 0 seconds
2020-09-22T12:10:17.870+0800 I  INDEX    [rsSync-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2020-09-22T12:10:17.873+0800 I  INDEX    [rsSync-0] index build: done building index ns_1_tag_1 on ns config.tags
2020-09-22T12:10:17.874+0800 I  SHARDING [rsSync-0] Marking collection config.version as collection version: <unsharded>
2020-09-22T12:10:17.874+0800 I  STORAGE  [rsSync-0] createCollection: config.version with generated UUID: 040e4fca-70a7-4d42-aac8-66895ee93871 and options: {}
2020-09-22T12:10:17.893+0800 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.version
2020-09-22T12:10:17.893+0800 I  SHARDING [rsSync-0] Marking collection config.locks as collection version: <unsharded>
2020-09-22T12:10:17.894+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:10:17.895+0800 I  SHARDING [PeriodicShardedIndexConsistencyChecker] Checking consistency of sharded collection indexes across the cluster
2020-09-22T12:10:17.895+0800 I  SHARDING [PeriodicShardedIndexConsistencyChecker] Marking collection config.collections as collection version: <unsharded>
2020-09-22T12:10:17.895+0800 I  SHARDING [PeriodicShardedIndexConsistencyChecker] Found 0 collections with inconsistent indexes
2020-09-22T12:10:17.895+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:10:18.564+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36846 #24 (5 connections now open)
2020-09-22T12:10:18.564+0800 I  NETWORK  [conn24] received client metadata from 47.96.5.198:36846 conn24: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.573+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36850 #25 (6 connections now open)
2020-09-22T12:10:18.573+0800 I  NETWORK  [conn25] received client metadata from 47.96.5.198:36850 conn25: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.573+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36852 #26 (7 connections now open)
2020-09-22T12:10:18.574+0800 I  NETWORK  [conn26] received client metadata from 47.96.5.198:36852 conn26: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.576+0800 I  SHARDING [conn25] Marking collection config.lockpings as collection version: <unsharded>
2020-09-22T12:10:18.579+0800 I  NETWORK  [listener] connection accepted from 47.96.16.32:54574 #27 (8 connections now open)
2020-09-22T12:10:18.579+0800 I  NETWORK  [conn27] received client metadata from 47.96.16.32:54574 conn27: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.582+0800 I  NETWORK  [listener] connection accepted from 120.55.194.98:50280 #28 (9 connections now open)
2020-09-22T12:10:18.582+0800 I  NETWORK  [conn28] received client metadata from 120.55.194.98:50280 conn28: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.585+0800 I  NETWORK  [listener] connection accepted from 120.55.194.98:50282 #29 (10 connections now open)
2020-09-22T12:10:18.585+0800 I  NETWORK  [listener] connection accepted from 120.55.194.98:50284 #30 (11 connections now open)
2020-09-22T12:10:18.585+0800 I  NETWORK  [conn29] received client metadata from 120.55.194.98:50282 conn29: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.586+0800 I  NETWORK  [conn30] received client metadata from 120.55.194.98:50284 conn30: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.588+0800 I  NETWORK  [listener] connection accepted from 47.96.16.32:54578 #31 (12 connections now open)
2020-09-22T12:10:18.588+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51278 #32 (13 connections now open)
2020-09-22T12:10:18.588+0800 I  NETWORK  [conn32] received client metadata from 118.31.43.238:51278 conn32: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.589+0800 I  NETWORK  [conn31] received client metadata from 47.96.16.32:54578 conn31: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.595+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51284 #33 (14 connections now open)
2020-09-22T12:10:18.595+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51282 #34 (15 connections now open)
2020-09-22T12:10:18.595+0800 I  NETWORK  [conn34] received client metadata from 118.31.43.238:51282 conn34: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.595+0800 I  NETWORK  [conn33] received client metadata from 118.31.43.238:51284 conn33: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.599+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51286 #35 (16 connections now open)
2020-09-22T12:10:18.600+0800 I  NETWORK  [conn35] received client metadata from 118.31.43.238:51286 conn35: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.600+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51288 #36 (17 connections now open)
2020-09-22T12:10:18.600+0800 I  NETWORK  [conn36] received client metadata from 118.31.43.238:51288 conn36: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.737+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54240 #37 (18 connections now open)
2020-09-22T12:10:18.737+0800 I  NETWORK  [conn37] received client metadata from 112.124.21.191:54240 conn37: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.739+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54242 #38 (19 connections now open)
2020-09-22T12:10:18.739+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54248 #39 (20 connections now open)
2020-09-22T12:10:18.740+0800 I  NETWORK  [conn39] received client metadata from 112.124.21.191:54248 conn39: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:18.740+0800 I  NETWORK  [conn38] received client metadata from 112.124.21.191:54242 conn38: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:19.044+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54250 #40 (21 connections now open)
2020-09-22T12:10:19.045+0800 I  NETWORK  [conn40] received client metadata from 112.124.21.191:54250 conn40: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:19.049+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47644 #41 (22 connections now open)
2020-09-22T12:10:19.049+0800 I  NETWORK  [conn41] received client metadata from 120.55.192.104:47644 conn41: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:19.059+0800 I  STORAGE  [conn16] Triggering the first stable checkpoint. Initial Data: Timestamp(1600747816, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1600747817, 3)
2020-09-22T12:10:19.060+0800 I  SHARDING [conn36] Marking collection admin.system.keys as collection version: <unsharded>
2020-09-22T12:10:19.060+0800 I  COMMAND  [conn36] command admin.system.keys command: find { find: "system.keys", filter: { purpose: "HMAC", expiresAt: { $gt: Timestamp(0, 0) } }, sort: { expiresAt: 1 }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "admin" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:553 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 457ms
2020-09-22T12:10:19.222+0800 I  COMMAND  [conn34] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: COLLSCAN keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:1 nreturned:0 reslen:549 locks:{ ReplicationStateTransition: { acquireCount: { w: 2 } }, Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 2 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 2 } } } storage:{} protocol:op_msg 624ms
2020-09-22T12:10:19.376+0800 I  COMMAND  [conn39] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: COLLSCAN keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:1 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 2 } }, Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 2 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 2 } } } storage:{} protocol:op_msg 635ms
2020-09-22T12:10:19.376+0800 I  COMMAND  [conn26] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: COLLSCAN keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:1 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 2 } }, Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 2 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 2 } } } storage:{} protocol:op_msg 799ms
2020-09-22T12:10:19.376+0800 I  COMMAND  [conn33] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: COLLSCAN keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:1 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 2 } }, Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 2 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 2 } } } storage:{} protocol:op_msg 778ms
2020-09-22T12:10:19.376+0800 I  COMMAND  [conn29] command config.version command: find { find: "version", readConcern: { level: "majority", afterOpTime: { ts: Timestamp(0, 0), t: -1 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: COLLSCAN keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:1 nreturned:0 reslen:550 locks:{ ReplicationStateTransition: { acquireCount: { w: 2 } }, Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 2 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 2 } } } storage:{} protocol:op_msg 790ms
2020-09-22T12:10:19.378+0800 I  SHARDING [Balancer] Marking collection config.settings as collection version: <unsharded>
2020-09-22T12:10:19.378+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:10:19.378+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:10:19.379+0800 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 394a26b5-28ef-428f-a02a-72801fb5a7a6 and options: {}
2020-09-22T12:10:19.380+0800 I  SHARDING [TransactionCoordinator] Marking collection config.transaction_coordinators as collection version: <unsharded>
2020-09-22T12:10:19.380+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:10:19.380+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:10:19.380+0800 I  COMMAND  [conn25] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.02:27017:1600747818:7156575011715358840" }, update: { $set: { ping: new Date(1600747818561) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: IDHACK keysExamined:0 docsExamined:0 nMatched:0 nModified:0 upsert:1 keysInserted:2 numYields:0 reslen:631 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 804ms
2020-09-22T12:10:19.380+0800 I  COMMAND  [conn30] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "iZbp157vvbma2xfbxku74xZ:27017:1600747818:7332321692021962316" }, update: { $set: { ping: new Date(1600747818580) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: IDHACK keysExamined:0 docsExamined:0 nMatched:0 nModified:0 upsert:1 keysInserted:2 numYields:0 reslen:634 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 794ms
2020-09-22T12:10:19.380+0800 I  COMMAND  [conn31] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.00:27017:1600747818:2541039578456057690" }, update: { $set: { ping: new Date(1600747818576) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: IDHACK keysExamined:0 docsExamined:0 nMatched:0 nModified:0 upsert:1 keysInserted:2 numYields:0 reslen:631 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 789ms
2020-09-22T12:10:19.381+0800 I  COMMAND  [conn35] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.01:27017:1600747818:3466407884530610701" }, update: { $set: { ping: new Date(1600747818585) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: IDHACK keysExamined:0 docsExamined:0 nMatched:0 nModified:0 upsert:1 keysInserted:2 numYields:0 reslen:631 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 778ms
2020-09-22T12:10:19.381+0800 I  COMMAND  [conn38] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Jepsen-Node-01:27017:1600747818:8589022375712416522" }, update: { $set: { ping: new Date(1600747818736) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(0, 0), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $configServerState: { opTime: { ts: Timestamp(0, 0), t: -1 } }, $db: "config" } planSummary: IDHACK keysExamined:0 docsExamined:0 nMatched:0 nModified:0 upsert:1 keysInserted:2 numYields:0 reslen:625 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 640ms
2020-09-22T12:10:19.397+0800 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
2020-09-22T12:10:19.846+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47648 #42 (23 connections now open)
2020-09-22T12:10:19.846+0800 I  NETWORK  [conn42] received client metadata from 120.55.192.104:47648 conn42: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:19.848+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47654 #43 (24 connections now open)
2020-09-22T12:10:19.848+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47656 #44 (25 connections now open)
2020-09-22T12:10:19.848+0800 I  NETWORK  [conn43] received client metadata from 120.55.192.104:47654 conn43: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:19.848+0800 I  NETWORK  [conn44] received client metadata from 120.55.192.104:47656 conn44: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:20.396+0800 I  SHARDING [conn33] Marking collection config.mongos as collection version: <unsharded>
2020-09-22T12:10:20.396+0800 I  STORAGE  [conn33] createCollection: config.mongos with generated UUID: 282775af-a43f-4c57-858a-851b59ec70fc and options: {}
2020-09-22T12:10:20.410+0800 I  INDEX    [conn33] index build: done building index _id_ on ns config.mongos
2020-09-22T12:10:21.411+0800 I  NETWORK  [listener] connection accepted from 47.96.16.32:54584 #45 (26 connections now open)
2020-09-22T12:10:21.411+0800 I  NETWORK  [conn45] received client metadata from 47.96.16.32:54584 conn45: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:22.987+0800 I  NETWORK  [conn29] Starting new replica set monitor for rs_shard1/47.96.5.198:27018
2020-09-22T12:10:22.987+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to 47.96.5.198:27018
2020-09-22T12:10:22.994+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:10:22.994+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to 118.31.43.238:27018
2020-09-22T12:10:22.994+0800 I  CONNPOOL [ReplicaSetMonitor-TaskExecutor] Connecting to 47.96.16.32:27018
2020-09-22T12:10:23.012+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36858 #50 (27 connections now open)
2020-09-22T12:10:23.013+0800 I  NETWORK  [conn50] received client metadata from 47.96.5.198:36858 conn50: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.014+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51298 #51 (28 connections now open)
2020-09-22T12:10:23.015+0800 I  NETWORK  [listener] connection accepted from 47.96.16.32:54586 #52 (29 connections now open)
2020-09-22T12:10:23.015+0800 I  NETWORK  [conn52] received client metadata from 47.96.16.32:54586 conn52: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.015+0800 I  NETWORK  [conn51] received client metadata from 118.31.43.238:51298 conn51: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.022+0800 I  NETWORK  [listener] connection accepted from 47.96.16.32:54596 #53 (30 connections now open)
2020-09-22T12:10:23.022+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51304 #54 (31 connections now open)
2020-09-22T12:10:23.022+0800 I  NETWORK  [conn53] received client metadata from 47.96.16.32:54596 conn53: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.022+0800 I  NETWORK  [conn54] received client metadata from 118.31.43.238:51304 conn54: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.035+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36864 #55 (32 connections now open)
2020-09-22T12:10:23.035+0800 I  NETWORK  [conn55] received client metadata from 47.96.5.198:36864 conn55: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.041+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36874 #56 (33 connections now open)
2020-09-22T12:10:23.042+0800 I  NETWORK  [conn56] received client metadata from 47.96.5.198:36874 conn56: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:23.074+0800 I  SHARDING [conn29] going to insert new entry for shard into config.shards: { _id: "rs_shard1", host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018", state: 1 }
2020-09-22T12:10:23.076+0800 I  STORAGE  [conn29] createCollection: config.changelog with generated UUID: 4eb1c795-cfa4-4b19-801d-0eab6874e47f and options: { capped: true, size: 209715200 }
2020-09-22T12:10:23.089+0800 I  INDEX    [conn29] index build: done building index _id_ on ns config.changelog
2020-09-22T12:10:23.107+0800 I  SHARDING [conn29] about to log metadata event into changelog: { _id: "iZbp157vvbma2xfbxku74xZ:27019-2020-09-22T12:10:23.106+0800-5f69792f513fac20f0ca1b43", server: "iZbp157vvbma2xfbxku74xZ:27019", shard: "config", clientAddr: "120.55.194.98:50282", time: new Date(1600747823106), what: "addShard", ns: "", details: { name: "rs_shard1", host: "rs_shard1/47.96.5.198:27018" } }
2020-09-22T12:10:23.107+0800 I  SHARDING [conn29] Marking collection config.changelog as collection version: <unsharded>
2020-09-22T12:10:23.111+0800 I  COMMAND  [conn29] command admin.$cmd command: _configsvrAddShard { _configsvrAddShard: "rs_shard1/47.96.5.198:27018", writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("199c0208-7251-4b21-b67b-cd1ddc01449a"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747821, 5), signature: { hash: BinData(0, 93182DD852888B086B96B1A3993347E20B2273A0), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "iZbp157vvbma2xfbxku74xZ:27017", client: "211.162.81.126:19065", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747821, 5), t: 1 } }, $db: "admin" } numYields:0 reslen:531 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 3 } }, ReplicationStateTransition: { acquireCount: { w: 6 } }, Global: { acquireCount: { r: 3, w: 3 } }, Database: { acquireCount: { r: 3, w: 3 } }, Collection: { acquireCount: { r: 4, w: 2, W: 1 } }, Metadata: { acquireCount: { W: 1 } }, Mutex: { acquireCount: { r: 10, W: 1 } } } flowControl:{ acquireCount: 3, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 123ms
2020-09-22T12:10:23.234+0800 I  SHARDING [conn33] distributed lock 'jepsendb' acquired for 'enableSharding', ts : 5f69792f513fac20f0ca1b55
2020-09-22T12:10:23.234+0800 I  SHARDING [conn33] Marking collection config.databases as collection version: <unsharded>
2020-09-22T12:10:23.241+0800 I  SHARDING [conn33] Registering new database { _id: "jepsendb", primary: "rs_shard1", partitioned: false, version: { uuid: UUID("ab627ffa-6894-4fde-896e-17748d17e74b"), lastMod: 1 } } in sharding catalog
2020-09-22T12:10:23.242+0800 I  STORAGE  [conn33] createCollection: config.databases with generated UUID: 4d7c98cb-17cc-41c8-adb8-aa748e2229f3 and options: {}
2020-09-22T12:10:23.255+0800 I  INDEX    [conn33] index build: done building index _id_ on ns config.databases
2020-09-22T12:10:23.271+0800 I  SHARDING [conn33] Enabling sharding for database [jepsendb] in config db
2020-09-22T12:10:23.277+0800 I  SHARDING [conn33] distributed lock with ts: 5f69792f513fac20f0ca1b55' unlocked.
2020-09-22T12:10:23.313+0800 I  SHARDING [conn38] distributed lock 'jepsendb' acquired for 'enableSharding', ts : 5f69792f513fac20f0ca1b97
2020-09-22T12:10:23.313+0800 I  SHARDING [conn38] Enabling sharding for database [jepsendb] in config db
2020-09-22T12:10:23.320+0800 I  SHARDING [conn38] distributed lock with ts: 5f69792f513fac20f0ca1b97' unlocked.
2020-09-22T12:10:23.322+0800 I  SHARDING [conn33] distributed lock 'jepsendb' acquired for 'createCollection', ts : 5f69792f513fac20f0ca1b9e
2020-09-22T12:10:23.326+0800 I  SHARDING [conn33] distributed lock 'jepsendb.jepsencoll' acquired for 'createCollection', ts : 5f69792f513fac20f0ca1bb2
2020-09-22T12:10:23.366+0800 I  SHARDING [conn33] distributed lock with ts: 5f69792f513fac20f0ca1bb2' unlocked.
2020-09-22T12:10:23.369+0800 I  SHARDING [conn33] distributed lock with ts: 5f69792f513fac20f0ca1b9e' unlocked.
2020-09-22T12:10:23.374+0800 I  SHARDING [conn38] distributed lock 'jepsendb' acquired for 'createCollection', ts : 5f69792f513fac20f0ca1bcb
2020-09-22T12:10:23.377+0800 I  SHARDING [conn38] distributed lock 'jepsendb.jepsencoll' acquired for 'createCollection', ts : 5f69792f513fac20f0ca1bd7
2020-09-22T12:10:23.385+0800 I  SHARDING [conn38] distributed lock with ts: 5f69792f513fac20f0ca1bd7' unlocked.
2020-09-22T12:10:23.388+0800 I  SHARDING [conn38] distributed lock with ts: 5f69792f513fac20f0ca1bcb' unlocked.
2020-09-22T12:10:23.398+0800 I  SHARDING [conn33] distributed lock 'jepsendb' acquired for 'shardCollection', ts : 5f69792f513fac20f0ca1bf8
2020-09-22T12:10:23.401+0800 I  SHARDING [conn33] distributed lock 'jepsendb.jepsencoll' acquired for 'shardCollection', ts : 5f69792f513fac20f0ca1bfe
2020-09-22T12:10:23.465+0800 I  STORAGE  [conn55] createCollection: config.collections with generated UUID: aff6d799-d8e5-42bb-9d1a-d8dc37c3bc27 and options: {}
2020-09-22T12:10:23.480+0800 I  INDEX    [conn55] index build: done building index _id_ on ns config.collections
2020-09-22T12:10:23.517+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database jepsendb from version {} to version { uuid: UUID("ab627ffa-6894-4fde-896e-17748d17e74b"), lastMod: 1 } took 0 ms
2020-09-22T12:10:23.517+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for collection jepsendb.jepsencoll to version 1|6||5f69792fb532ea868744a14e took 0 ms
2020-09-22T12:10:23.521+0800 I  SHARDING [conn33] distributed lock with ts: 5f69792f513fac20f0ca1bfe' unlocked.
2020-09-22T12:10:23.525+0800 I  SHARDING [conn33] distributed lock with ts: 5f69792f513fac20f0ca1bf8' unlocked.
2020-09-22T12:10:23.525+0800 I  COMMAND  [conn33] command admin.$cmd command: _configsvrShardCollection { _configsvrShardCollection: "jepsendb.jepsencoll", key: { _id: "hashed" }, unique: false, numInitialChunks: 7, getUUIDfromPrimaryShard: true, writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("d0fdeeec-7dda-442f-86a9-337ca1e19900"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747823, 20), signature: { hash: BinData(0, 3049FE956407688F2A28BDA23ECB26EC224DA478), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "Disalg.Hangzhou.I.01:27017", client: "211.162.81.126:35276", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747823, 19), t: 1 } }, $db: "admin" } numYields:0 reslen:585 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 4 } }, ReplicationStateTransition: { acquireCount: { w: 6 } }, Global: { acquireCount: { r: 2, w: 4 } }, Database: { acquireCount: { r: 2, w: 4 } }, Collection: { acquireCount: { r: 2, w: 4 } }, Mutex: { acquireCount: { r: 10, W: 1 } } } flowControl:{ acquireCount: 4, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 132ms
2020-09-22T12:10:23.735+0800 I  SHARDING [conn45] distributed lock 'jepsendb' acquired for 'enableSharding', ts : 5f69792f513fac20f0ca1b58
2020-09-22T12:10:23.736+0800 I  SHARDING [conn45] Enabling sharding for database [jepsendb] in config db
2020-09-22T12:10:23.743+0800 I  SHARDING [conn45] distributed lock with ts: 5f69792f513fac20f0ca1b58' unlocked.
2020-09-22T12:10:23.743+0800 I  COMMAND  [conn45] command admin.$cmd command: _configsvrEnableSharding { _configsvrEnableSharding: "jepsendb", writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("c28eabec-81ab-4380-add7-8cd5492fe840"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747821, 5), signature: { hash: BinData(0, 93182DD852888B086B96B1A3993347E20B2273A0), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "Disalg.Hangzhou.I.00:27017", client: "211.162.81.126:54656", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747821, 5), t: 1 } }, $db: "admin" } numYields:0 reslen:505 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 4 } }, ReplicationStateTransition: { acquireCount: { w: 6 } }, Global: { acquireCount: { r: 2, w: 4 } }, Database: { acquireCount: { r: 2, w: 4 } }, Collection: { acquireCount: { r: 2, w: 4 } }, Mutex: { acquireCount: { r: 8 } } } flowControl:{ acquireCount: 4, timeAcquiringMicros: 5 } storage:{} protocol:op_msg 516ms
2020-09-22T12:10:23.771+0800 I  SHARDING [conn45] distributed lock 'jepsendb' acquired for 'createCollection', ts : 5f69792f513fac20f0ca1c81
2020-09-22T12:10:23.776+0800 I  SHARDING [conn45] distributed lock 'jepsendb.jepsencoll' acquired for 'createCollection', ts : 5f69792f513fac20f0ca1c86
2020-09-22T12:10:23.784+0800 I  SHARDING [conn45] distributed lock with ts: 5f69792f513fac20f0ca1c86' unlocked.
2020-09-22T12:10:23.788+0800 I  SHARDING [conn45] distributed lock with ts: 5f69792f513fac20f0ca1c81' unlocked.
2020-09-22T12:10:23.808+0800 I  SHARDING [conn45] distributed lock 'jepsendb' acquired for 'shardCollection', ts : 5f69792f513fac20f0ca1caa
2020-09-22T12:10:23.812+0800 I  SHARDING [conn45] distributed lock 'jepsendb.jepsencoll' acquired for 'shardCollection', ts : 5f69792f513fac20f0ca1caf
2020-09-22T12:10:23.820+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database jepsendb from version {} to version { uuid: UUID("ab627ffa-6894-4fde-896e-17748d17e74b"), lastMod: 1 } took 0 ms
2020-09-22T12:10:23.821+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for collection jepsendb.jepsencoll to version 1|6||5f69792fb532ea868744a14e took 0 ms
2020-09-22T12:10:23.824+0800 I  SHARDING [conn45] distributed lock with ts: 5f69792f513fac20f0ca1caf' unlocked.
2020-09-22T12:10:23.831+0800 I  SHARDING [conn45] distributed lock with ts: 5f69792f513fac20f0ca1caa' unlocked.
2020-09-22T12:10:23.936+0800 I  SHARDING [conn38] distributed lock 'jepsendb' acquired for 'shardCollection', ts : 5f69792f513fac20f0ca1c0f
2020-09-22T12:10:23.939+0800 I  SHARDING [conn38] distributed lock 'jepsendb.jepsencoll' acquired for 'shardCollection', ts : 5f69792f513fac20f0ca1cdc
2020-09-22T12:10:23.950+0800 I  SHARDING [conn38] distributed lock with ts: 5f69792f513fac20f0ca1cdc' unlocked.
2020-09-22T12:10:23.953+0800 I  SHARDING [conn38] distributed lock with ts: 5f69792f513fac20f0ca1c0f' unlocked.
2020-09-22T12:10:23.953+0800 I  COMMAND  [conn38] command admin.$cmd command: _configsvrShardCollection { _configsvrShardCollection: "jepsendb.jepsencoll", key: { _id: "hashed" }, unique: false, numInitialChunks: 7, getUUIDfromPrimaryShard: true, writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("96fdf88d-e860-4728-87a3-9b73a42c7bab"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747823, 23), signature: { hash: BinData(0, 3049FE956407688F2A28BDA23ECB26EC224DA478), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "Jepsen-Node-01:27017", client: "114.212.84.175:54238", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747823, 23), t: 1 } }, $db: "admin" } numYields:0 reslen:585 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 6 } }, ReplicationStateTransition: { acquireCount: { w: 10 } }, Global: { acquireCount: { r: 5, w: 5 } }, Database: { acquireCount: { r: 4, w: 5 } }, Collection: { acquireCount: { r: 3, w: 5 } }, Mutex: { acquireCount: { r: 13, W: 1 } }, oplog: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 5, timeAcquiringMicros: 28 } storage:{} protocol:op_msg 523ms
2020-09-22T12:10:24.243+0800 I  SHARDING [conn43] distributed lock 'jepsendb' acquired for 'enableSharding', ts : 5f69792f513fac20f0ca1b60
2020-09-22T12:10:24.246+0800 I  SHARDING [conn43] Enabling sharding for database [jepsendb] in config db
2020-09-22T12:10:24.249+0800 I  SHARDING [conn43] distributed lock with ts: 5f69792f513fac20f0ca1b60' unlocked.
2020-09-22T12:10:24.250+0800 I  COMMAND  [conn43] command admin.$cmd command: _configsvrEnableSharding { _configsvrEnableSharding: "jepsendb", writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("a9dcdac1-05c0-414f-b0c7-287046451274"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747821, 5), signature: { hash: BinData(0, 93182DD852888B086B96B1A3993347E20B2273A0), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "Jepsen-Node-02:27017", client: "211.162.81.126:30242", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747821, 5), t: 1 } }, $db: "admin" } numYields:4 reslen:505 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 11 } }, ReplicationStateTransition: { acquireCount: { w: 18 } }, Global: { acquireCount: { r: 9, w: 9 } }, Database: { acquireCount: { r: 7, w: 9 } }, Collection: { acquireCount: { r: 5, w: 9 } }, Mutex: { acquireCount: { r: 14 } }, oplog: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 9, timeAcquiringMicros: 5 } storage:{} protocol:op_msg 1021ms
2020-09-22T12:10:24.272+0800 I  SHARDING [conn43] distributed lock 'jepsendb' acquired for 'createCollection', ts : 5f697930513fac20f0ca1d1b
2020-09-22T12:10:24.276+0800 I  SHARDING [conn43] distributed lock 'jepsendb.jepsencoll' acquired for 'createCollection', ts : 5f697930513fac20f0ca1d21
2020-09-22T12:10:24.285+0800 I  SHARDING [conn43] distributed lock with ts: 5f697930513fac20f0ca1d21' unlocked.
2020-09-22T12:10:24.289+0800 I  SHARDING [conn43] distributed lock with ts: 5f697930513fac20f0ca1d1b' unlocked.
2020-09-22T12:10:24.309+0800 I  SHARDING [conn43] distributed lock 'jepsendb' acquired for 'shardCollection', ts : 5f697930513fac20f0ca1d44
2020-09-22T12:10:24.313+0800 I  SHARDING [conn43] distributed lock 'jepsendb.jepsencoll' acquired for 'shardCollection', ts : 5f697930513fac20f0ca1d4e
2020-09-22T12:10:24.320+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database jepsendb from version {} to version { uuid: UUID("ab627ffa-6894-4fde-896e-17748d17e74b"), lastMod: 1 } took 0 ms
2020-09-22T12:10:24.321+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for collection jepsendb.jepsencoll to version 1|6||5f69792fb532ea868744a14e took 0 ms
2020-09-22T12:10:24.324+0800 I  SHARDING [conn43] distributed lock with ts: 5f697930513fac20f0ca1d4e' unlocked.
2020-09-22T12:10:24.328+0800 I  SHARDING [conn43] distributed lock with ts: 5f697930513fac20f0ca1d44' unlocked.
2020-09-22T12:10:24.749+0800 I  SHARDING [conn26] distributed lock 'jepsendb' acquired for 'enableSharding', ts : 5f69792f513fac20f0ca1b5b
2020-09-22T12:10:24.749+0800 I  SHARDING [conn26] Enabling sharding for database [jepsendb] in config db
2020-09-22T12:10:24.755+0800 I  SHARDING [conn26] distributed lock with ts: 5f69792f513fac20f0ca1b5b' unlocked.
2020-09-22T12:10:24.755+0800 I  COMMAND  [conn26] command admin.$cmd command: _configsvrEnableSharding { _configsvrEnableSharding: "jepsendb", writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("6709c016-dc0c-45d2-b728-fc7035cfe802"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747821, 5), signature: { hash: BinData(0, 93182DD852888B086B96B1A3993347E20B2273A0), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "Disalg.Hangzhou.I.02:27017", client: "211.162.81.126:55003", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747821, 5), t: 1 } }, $db: "admin" } numYields:15 reslen:505 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 23 } }, ReplicationStateTransition: { acquireCount: { w: 31 } }, Global: { acquireCount: { r: 10, w: 21 } }, Database: { acquireCount: { r: 8, w: 21 } }, Collection: { acquireCount: { r: 6, w: 21 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 21, timeAcquiringMicros: 11 } storage:{} protocol:op_msg 1528ms
2020-09-22T12:10:24.785+0800 I  SHARDING [conn26] distributed lock 'jepsendb' acquired for 'createCollection', ts : 5f697930513fac20f0ca1d8a
2020-09-22T12:10:24.788+0800 I  SHARDING [conn26] distributed lock 'jepsendb.jepsencoll' acquired for 'createCollection', ts : 5f697930513fac20f0ca1d90
2020-09-22T12:10:24.796+0800 I  SHARDING [conn26] distributed lock with ts: 5f697930513fac20f0ca1d90' unlocked.
2020-09-22T12:10:24.801+0800 I  SHARDING [conn26] distributed lock with ts: 5f697930513fac20f0ca1d8a' unlocked.
2020-09-22T12:10:24.824+0800 I  SHARDING [conn26] distributed lock 'jepsendb' acquired for 'shardCollection', ts : 5f697930513fac20f0ca1db3
2020-09-22T12:10:24.827+0800 I  SHARDING [conn26] distributed lock 'jepsendb.jepsencoll' acquired for 'shardCollection', ts : 5f697930513fac20f0ca1dbd
2020-09-22T12:10:24.835+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database jepsendb from version {} to version { uuid: UUID("ab627ffa-6894-4fde-896e-17748d17e74b"), lastMod: 1 } took 0 ms
2020-09-22T12:10:24.835+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for collection jepsendb.jepsencoll to version 1|6||5f69792fb532ea868744a14e took 0 ms
2020-09-22T12:10:24.839+0800 I  SHARDING [conn26] distributed lock with ts: 5f697930513fac20f0ca1dbd' unlocked.
2020-09-22T12:10:24.843+0800 I  SHARDING [conn26] distributed lock with ts: 5f697930513fac20f0ca1db3' unlocked.
2020-09-22T12:10:25.256+0800 I  SHARDING [conn29] distributed lock 'jepsendb' acquired for 'enableSharding', ts : 5f69792f513fac20f0ca1b6e
2020-09-22T12:10:25.256+0800 I  SHARDING [conn29] Enabling sharding for database [jepsendb] in config db
2020-09-22T12:10:25.260+0800 I  SHARDING [conn29] distributed lock with ts: 5f69792f513fac20f0ca1b6e' unlocked.
2020-09-22T12:10:25.260+0800 I  COMMAND  [conn29] command admin.$cmd command: _configsvrEnableSharding { _configsvrEnableSharding: "jepsendb", writeConcern: { w: "majority", wtimeout: 60000 }, lsid: { id: UUID("8349381a-a200-45ee-9cf5-777a9b848103"), uid: BinData(0, E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855) }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747823, 7), signature: { hash: BinData(0, 3049FE956407688F2A28BDA23ECB26EC224DA478), keyId: 6875159523158392863 } }, $client: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04", mongos: { host: "iZbp157vvbma2xfbxku74xZ:27017", client: "211.162.81.126:15734", version: "4.2.6" } }, $configServerState: { opTime: { ts: Timestamp(1600747823, 7), t: 1 } }, $db: "admin" } numYields:9 reslen:505 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 20 } }, ReplicationStateTransition: { acquireCount: { w: 33 } }, Global: { acquireCount: { r: 17, w: 16 } }, Database: { acquireCount: { r: 13, w: 16 } }, Collection: { acquireCount: { r: 9, w: 16 } }, Mutex: { acquireCount: { r: 22 } }, oplog: { acquireCount: { r: 4 } } } flowControl:{ acquireCount: 16, timeAcquiringMicros: 7 } storage:{} protocol:op_msg 2024ms
2020-09-22T12:10:25.278+0800 I  SHARDING [conn29] distributed lock 'jepsendb' acquired for 'createCollection', ts : 5f697931513fac20f0ca1df7
2020-09-22T12:10:25.283+0800 I  SHARDING [conn29] distributed lock 'jepsendb.jepsencoll' acquired for 'createCollection', ts : 5f697931513fac20f0ca1dfc
2020-09-22T12:10:25.291+0800 I  SHARDING [conn29] distributed lock with ts: 5f697931513fac20f0ca1dfc' unlocked.
2020-09-22T12:10:25.294+0800 I  SHARDING [conn29] distributed lock with ts: 5f697931513fac20f0ca1df7' unlocked.
2020-09-22T12:10:25.314+0800 I  SHARDING [conn29] distributed lock 'jepsendb' acquired for 'shardCollection', ts : 5f697931513fac20f0ca1e1f
2020-09-22T12:10:25.318+0800 I  SHARDING [conn29] distributed lock 'jepsendb.jepsencoll' acquired for 'shardCollection', ts : 5f697931513fac20f0ca1e27
2020-09-22T12:10:25.326+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database jepsendb from version {} to version { uuid: UUID("ab627ffa-6894-4fde-896e-17748d17e74b"), lastMod: 1 } took 0 ms
2020-09-22T12:10:25.326+0800 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for collection jepsendb.jepsencoll to version 1|6||5f69792fb532ea868744a14e took 0 ms
2020-09-22T12:10:25.331+0800 I  SHARDING [conn29] distributed lock with ts: 5f697931513fac20f0ca1e27' unlocked.
2020-09-22T12:10:25.334+0800 I  SHARDING [conn29] distributed lock with ts: 5f697931513fac20f0ca1e1f' unlocked.
2020-09-22T12:10:26.008+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22058 #58 (34 connections now open)
2020-09-22T12:10:26.009+0800 I  NETWORK  [conn58] received client metadata from 211.162.81.126:22058 conn58: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:26.009+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11614 #59 (35 connections now open)
2020-09-22T12:10:26.011+0800 I  NETWORK  [conn59] received client metadata from 211.162.81.126:11614 conn59: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:26.077+0800 I  NETWORK  [conn58] end connection 211.162.81.126:22058 (34 connections now open)
2020-09-22T12:10:26.077+0800 I  NETWORK  [conn59] end connection 211.162.81.126:11614 (33 connections now open)
2020-09-22T12:10:34.127+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22059 #60 (34 connections now open)
2020-09-22T12:10:34.128+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22060 #61 (35 connections now open)
2020-09-22T12:10:34.128+0800 I  NETWORK  [conn60] received client metadata from 211.162.81.126:22059 conn60: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:34.129+0800 I  NETWORK  [conn61] received client metadata from 211.162.81.126:22060 conn61: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:34.187+0800 I  NETWORK  [conn61] end connection 211.162.81.126:22060 (34 connections now open)
2020-09-22T12:10:34.188+0800 I  NETWORK  [conn60] end connection 211.162.81.126:22059 (33 connections now open)
2020-09-22T12:10:34.720+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22061 #62 (34 connections now open)
2020-09-22T12:10:34.721+0800 I  NETWORK  [conn62] received client metadata from 211.162.81.126:22061 conn62: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:34.724+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11615 #63 (35 connections now open)
2020-09-22T12:10:34.724+0800 I  NETWORK  [conn63] received client metadata from 211.162.81.126:11615 conn63: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:34.789+0800 I  NETWORK  [conn62] end connection 211.162.81.126:22061 (34 connections now open)
2020-09-22T12:10:34.790+0800 I  NETWORK  [conn63] end connection 211.162.81.126:11615 (33 connections now open)
2020-09-22T12:10:35.585+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11616 #64 (34 connections now open)
2020-09-22T12:10:35.586+0800 I  NETWORK  [conn64] received client metadata from 211.162.81.126:11616 conn64: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:35.609+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22062 #65 (35 connections now open)
2020-09-22T12:10:35.621+0800 I  NETWORK  [conn65] received client metadata from 211.162.81.126:22062 conn65: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:35.683+0800 I  NETWORK  [conn64] end connection 211.162.81.126:11616 (34 connections now open)
2020-09-22T12:10:35.685+0800 I  NETWORK  [conn65] end connection 211.162.81.126:22062 (33 connections now open)
2020-09-22T12:10:38.627+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11617 #66 (34 connections now open)
2020-09-22T12:10:38.628+0800 I  NETWORK  [conn66] received client metadata from 211.162.81.126:11617 conn66: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:38.629+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11618 #67 (35 connections now open)
2020-09-22T12:10:38.630+0800 I  NETWORK  [conn67] received client metadata from 211.162.81.126:11618 conn67: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:38.692+0800 I  NETWORK  [conn66] end connection 211.162.81.126:11617 (34 connections now open)
2020-09-22T12:10:38.692+0800 I  NETWORK  [conn67] end connection 211.162.81.126:11618 (33 connections now open)
2020-09-22T12:10:39.685+0800 I  REPL     [replexec-3] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:10:39.685+0800 I  REPL     [replexec-3] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:10:39.685+0800 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-09-22T12:10:39.685+0800 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-09-22T12:10:39.685+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:10:39.685+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:10:39.685+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:10:39.685+0800 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-09-22T12:10:39.686+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:10:40.354+0800 I  REPL     [replexec-6] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:10:40.452+0800 I  NETWORK  [conn40] end connection 112.124.21.191:54250 (32 connections now open)
2020-09-22T12:10:40.456+0800 I  NETWORK  [conn12] end connection 120.55.192.104:47628 (31 connections now open)
2020-09-22T12:10:40.456+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47702 #68 (32 connections now open)
2020-09-22T12:10:40.457+0800 I  NETWORK  [conn68] received client metadata from 120.55.192.104:47702 conn68: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:40.539+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:10:40.539+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:10:40.700+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54298 #70 (33 connections now open)
2020-09-22T12:10:40.700+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54296 #71 (34 connections now open)
2020-09-22T12:10:40.700+0800 I  NETWORK  [conn71] received client metadata from 112.124.21.191:54296 conn71: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:40.700+0800 I  NETWORK  [conn70] received client metadata from 112.124.21.191:54298 conn70: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:40.701+0800 I  NETWORK  [conn71] end connection 112.124.21.191:54296 (33 connections now open)
2020-09-22T12:10:41.040+0800 I  REPL     [replexec-2] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:10:41.083+0800 I  NETWORK  [conn11] end connection 112.124.21.191:54220 (32 connections now open)
2020-09-22T12:10:41.434+0800 I  ELECTION [replexec-6] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-09-22T12:10:41.434+0800 I  ELECTION [replexec-6] conducting a dry run election to see if we could be elected. current term: 2
2020-09-22T12:10:41.434+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 82 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 2, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747831, 5), t: 1 } }
2020-09-22T12:10:41.434+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 83 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 2, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747831, 5), t: 1 } }
2020-09-22T12:10:41.435+0800 I  ELECTION [replexec-2] VoteRequester(term 2 dry run) received a no vote from 120.55.192.104:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747831, 5), t: 1 }, my last applied OpTime: { ts: Timestamp(1600747840, 1), t: 2 }"; response message: { term: 2, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747831, 5), t: 1 }, my last applied OpTime: { ts: Timestamp...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('000000000000000000000000') }, lastCommittedOpTime: Timestamp(1600747831, 5), $clusterTime: { clusterTime: Timestamp(1600747841, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747840, 1) }
2020-09-22T12:10:41.435+0800 I  ELECTION [replexec-2] VoteRequester(term 2 dry run) received a no vote from 112.124.21.191:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747831, 5), t: 1 }, my last applied OpTime: { ts: Timestamp(1600747840, 1), t: 2 }"; response message: { term: 2, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747831, 5), t: 1 }, my last applied OpTime: { ts: Timestamp...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000002') }, lastCommittedOpTime: Timestamp(1600747831, 5), $clusterTime: { clusterTime: Timestamp(1600747841, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747840, 1) }
2020-09-22T12:10:41.435+0800 I  ELECTION [replexec-2] not running for primary, we received insufficient votes
2020-09-22T12:10:41.435+0800 I  ELECTION [replexec-2] Lost dry run election due to internal error
2020-09-22T12:10:41.686+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:10:41.687+0800 I  CONNPOOL [RS] Connecting to 112.124.21.191:27019
2020-09-22T12:10:41.689+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:10:41.692+0800 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 112.124.21.191:27019, my last fetched oplog optime: { ts: Timestamp(1600747840, 1), t: 2 }, latest oplog optime of sync source: { ts: Timestamp(1600747840, 1), t: 2 } (sync source does not know the primary)
2020-09-22T12:10:41.692+0800 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 112.124.21.191:27019, OpTime { ts: Timestamp(1600747840, 1), t: 2 }, its sync source index:-1
2020-09-22T12:10:41.693+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 112.124.21.191:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747840, 1), t: 2 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:10:41.693+0800 I  REPL     [rsBackgroundSync] Clearing sync source 112.124.21.191:27019 to choose a new one.
2020-09-22T12:10:41.693+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-09-22T12:10:41.717+0800 I  ELECTION [conn70] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 2, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747840, 1), t: 2 } }
2020-09-22T12:10:41.717+0800 I  ELECTION [conn70] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2020-09-22T12:10:41.718+0800 I  NETWORK  [conn70] end connection 112.124.21.191:54298 (31 connections now open)
2020-09-22T12:10:41.718+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54312 #74 (32 connections now open)
2020-09-22T12:10:41.719+0800 I  NETWORK  [conn74] received client metadata from 112.124.21.191:54312 conn74: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:41.719+0800 I  ELECTION [conn74] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 3, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747840, 1), t: 2 } }
2020-09-22T12:10:41.719+0800 I  ELECTION [conn74] Sending vote response: { term: 3, voteGranted: true, reason: "" }
2020-09-22T12:10:41.721+0800 I  NETWORK  [conn74] end connection 112.124.21.191:54312 (31 connections now open)
2020-09-22T12:10:41.721+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54314 #75 (32 connections now open)
2020-09-22T12:10:41.721+0800 I  NETWORK  [conn75] received client metadata from 112.124.21.191:54314 conn75: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:42.193+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:10:42.194+0800 I  REPL     [replexec-5] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:10:42.194+0800 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-09-22T12:10:43.249+0800
2020-09-22T12:10:42.693+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:10:42.694+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:10:42.698+0800 I  COMMAND  [conn43] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747842, 3), t: 3 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747842, 24), signature: { hash: BinData(0, 6136FD4D0F52C2BA53436B8599B59E3F05F5AE9C), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747842, 3), t: 3 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 366ms
2020-09-22T12:10:42.699+0800 I  COMMAND  [conn33] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747842, 3), t: 3 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747842, 24), signature: { hash: BinData(0, 6136FD4D0F52C2BA53436B8599B59E3F05F5AE9C), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747842, 3), t: 3 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 361ms
2020-09-22T12:10:42.699+0800 I  COMMAND  [conn26] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747842, 3), t: 3 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747842, 24), signature: { hash: BinData(0, 6136FD4D0F52C2BA53436B8599B59E3F05F5AE9C), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747842, 3), t: 3 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 365ms
2020-09-22T12:10:42.699+0800 I  COMMAND  [conn29] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747842, 3), t: 3 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747842, 26), signature: { hash: BinData(0, 6136FD4D0F52C2BA53436B8599B59E3F05F5AE9C), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747842, 3), t: 3 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 366ms
2020-09-22T12:10:43.249+0800 I  REPL     [replexec-2] Canceling priority takeover callback
2020-09-22T12:10:43.249+0800 I  ELECTION [replexec-2] Starting an election for a priority takeover
2020-09-22T12:10:43.249+0800 I  ELECTION [replexec-2] conducting a dry run election to see if we could be elected. current term: 3
2020-09-22T12:10:43.249+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 108 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 3, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747842, 3), t: 3 } }
2020-09-22T12:10:43.249+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 109 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 3, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747842, 3), t: 3 } }
2020-09-22T12:10:43.249+0800 I  ELECTION [replexec-5] VoteRequester(term 3 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 3, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('000000000000000000000000') }, lastCommittedOpTime: Timestamp(1600747842, 3), $clusterTime: { clusterTime: Timestamp(1600747842, 35), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747842, 3) }
2020-09-22T12:10:43.250+0800 I  ELECTION [replexec-5] dry election run succeeded, running for election in term 4
2020-09-22T12:10:43.251+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 110 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 4, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747842, 3), t: 3 } }
2020-09-22T12:10:43.251+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 111 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 4, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747842, 3), t: 3 } }
2020-09-22T12:10:43.253+0800 I  ELECTION [replexec-2] VoteRequester(term 4) received a yes vote from 120.55.192.104:27019; response message: { term: 4, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('000000000000000000000000') }, lastCommittedOpTime: Timestamp(1600747842, 3), $clusterTime: { clusterTime: Timestamp(1600747842, 35), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747842, 3) }
2020-09-22T12:10:43.253+0800 I  ELECTION [replexec-2] election succeeded, assuming primary role in term 4
2020-09-22T12:10:43.253+0800 I  REPL     [replexec-2] transition to PRIMARY from SECONDARY
2020-09-22T12:10:43.253+0800 I  REPL     [replexec-2] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:10:43.253+0800 I  REPL     [replexec-2] Entering primary catch-up mode.
2020-09-22T12:10:43.253+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:10:43.253+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:10:43.255+0800 I  REPL     [replexec-4] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:10:43.255+0800 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747842, 3), t: 3 }. My Last Applied: { ts: Timestamp(1600747842, 3), t: 3 }
2020-09-22T12:10:43.255+0800 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-09-22T12:10:43.255+0800 I  REPL     [replexec-4] Stopping replication producer
2020-09-22T12:10:43.255+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 4
2020-09-22T12:10:43.255+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 4
2020-09-22T12:10:43.255+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:10:43.255+0800 I  CONNPOOL [RS] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:10:43.255+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:10:43.255+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:10:43.255+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:10:43.257+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:10:43.258+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:10:43.258+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:10:43.258+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:10:43.258+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:10:43.700+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:10:44.256+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54320 #77 (33 connections now open)
2020-09-22T12:10:44.256+0800 I  NETWORK  [conn77] received client metadata from 112.124.21.191:54320 conn77: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:44.259+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:10:44.259+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:10:44.259+0800 W  SHARDING [replSetDistLockPinger] pinging failed for distributed lock pinger :: caused by :: LockStateChangeFailed: findAndModify query predicate didn't match any lock document
2020-09-22T12:10:47.006+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22063 #78 (34 connections now open)
2020-09-22T12:10:47.007+0800 I  NETWORK  [conn78] received client metadata from 211.162.81.126:22063 conn78: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:47.008+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11619 #79 (35 connections now open)
2020-09-22T12:10:47.009+0800 I  NETWORK  [conn79] received client metadata from 211.162.81.126:11619 conn79: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:47.072+0800 I  NETWORK  [conn78] end connection 211.162.81.126:22063 (34 connections now open)
2020-09-22T12:10:47.074+0800 I  NETWORK  [conn79] end connection 211.162.81.126:11619 (33 connections now open)
2020-09-22T12:10:47.838+0800 I  REPL     [replexec-3] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:10:47.838+0800 I  REPL     [replexec-3] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:10:47.838+0800 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-09-22T12:10:47.838+0800 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-09-22T12:10:47.838+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:10:47.838+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:10:47.838+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:10:47.838+0800 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-09-22T12:10:47.839+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:10:48.070+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:10:48.070+0800 I  ELECTION [replexec-1] Scheduling priority takeover at 2020-09-22T12:10:49.195+0800
2020-09-22T12:10:48.070+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:10:48.095+0800 I  ELECTION [conn75] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 4, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747844, 1), t: 4 } }
2020-09-22T12:10:48.095+0800 I  ELECTION [conn75] Sending vote response: { term: 5, voteGranted: false, reason: "candidate's term (4) is lower than mine (5)" }
2020-09-22T12:10:48.095+0800 I  NETWORK  [conn75] end connection 112.124.21.191:54314 (32 connections now open)
2020-09-22T12:10:48.154+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22064 #80 (33 connections now open)
2020-09-22T12:10:48.155+0800 I  NETWORK  [conn80] received client metadata from 211.162.81.126:22064 conn80: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:48.158+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11620 #81 (34 connections now open)
2020-09-22T12:10:48.158+0800 I  NETWORK  [conn81] received client metadata from 211.162.81.126:11620 conn81: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:48.217+0800 I  NETWORK  [conn81] end connection 211.162.81.126:11620 (33 connections now open)
2020-09-22T12:10:48.217+0800 I  NETWORK  [conn80] end connection 211.162.81.126:22064 (32 connections now open)
2020-09-22T12:10:48.880+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54326 #82 (33 connections now open)
2020-09-22T12:10:48.880+0800 I  NETWORK  [conn82] received client metadata from 112.124.21.191:54326 conn82: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:48.883+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54328 #83 (34 connections now open)
2020-09-22T12:10:48.884+0800 I  NETWORK  [conn83] received client metadata from 112.124.21.191:54328 conn83: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:48.955+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11621 #84 (35 connections now open)
2020-09-22T12:10:48.956+0800 I  NETWORK  [conn84] received client metadata from 211.162.81.126:11621 conn84: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:48.964+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22065 #85 (36 connections now open)
2020-09-22T12:10:48.965+0800 I  NETWORK  [conn85] received client metadata from 211.162.81.126:22065 conn85: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:49.024+0800 I  NETWORK  [conn84] end connection 211.162.81.126:11621 (35 connections now open)
2020-09-22T12:10:49.024+0800 I  NETWORK  [conn85] end connection 211.162.81.126:22065 (34 connections now open)
2020-09-22T12:10:49.195+0800 I  REPL     [replexec-3] Canceling priority takeover callback
2020-09-22T12:10:49.195+0800 I  ELECTION [replexec-3] Not starting an election for a priority takeover, since we are not electable due to: Not standing for election because member is not caught up enough to the most up-to-date member to call for priority takeover - must be within 2 seconds (mask 0x80)
2020-09-22T12:10:49.553+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11622 #86 (35 connections now open)
2020-09-22T12:10:49.553+0800 I  NETWORK  [conn86] received client metadata from 211.162.81.126:11622 conn86: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:49.556+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22066 #87 (36 connections now open)
2020-09-22T12:10:49.557+0800 I  NETWORK  [conn87] received client metadata from 211.162.81.126:22066 conn87: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:49.570+0800 I  ELECTION [replexec-0] Scheduling priority takeover at 2020-09-22T12:10:50.601+0800
2020-09-22T12:10:49.620+0800 I  NETWORK  [conn86] end connection 211.162.81.126:11622 (35 connections now open)
2020-09-22T12:10:49.620+0800 I  NETWORK  [conn87] end connection 211.162.81.126:22066 (34 connections now open)
2020-09-22T12:10:49.839+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:10:50.043+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:10:50.044+0800 I  CONNPOOL [RS] Connecting to 112.124.21.191:27019
2020-09-22T12:10:50.417+0800 I  ELECTION [conn68] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 5, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 1), t: 5 } }
2020-09-22T12:10:50.417+0800 I  ELECTION [conn68] Sending vote response: { term: 5, voteGranted: true, reason: "" }
2020-09-22T12:10:50.419+0800 I  REPL     [conn68] Canceling priority takeover callback
2020-09-22T12:10:50.419+0800 I  ELECTION [conn68] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 6, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 1), t: 5 } }
2020-09-22T12:10:50.419+0800 I  ELECTION [conn68] Sending vote response: { term: 6, voteGranted: true, reason: "" }
2020-09-22T12:10:50.568+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 112.124.21.191:27019, my last fetched oplog optime: { ts: Timestamp(1600747850, 1), t: 5 }, latest oplog optime of sync source: { ts: Timestamp(1600747850, 1), t: 5 } (sync source does not know the primary)
2020-09-22T12:10:50.568+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 112.124.21.191:27019, OpTime { ts: Timestamp(1600747850, 1), t: 5 }, its sync source index:-1
2020-09-22T12:10:50.568+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 112.124.21.191:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747850, 1), t: 5 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:10:50.568+0800 I  REPL     [rsBackgroundSync] Clearing sync source 112.124.21.191:27019 to choose a new one.
2020-09-22T12:10:50.568+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-09-22T12:10:50.569+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:10:50.569+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:10:50.569+0800 I  ELECTION [replexec-0] Scheduling priority takeover at 2020-09-22T12:10:51.686+0800
2020-09-22T12:10:50.573+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22067 #89 (35 connections now open)
2020-09-22T12:10:50.574+0800 I  NETWORK  [conn89] received client metadata from 211.162.81.126:22067 conn89: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:50.578+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22068 #90 (36 connections now open)
2020-09-22T12:10:50.578+0800 I  NETWORK  [conn90] received client metadata from 211.162.81.126:22068 conn90: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:50.641+0800 I  NETWORK  [conn90] end connection 211.162.81.126:22068 (35 connections now open)
2020-09-22T12:10:50.641+0800 I  NETWORK  [conn89] end connection 211.162.81.126:22067 (34 connections now open)
2020-09-22T12:10:51.069+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:10:51.536+0800 I  ELECTION [conn83] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 6, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 1), t: 5 } }
2020-09-22T12:10:51.536+0800 I  ELECTION [conn83] Sending vote response: { term: 6, voteGranted: true, reason: "" }
2020-09-22T12:10:51.540+0800 I  REPL     [conn83] Canceling priority takeover callback
2020-09-22T12:10:51.540+0800 I  ELECTION [conn83] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 7, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 1), t: 5 } }
2020-09-22T12:10:51.540+0800 I  ELECTION [conn83] Sending vote response: { term: 7, voteGranted: true, reason: "" }
2020-09-22T12:10:51.569+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:10:51.569+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:10:51.570+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:10:51.570+0800 I  ELECTION [replexec-0] Scheduling priority takeover at 2020-09-22T12:10:52.614+0800
2020-09-22T12:10:51.570+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:10:51.571+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:10:51.574+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747850, 8), t: 6 }, latest oplog optime of sync source: { ts: Timestamp(1600747850, 8), t: 6 } (sync source does not know the primary)
2020-09-22T12:10:51.574+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747850, 8), t: 6 }, its sync source index:-1
2020-09-22T12:10:51.574+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747850, 8), t: 6 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:10:51.574+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:10:51.574+0800 I  REPL     [rsBackgroundSync] could not find member to sync from
2020-09-22T12:10:51.708+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22069 #93 (35 connections now open)
2020-09-22T12:10:51.709+0800 I  NETWORK  [conn93] received client metadata from 211.162.81.126:22069 conn93: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:51.710+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11623 #94 (36 connections now open)
2020-09-22T12:10:51.711+0800 I  NETWORK  [conn94] received client metadata from 211.162.81.126:11623 conn94: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:51.768+0800 I  NETWORK  [conn93] end connection 211.162.81.126:22069 (35 connections now open)
2020-09-22T12:10:51.768+0800 I  NETWORK  [conn94] end connection 211.162.81.126:11623 (34 connections now open)
2020-09-22T12:10:52.075+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:10:52.614+0800 I  REPL     [replexec-1] Canceling priority takeover callback
2020-09-22T12:10:52.614+0800 I  ELECTION [replexec-1] Starting an election for a priority takeover
2020-09-22T12:10:52.614+0800 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 7
2020-09-22T12:10:52.614+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 161 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 7, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 8), t: 6 } }
2020-09-22T12:10:52.614+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 162 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 7, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 8), t: 6 } }
2020-09-22T12:10:52.614+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:10:52.615+0800 I  ELECTION [replexec-6] VoteRequester(term 7 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 7, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000006') }, lastCommittedOpTime: Timestamp(1600747850, 1), $clusterTime: { clusterTime: Timestamp(1600747851, 18), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747850, 8) }
2020-09-22T12:10:52.615+0800 I  ELECTION [replexec-6] dry election run succeeded, running for election in term 8
2020-09-22T12:10:52.616+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 163 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 8, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 8), t: 6 } }
2020-09-22T12:10:52.616+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 164 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 8, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747850, 8), t: 6 } }
2020-09-22T12:10:52.617+0800 I  ELECTION [replexec-1] VoteRequester(term 8) received a yes vote from 120.55.192.104:27019; response message: { term: 8, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000006') }, lastCommittedOpTime: Timestamp(1600747850, 1), $clusterTime: { clusterTime: Timestamp(1600747851, 18), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747850, 8) }
2020-09-22T12:10:52.617+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 8
2020-09-22T12:10:52.617+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2020-09-22T12:10:52.617+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was :27017
2020-09-22T12:10:52.617+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2020-09-22T12:10:52.618+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 2 connections to that host remain open
2020-09-22T12:10:53.075+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-09-22T12:10:53.075+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747850, 8), t: 6 }. My Last Applied: { ts: Timestamp(1600747850, 8), t: 6 }
2020-09-22T12:10:53.075+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-09-22T12:10:53.075+0800 I  REPL     [replexec-0] Stopping replication producer
2020-09-22T12:10:53.075+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 8
2020-09-22T12:10:53.075+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 8
2020-09-22T12:10:53.075+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:10:53.075+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:10:53.075+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:10:53.076+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:10:53.077+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:10:53.077+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:10:53.078+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:10:53.078+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:10:53.546+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:10:53.546+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:10:53.546+0800 I  COMMAND  [conn26] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "Disalg.Hangzhou.I.02:27017" }, u: { $set: { _id: "Disalg.Hangzhou.I.02:27017", ping: new Date(1600747852704), up: 31, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [ "Disalg.Hangzhou.I.02" ] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747853, 1), signature: { hash: BinData(0, 771597ACB8ED1BEC1583BFA9E51968455D8740A5), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747850, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 337ms
2020-09-22T12:10:53.547+0800 I  COMMAND  [conn33] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "Disalg.Hangzhou.I.01:27017" }, u: { $set: { _id: "Disalg.Hangzhou.I.01:27017", ping: new Date(1600747852700), up: 32, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [ "Disalg.Hangzhou.I.01" ] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747852, 23), signature: { hash: BinData(0, 71C8E93B7344D6F7CE2D4F290862D0D25AA63F70), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747850, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 341ms
2020-09-22T12:10:53.547+0800 I  COMMAND  [conn45] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "Disalg.Hangzhou.I.00:27017" }, u: { $set: { _id: "Disalg.Hangzhou.I.00:27017", ping: new Date(1600747852339), up: 30, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [ "Disalg.Hangzhou.I.00" ] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747852, 23), signature: { hash: BinData(0, 71C8E93B7344D6F7CE2D4F290862D0D25AA63F70), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747850, 1), t: 5 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 202ms
2020-09-22T12:10:54.094+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22070 #95 (35 connections now open)
2020-09-22T12:10:54.095+0800 I  NETWORK  [conn95] received client metadata from 211.162.81.126:22070 conn95: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:54.096+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11624 #96 (36 connections now open)
2020-09-22T12:10:54.096+0800 I  NETWORK  [conn96] received client metadata from 211.162.81.126:11624 conn96: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:54.154+0800 I  NETWORK  [conn96] end connection 211.162.81.126:11624 (35 connections now open)
2020-09-22T12:10:54.154+0800 I  NETWORK  [conn95] end connection 211.162.81.126:22070 (34 connections now open)
2020-09-22T12:10:54.163+0800 I  NETWORK  [conn83] end connection 112.124.21.191:54328 (33 connections now open)
2020-09-22T12:10:54.968+0800 I  REPL     [replexec-2] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:10:54.968+0800 I  REPL     [replexec-2] can't see a majority of the set, relinquishing primary
2020-09-22T12:10:54.968+0800 I  REPL     [replexec-2] Stepping down from primary in response to heartbeat
2020-09-22T12:10:54.968+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:10:54.968+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:10:54.968+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:10:54.968+0800 I  REPL     [replexec-2] transition to SECONDARY from PRIMARY
2020-09-22T12:10:54.969+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:10:55.618+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:10:55.618+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:10:55.981+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:10:57.054+0800 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:10:58.077+0800 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:10:58.452+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54342 #97 (34 connections now open)
2020-09-22T12:10:58.452+0800 I  NETWORK  [conn97] received client metadata from 112.124.21.191:54342 conn97: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:58.485+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11625 #98 (35 connections now open)
2020-09-22T12:10:58.485+0800 I  NETWORK  [conn98] received client metadata from 211.162.81.126:11625 conn98: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:58.487+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22071 #99 (36 connections now open)
2020-09-22T12:10:58.487+0800 I  NETWORK  [conn99] received client metadata from 211.162.81.126:22071 conn99: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:10:58.544+0800 I  NETWORK  [conn98] end connection 211.162.81.126:11625 (35 connections now open)
2020-09-22T12:10:58.544+0800 I  NETWORK  [conn99] end connection 211.162.81.126:22071 (34 connections now open)
2020-09-22T12:10:58.967+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54348 #101 (35 connections now open)
2020-09-22T12:10:58.968+0800 I  NETWORK  [conn101] received client metadata from 112.124.21.191:54348 conn101: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:59.118+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:10:59.119+0800 I  ELECTION [replexec-1] Scheduling priority takeover at 2020-09-22T12:11:00.244+0800
2020-09-22T12:10:59.333+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47736 #102 (36 connections now open)
2020-09-22T12:10:59.333+0800 I  NETWORK  [conn42] end connection 120.55.192.104:47648 (35 connections now open)
2020-09-22T12:10:59.333+0800 I  NETWORK  [conn37] end connection 112.124.21.191:54240 (34 connections now open)
2020-09-22T12:10:59.333+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54350 #103 (35 connections now open)
2020-09-22T12:10:59.334+0800 I  NETWORK  [conn102] received client metadata from 120.55.192.104:47736 conn102: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:59.334+0800 I  NETWORK  [conn103] received client metadata from 112.124.21.191:54350 conn103: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:59.372+0800 I  NETWORK  [conn41] end connection 120.55.192.104:47644 (34 connections now open)
2020-09-22T12:10:59.620+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54344 #104 (35 connections now open)
2020-09-22T12:10:59.620+0800 I  NETWORK  [conn104] received client metadata from 112.124.21.191:54344 conn104: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:10:59.627+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:10:59.970+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:10:59.975+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:10:59.980+0800 I  COMMAND  [conn53] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747856, 1), t: 9 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747857, 2), signature: { hash: BinData(0, 0852F940CCF78EF1D05D62DB6B72816A04C60F5B), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747856, 1), t: 9 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 2044ms
2020-09-22T12:10:59.980+0800 I  COMMAND  [conn43] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747856, 1), t: 9 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747856, 1), signature: { hash: BinData(0, 48D457F6C7F7C70A757E8673ACA37248F042ECD9), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747856, 1), t: 9 } }, $db: "config" } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:571 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 2 } }, Mutex: { acquireCount: { r: 1 } } } protocol:op_msg 312ms
2020-09-22T12:11:00.244+0800 I  REPL     [replexec-0] Canceling priority takeover callback
2020-09-22T12:11:00.244+0800 I  ELECTION [replexec-0] Starting an election for a priority takeover
2020-09-22T12:11:00.244+0800 I  ELECTION [replexec-0] conducting a dry run election to see if we could be elected. current term: 9
2020-09-22T12:11:00.244+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 192 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 9, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747859, 5), t: 9 } }
2020-09-22T12:11:00.244+0800 I  REPL     [replexec-0] Scheduling remote command request for vote request: RemoteCommand 193 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 9, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747859, 5), t: 9 } }
2020-09-22T12:11:00.245+0800 I  ELECTION [replexec-2] VoteRequester(term 9 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 9, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000009') }, lastCommittedOpTime: Timestamp(1600747859, 5), $clusterTime: { clusterTime: Timestamp(1600747859, 28), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747859, 5) }
2020-09-22T12:11:00.245+0800 I  ELECTION [replexec-2] dry election run succeeded, running for election in term 10
2020-09-22T12:11:00.246+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 194 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 10, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747859, 5), t: 9 } }
2020-09-22T12:11:00.246+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 195 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 10, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747859, 5), t: 9 } }
2020-09-22T12:11:00.248+0800 I  ELECTION [replexec-0] VoteRequester(term 10) received a yes vote from 112.124.21.191:27019; response message: { term: 10, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000007') }, lastCommittedOpTime: Timestamp(1600747859, 5), $clusterTime: { clusterTime: Timestamp(1600747859, 28), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747859, 5) }
2020-09-22T12:11:00.248+0800 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 10
2020-09-22T12:11:00.248+0800 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
2020-09-22T12:11:00.248+0800 I  REPL     [replexec-0] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:11:00.248+0800 I  REPL     [replexec-0] Entering primary catch-up mode.
2020-09-22T12:11:00.248+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:00.248+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:00.250+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:00.250+0800 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747859, 5), t: 9 }. My Last Applied: { ts: Timestamp(1600747859, 5), t: 9 }
2020-09-22T12:11:00.250+0800 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-09-22T12:11:00.250+0800 I  REPL     [replexec-4] Stopping replication producer
2020-09-22T12:11:00.250+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:00.250+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 10
2020-09-22T12:11:00.250+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 10
2020-09-22T12:11:00.250+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:11:00.250+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:00.250+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:00.250+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:11:00.251+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:11:00.251+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:11:00.252+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:11:00.252+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:11:00.252+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:11:00.304+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22072 #108 (36 connections now open)
2020-09-22T12:11:00.305+0800 I  NETWORK  [conn108] received client metadata from 211.162.81.126:22072 conn108: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:00.308+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22073 #109 (37 connections now open)
2020-09-22T12:11:00.310+0800 I  NETWORK  [conn109] received client metadata from 211.162.81.126:22073 conn109: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:00.367+0800 I  NETWORK  [conn108] end connection 211.162.81.126:22072 (36 connections now open)
2020-09-22T12:11:00.367+0800 I  NETWORK  [conn109] end connection 211.162.81.126:22073 (35 connections now open)
2020-09-22T12:11:00.478+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:11:00.992+0800 I  NETWORK  [conn82] end connection 112.124.21.191:54326 (34 connections now open)
2020-09-22T12:11:01.132+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11626 #110 (35 connections now open)
2020-09-22T12:11:01.132+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22229 #111 (36 connections now open)
2020-09-22T12:11:01.133+0800 I  NETWORK  [conn111] received client metadata from 211.162.81.126:22229 conn111: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:01.133+0800 I  NETWORK  [conn110] received client metadata from 211.162.81.126:11626 conn110: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:01.191+0800 I  NETWORK  [conn111] end connection 211.162.81.126:22229 (35 connections now open)
2020-09-22T12:11:01.192+0800 I  NETWORK  [conn110] end connection 211.162.81.126:11626 (34 connections now open)
2020-09-22T12:11:01.252+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:11:01.252+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:11:01.252+0800 I  COMMAND  [conn33] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747860, 23), signature: { hash: BinData(0, 4F304EE64104CB938DE489348E5435F6CD7440BD), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747859, 5), t: 9 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 667ms
2020-09-22T12:11:01.253+0800 I  COMMAND  [conn45] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747860, 28), signature: { hash: BinData(0, 4F304EE64104CB938DE489348E5435F6CD7440BD), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747859, 5), t: 9 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 631ms
2020-09-22T12:11:01.253+0800 I  COMMAND  [conn26] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747860, 28), signature: { hash: BinData(0, 4F304EE64104CB938DE489348E5435F6CD7440BD), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747859, 5), t: 9 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } storage:{} protocol:op_msg 631ms
2020-09-22T12:11:01.253+0800 I  COMMAND  [conn43] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747860, 22), signature: { hash: BinData(0, 4F304EE64104CB938DE489348E5435F6CD7440BD), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747859, 5), t: 9 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 272ms
2020-09-22T12:11:01.253+0800 I  COMMAND  [conn29] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747860, 28), signature: { hash: BinData(0, 4F304EE64104CB938DE489348E5435F6CD7440BD), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747859, 5), t: 9 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 233ms
2020-09-22T12:11:01.253+0800 I  COMMAND  [conn38] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747860, 38), signature: { hash: BinData(0, 4F304EE64104CB938DE489348E5435F6CD7440BD), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747859, 5), t: 9 } }, $db: "config" } numYields:0 reslen:587 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } storage:{} protocol:op_msg 143ms
2020-09-22T12:11:01.321+0800 I  ELECTION [conn68] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 8, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747853, 9), t: 8 } }
2020-09-22T12:11:01.321+0800 I  ELECTION [conn68] Sending vote response: { term: 10, voteGranted: false, reason: "candidate's term (8) is lower than mine (10)" }
2020-09-22T12:11:01.325+0800 I  NETWORK  [conn68] end connection 120.55.192.104:47702 (33 connections now open)
2020-09-22T12:11:01.985+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22230 #112 (34 connections now open)
2020-09-22T12:11:01.986+0800 I  NETWORK  [conn112] received client metadata from 211.162.81.126:22230 conn112: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:01.992+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22231 #113 (35 connections now open)
2020-09-22T12:11:01.992+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47730 #114 (36 connections now open)
2020-09-22T12:11:01.992+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47732 #115 (37 connections now open)
2020-09-22T12:11:01.992+0800 I  NETWORK  [conn115] received client metadata from 120.55.192.104:47732 conn115: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:01.993+0800 I  NETWORK  [conn114] received client metadata from 120.55.192.104:47730 conn114: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:01.993+0800 I  NETWORK  [conn113] received client metadata from 211.162.81.126:22231 conn113: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:01.993+0800 I  ELECTION [conn115] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 10, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747859, 5), t: 9 } }
2020-09-22T12:11:01.993+0800 I  ELECTION [conn115] Sending vote response: { term: 10, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747859, 5), t: 9 }, my last applied OpTime: { ts: Timestamp..." }
2020-09-22T12:11:02.051+0800 I  NETWORK  [conn112] end connection 211.162.81.126:22230 (36 connections now open)
2020-09-22T12:11:02.052+0800 I  NETWORK  [conn113] end connection 211.162.81.126:22231 (35 connections now open)
2020-09-22T12:11:02.568+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11782 #116 (36 connections now open)
2020-09-22T12:11:02.568+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22232 #117 (37 connections now open)
2020-09-22T12:11:02.568+0800 I  NETWORK  [conn116] received client metadata from 211.162.81.126:11782 conn116: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:02.568+0800 I  NETWORK  [conn117] received client metadata from 211.162.81.126:22232 conn117: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:02.634+0800 I  NETWORK  [conn117] end connection 211.162.81.126:22232 (36 connections now open)
2020-09-22T12:11:02.635+0800 I  NETWORK  [conn116] end connection 211.162.81.126:11782 (35 connections now open)
2020-09-22T12:11:03.753+0800 I  REPL     [replexec-5] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:03.753+0800 I  REPL     [replexec-5] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:03.753+0800 I  REPL     [replexec-5] can't see a majority of the set, relinquishing primary
2020-09-22T12:11:03.753+0800 I  REPL     [replexec-5] Stepping down from primary in response to heartbeat
2020-09-22T12:11:03.753+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:03.753+0800 W  COMMAND  [conn33] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:03.753+0800 I  COMMAND  [conn33] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "Disalg.Hangzhou.I.01:27017" }, u: { $set: { _id: "Disalg.Hangzhou.I.01:27017", ping: new Date(1600747863555), up: 43, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [ "Disalg.Hangzhou.I.01" ] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747862, 1), signature: { hash: BinData(0, EA8B11C554E345C1E258BA30CA76E4D80FB0360A), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747860, 2), t: 10 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 197ms
2020-09-22T12:11:03.754+0800 W  COMMAND  [conn45] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:03.754+0800 I  COMMAND  [conn45] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "Disalg.Hangzhou.I.00:27017" }, u: { $set: { _id: "Disalg.Hangzhou.I.00:27017", ping: new Date(1600747863555), up: 42, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [ "Disalg.Hangzhou.I.00" ] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747862, 3), signature: { hash: BinData(0, EA8B11C554E345C1E258BA30CA76E4D80FB0360A), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747860, 2), t: 10 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 197ms
2020-09-22T12:11:03.755+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:03.755+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 2, userOpsRunning: 0 }
2020-09-22T12:11:03.755+0800 I  REPL     [replexec-5] transition to SECONDARY from PRIMARY
2020-09-22T12:11:03.756+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:11:04.763+0800 I  NETWORK  [conn16] end connection 112.124.21.191:54230 (34 connections now open)
2020-09-22T12:11:04.810+0800 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:11:04.868+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:04.995+0800 I  ELECTION [conn97] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 10, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747860, 2), t: 10 } }
2020-09-22T12:11:04.995+0800 I  ELECTION [conn97] Sending vote response: { term: 11, voteGranted: false, reason: "candidate's term (10) is lower than mine (11)" }
2020-09-22T12:11:04.995+0800 I  NETWORK  [conn97] end connection 112.124.21.191:54342 (33 connections now open)
2020-09-22T12:11:05.066+0800 I  REPL     [replexec-5] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:11:05.066+0800 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-09-22T12:11:06.167+0800
2020-09-22T12:11:05.756+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:05.758+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:05.758+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:11:05.759+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747863, 2), t: 10 }. source's GTE: { ts: Timestamp(1600747864, 1), t: 11 }
2020-09-22T12:11:05.759+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747860, 2), t: 10 }
2020-09-22T12:11:05.759+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:05.759+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 120.55.192.104:27019)
2020-09-22T12:11:05.759+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:05.759+0800 I  COMMAND  [conn33] command config.$cmd command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747864, 25), t: 11 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747864, 28), signature: { hash: BinData(0, DE18B84D936A998618E0A155398AD8481B0F9D0D), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747864, 25), t: 11 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1600747864, 25), t: 11 }, current relevant optime is { ts: Timestamp(1600747860, 2), t: 10 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:766 locks:{} protocol:op_msg 982ms
2020-09-22T12:11:05.759+0800 I  COMMAND  [conn53] command admin.$cmd command: find { find: "system.keys", filter: { purpose: "HMAC", expiresAt: { $gt: Timestamp(1616299817, 0) } }, sort: { expiresAt: 1 }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747864, 25), t: 11 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747865, 1), signature: { hash: BinData(0, C4F409E9C115CD12A6A484A82D0B1BDE5766DD1E), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747864, 25), t: 11 } }, $db: "admin" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1600747864, 25), t: 11 }, current relevant optime is { ts: Timestamp(1600747860, 2), t: 10 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:766 locks:{} protocol:op_msg 337ms
2020-09-22T12:11:05.760+0800 I  COMMAND  [conn45] command config.$cmd command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747864, 25), t: 11 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747864, 28), signature: { hash: BinData(0, DE18B84D936A998618E0A155398AD8481B0F9D0D), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747864, 25), t: 11 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1600747864, 25), t: 11 }, current relevant optime is { ts: Timestamp(1600747860, 2), t: 10 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:766 locks:{} protocol:op_msg 982ms
2020-09-22T12:11:05.760+0800 I  COMMAND  [conn26] command config.$cmd command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747865, 1), t: 11 } }, limit: 1, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747865, 1), signature: { hash: BinData(0, C4F409E9C115CD12A6A484A82D0B1BDE5766DD1E), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747865, 1), t: 11 } }, $db: "config" } numYields:0 ok:0 errMsg:"Error waiting for snapshot not less than { ts: Timestamp(1600747865, 1), t: 11 }, current relevant optime is { ts: Timestamp(1600747860, 2), t: 10 }. :: caused by :: operation was interrupted" errName:InterruptedDueToReplStateChange errCode:11602 reslen:765 locks:{} protocol:op_msg 694ms
2020-09-22T12:11:05.761+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 4, userOpsRunning: 30 }
2020-09-22T12:11:05.761+0800 I  REPL     [rsBackgroundSync] Canceling priority takeover callback
2020-09-22T12:11:05.761+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 115
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 114
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 77
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 56
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 36
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 35
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 34
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 31
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 30
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 25
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:05.761+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:05.761+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:05.761+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:05.761+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:05.765+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747860, 2), t: 10 }
2020-09-22T12:11:05.766+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 2
2020-09-22T12:11:05.766+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:05.766+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.mongos with uuid 282775af-a43f-4c57-858a-851b59ec70fc to /var/lib/mongodb/rollback/config.mongos/removed.2020-09-22T04-11-05.0.bson
2020-09-22T12:11:05.767+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:05.767+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:05.767+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:05.767+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:05.867+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747860, 2) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:05.869+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:05.875+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:05.875+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 165 records totaling to 37308 bytes
2020-09-22T12:11:05.875+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:05.875+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:05.878+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:05.878+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:05.888+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:05.888+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:05.888+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747860, 2)
2020-09-22T12:11:05.888+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 2 update operations and 0 delete operations.
2020-09-22T12:11:05.889+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747863, 1), t: 10 }
2020-09-22T12:11:05.889+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747863, 1) }
2020-09-22T12:11:05.889+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:05.890+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747860, 2) (top of oplog: { ts: Timestamp(1600747860, 2), t: 10 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:05.890+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747860, 2)
2020-09-22T12:11:05.890+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:05.890+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:05.759+0800
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:05.891+0800
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 120.55.192.104:27019
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.mongos
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 2
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747863, 2), t: 10 }
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747860, 2), t: 10 }
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:03.557+0800
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:03.556+0800
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747863, 1)
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747860, 2)
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 		config.mongos
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 		update: 2
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 2
2020-09-22T12:11:05.891+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:05.891+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:05.892+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:06.066+0800 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-09-22T12:11:07.194+0800
2020-09-22T12:11:06.076+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22233 #120 (34 connections now open)
2020-09-22T12:11:06.077+0800 I  NETWORK  [conn120] received client metadata from 211.162.81.126:22233 conn120: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:06.078+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22234 #121 (35 connections now open)
2020-09-22T12:11:06.079+0800 I  NETWORK  [conn121] received client metadata from 211.162.81.126:22234 conn121: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:06.138+0800 I  NETWORK  [conn120] end connection 211.162.81.126:22233 (34 connections now open)
2020-09-22T12:11:06.140+0800 I  NETWORK  [conn121] end connection 211.162.81.126:22234 (33 connections now open)
2020-09-22T12:11:06.498+0800 I  ELECTION [conn115] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 11, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747865, 1), t: 11 } }
2020-09-22T12:11:06.498+0800 I  ELECTION [conn115] Sending vote response: { term: 11, voteGranted: true, reason: "" }
2020-09-22T12:11:06.499+0800 I  NETWORK  [conn115] end connection 120.55.192.104:47732 (32 connections now open)
2020-09-22T12:11:06.500+0800 I  REPL     [conn114] Canceling priority takeover callback
2020-09-22T12:11:06.500+0800 I  ELECTION [conn114] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 12, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747865, 1), t: 11 } }
2020-09-22T12:11:06.500+0800 I  ELECTION [conn114] Sending vote response: { term: 12, voteGranted: true, reason: "" }
2020-09-22T12:11:06.560+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54352 #122 (33 connections now open)
2020-09-22T12:11:06.560+0800 I  NETWORK  [conn122] received client metadata from 112.124.21.191:54352 conn122: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:06.563+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54354 #123 (34 connections now open)
2020-09-22T12:11:06.564+0800 I  NETWORK  [conn123] received client metadata from 112.124.21.191:54354 conn123: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:07.504+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54358 #124 (35 connections now open)
2020-09-22T12:11:07.504+0800 I  NETWORK  [conn124] received client metadata from 112.124.21.191:54358 conn124: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:07.868+0800 I  REPL     [replexec-2] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:11:07.869+0800 I  ELECTION [replexec-2] Scheduling priority takeover at 2020-09-22T12:11:08.953+0800
2020-09-22T12:11:08.066+0800 I  REPL     [replexec-3] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:08.953+0800 I  REPL     [replexec-1] Canceling priority takeover callback
2020-09-22T12:11:08.953+0800 I  ELECTION [replexec-1] Starting an election for a priority takeover
2020-09-22T12:11:08.953+0800 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 12
2020-09-22T12:11:08.953+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 242 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 12, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747866, 7), t: 12 } }
2020-09-22T12:11:08.953+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 243 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 12, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747866, 7), t: 12 } }
2020-09-22T12:11:08.954+0800 I  ELECTION [replexec-3] VoteRequester(term 12 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 12, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000c') }, lastCommittedOpTime: Timestamp(1600747866, 7), $clusterTime: { clusterTime: Timestamp(1600747867, 26), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747866, 7) }
2020-09-22T12:11:08.954+0800 I  ELECTION [replexec-3] dry election run succeeded, running for election in term 13
2020-09-22T12:11:08.955+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 244 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 13, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747866, 7), t: 12 } }
2020-09-22T12:11:08.955+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 245 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 13, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747866, 7), t: 12 } }
2020-09-22T12:11:08.956+0800 I  ELECTION [replexec-1] VoteRequester(term 13) received a yes vote from 112.124.21.191:27019; response message: { term: 13, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000b') }, lastCommittedOpTime: Timestamp(1600747866, 7), $clusterTime: { clusterTime: Timestamp(1600747867, 26), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747866, 7) }
2020-09-22T12:11:08.956+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 13
2020-09-22T12:11:08.956+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2020-09-22T12:11:08.956+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:11:08.957+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2020-09-22T12:11:08.957+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:08.957+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:08.958+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:08.958+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747866, 7), t: 12 }. My Last Applied: { ts: Timestamp(1600747866, 7), t: 12 }
2020-09-22T12:11:08.958+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-09-22T12:11:08.958+0800 I  REPL     [replexec-0] Stopping replication producer
2020-09-22T12:11:08.958+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 13
2020-09-22T12:11:08.958+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 13
2020-09-22T12:11:08.958+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:11:08.959+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:08.959+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:08.959+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:08.959+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-09-22T12:11:08.960+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:11:08.960+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:11:08.961+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:11:08.961+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:11:08.961+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:11:08.961+0800 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:11:08.963+0800 W  NETWORK  [ReplicaSetMonitor-TaskExecutor] Unable to reach primary for set rs_shard1
2020-09-22T12:11:08.965+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:11:08.965+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:11:09.007+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:11:09.463+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:11:09.463+0800 I  CONNPOOL [ShardRegistry] Connecting to 47.96.16.32:27018
2020-09-22T12:11:09.927+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22235 #127 (36 connections now open)
2020-09-22T12:11:09.928+0800 I  NETWORK  [conn127] received client metadata from 211.162.81.126:22235 conn127: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:09.931+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22236 #128 (37 connections now open)
2020-09-22T12:11:09.932+0800 I  NETWORK  [conn128] received client metadata from 211.162.81.126:22236 conn128: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:09.994+0800 I  NETWORK  [conn127] end connection 211.162.81.126:22235 (36 connections now open)
2020-09-22T12:11:09.994+0800 I  NETWORK  [conn128] end connection 211.162.81.126:22236 (35 connections now open)
2020-09-22T12:11:10.087+0800 I  ELECTION [conn114] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 13, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747866, 7), t: 12 } }
2020-09-22T12:11:10.087+0800 I  ELECTION [conn114] Sending vote response: { term: 13, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747866, 7), t: 12 }, my last applied OpTime: { ts: Timestam..." }
2020-09-22T12:11:11.706+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22237 #129 (36 connections now open)
2020-09-22T12:11:11.706+0800 I  NETWORK  [conn129] received client metadata from 211.162.81.126:22237 conn129: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:11.711+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11783 #130 (37 connections now open)
2020-09-22T12:11:11.711+0800 I  NETWORK  [conn130] received client metadata from 211.162.81.126:11783 conn130: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:11.766+0800 I  NETWORK  [conn129] end connection 211.162.81.126:22237 (36 connections now open)
2020-09-22T12:11:11.768+0800 I  NETWORK  [conn130] end connection 211.162.81.126:11783 (35 connections now open)
2020-09-22T12:11:13.624+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11784 #131 (36 connections now open)
2020-09-22T12:11:13.625+0800 I  NETWORK  [conn131] received client metadata from 211.162.81.126:11784 conn131: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:13.632+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22238 #132 (37 connections now open)
2020-09-22T12:11:13.633+0800 I  NETWORK  [conn132] received client metadata from 211.162.81.126:22238 conn132: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:13.692+0800 I  NETWORK  [conn131] end connection 211.162.81.126:11784 (36 connections now open)
2020-09-22T12:11:13.692+0800 I  NETWORK  [conn132] end connection 211.162.81.126:22238 (35 connections now open)
2020-09-22T12:11:14.225+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22240 #133 (36 connections now open)
2020-09-22T12:11:14.226+0800 I  NETWORK  [conn133] received client metadata from 211.162.81.126:22240 conn133: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:14.231+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22239 #134 (37 connections now open)
2020-09-22T12:11:14.231+0800 I  NETWORK  [conn134] received client metadata from 211.162.81.126:22239 conn134: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:14.283+0800 I  NETWORK  [conn133] end connection 211.162.81.126:22240 (36 connections now open)
2020-09-22T12:11:14.286+0800 I  NETWORK  [conn134] end connection 211.162.81.126:22239 (35 connections now open)
2020-09-22T12:11:14.807+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11785 #135 (36 connections now open)
2020-09-22T12:11:14.809+0800 I  NETWORK  [conn135] received client metadata from 211.162.81.126:11785 conn135: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:14.809+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11786 #136 (37 connections now open)
2020-09-22T12:11:14.810+0800 I  NETWORK  [conn136] received client metadata from 211.162.81.126:11786 conn136: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:14.870+0800 I  NETWORK  [conn135] end connection 211.162.81.126:11785 (36 connections now open)
2020-09-22T12:11:14.871+0800 I  NETWORK  [conn136] end connection 211.162.81.126:11786 (35 connections now open)
2020-09-22T12:11:16.684+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22241 #137 (36 connections now open)
2020-09-22T12:11:16.684+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11787 #138 (37 connections now open)
2020-09-22T12:11:16.685+0800 I  NETWORK  [conn138] received client metadata from 211.162.81.126:11787 conn138: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:16.685+0800 I  NETWORK  [conn137] received client metadata from 211.162.81.126:22241 conn137: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:16.742+0800 I  NETWORK  [conn138] end connection 211.162.81.126:11787 (36 connections now open)
2020-09-22T12:11:16.742+0800 I  NETWORK  [conn137] end connection 211.162.81.126:22241 (35 connections now open)
2020-09-22T12:11:17.774+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:17.774+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:17.774+0800 I  REPL     [replexec-1] can't see a majority of the set, relinquishing primary
2020-09-22T12:11:17.774+0800 I  REPL     [replexec-1] Stepping down from primary in response to heartbeat
2020-09-22T12:11:17.774+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:17.774+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:17.774+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:11:17.774+0800 I  REPL     [replexec-1] transition to SECONDARY from PRIMARY
2020-09-22T12:11:17.775+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:11:17.957+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:17.958+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:11:17.958+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:18.375+0800 I  NETWORK  [conn123] end connection 112.124.21.191:54354 (34 connections now open)
2020-09-22T12:11:18.403+0800 I  ELECTION [conn122] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 13, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747875, 27), t: 13 } }
2020-09-22T12:11:18.403+0800 I  ELECTION [conn122] Sending vote response: { term: 13, voteGranted: true, reason: "" }
2020-09-22T12:11:18.404+0800 I  NETWORK  [conn122] end connection 112.124.21.191:54352 (33 connections now open)
2020-09-22T12:11:18.457+0800 I  REPL     [replexec-5] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:11:18.457+0800 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-09-22T12:11:19.604+0800
2020-09-22T12:11:18.636+0800 I  NETWORK  [conn114] end connection 120.55.192.104:47730 (32 connections now open)
2020-09-22T12:11:18.764+0800 I  NETWORK  [conn77] end connection 112.124.21.191:54320 (31 connections now open)
2020-09-22T12:11:18.780+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54364 #139 (32 connections now open)
2020-09-22T12:11:18.780+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54362 #140 (33 connections now open)
2020-09-22T12:11:18.780+0800 I  NETWORK  [conn140] received client metadata from 112.124.21.191:54362 conn140: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:18.780+0800 I  NETWORK  [conn139] received client metadata from 112.124.21.191:54364 conn139: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:18.959+0800 I  REPL     [replexec-3] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:19.004+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47748 #142 (34 connections now open)
2020-09-22T12:11:19.005+0800 I  NETWORK  [conn142] received client metadata from 120.55.192.104:47748 conn142: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:19.010+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22242 #143 (35 connections now open)
2020-09-22T12:11:19.010+0800 I  NETWORK  [conn143] received client metadata from 211.162.81.126:22242 conn143: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:19.012+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11788 #144 (36 connections now open)
2020-09-22T12:11:19.013+0800 I  NETWORK  [conn144] received client metadata from 211.162.81.126:11788 conn144: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:19.062+0800 I  NETWORK  [conn36] end connection 118.31.43.238:51288 (35 connections now open)
2020-09-22T12:11:19.068+0800 I  NETWORK  [conn143] end connection 211.162.81.126:22242 (34 connections now open)
2020-09-22T12:11:19.071+0800 I  NETWORK  [conn144] end connection 211.162.81.126:11788 (33 connections now open)
2020-09-22T12:11:19.264+0800 I  NETWORK  [conn34] end connection 118.31.43.238:51282 (32 connections now open)
2020-09-22T12:11:19.587+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11789 #145 (33 connections now open)
2020-09-22T12:11:19.587+0800 I  NETWORK  [conn145] received client metadata from 211.162.81.126:11789 conn145: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:19.588+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22243 #146 (34 connections now open)
2020-09-22T12:11:19.589+0800 I  NETWORK  [conn146] received client metadata from 211.162.81.126:22243 conn146: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:19.604+0800 I  REPL     [replexec-5] Canceling priority takeover callback
2020-09-22T12:11:19.604+0800 I  ELECTION [replexec-5] Not starting an election for a priority takeover, since we are not electable due to: Not standing for election because member is not caught up enough to the most up-to-date member to call for priority takeover - must be within 2 seconds (mask 0x80)
2020-09-22T12:11:19.645+0800 I  NETWORK  [conn145] end connection 211.162.81.126:11789 (33 connections now open)
2020-09-22T12:11:19.645+0800 I  NETWORK  [conn146] end connection 211.162.81.126:22243 (32 connections now open)
2020-09-22T12:11:19.775+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:19.777+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:19.777+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:11:20.366+0800 I  ELECTION [replexec-6] Scheduling priority takeover at 2020-09-22T12:11:21.429+0800
2020-09-22T12:11:20.929+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11790 #148 (33 connections now open)
2020-09-22T12:11:20.930+0800 I  NETWORK  [conn148] received client metadata from 211.162.81.126:11790 conn148: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:20.931+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22244 #149 (34 connections now open)
2020-09-22T12:11:20.931+0800 I  NETWORK  [conn149] received client metadata from 211.162.81.126:22244 conn149: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:20.990+0800 I  NETWORK  [conn149] end connection 211.162.81.126:22244 (33 connections now open)
2020-09-22T12:11:20.991+0800 I  NETWORK  [conn148] end connection 211.162.81.126:11790 (32 connections now open)
2020-09-22T12:11:21.074+0800 I  ELECTION [conn142] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 14, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747880, 82), t: 14 } }
2020-09-22T12:11:21.074+0800 I  ELECTION [conn142] Sending vote response: { term: 14, voteGranted: true, reason: "" }
2020-09-22T12:11:21.075+0800 I  NETWORK  [conn142] end connection 120.55.192.104:47748 (31 connections now open)
2020-09-22T12:11:21.076+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47750 #150 (32 connections now open)
2020-09-22T12:11:21.076+0800 I  NETWORK  [conn150] received client metadata from 120.55.192.104:47750 conn150: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:21.077+0800 I  REPL     [conn150] Canceling priority takeover callback
2020-09-22T12:11:21.077+0800 I  ELECTION [conn150] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 15, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747880, 82), t: 14 } }
2020-09-22T12:11:21.077+0800 I  ELECTION [conn150] Sending vote response: { term: 15, voteGranted: true, reason: "" }
2020-09-22T12:11:21.390+0800 I  NETWORK  [conn30] end connection 120.55.194.98:50284 (31 connections now open)
2020-09-22T12:11:21.960+0800 I  REPL     [replexec-2] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:11:21.960+0800 I  ELECTION [replexec-2] Scheduling priority takeover at 2020-09-22T12:11:22.984+0800
2020-09-22T12:11:22.366+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:22.984+0800 I  REPL     [replexec-5] Canceling priority takeover callback
2020-09-22T12:11:22.984+0800 I  ELECTION [replexec-5] Starting an election for a priority takeover
2020-09-22T12:11:22.984+0800 I  ELECTION [replexec-5] conducting a dry run election to see if we could be elected. current term: 15
2020-09-22T12:11:22.984+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 311 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 15, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747881, 3), t: 15 } }
2020-09-22T12:11:22.984+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 312 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 15, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747881, 3), t: 15 } }
2020-09-22T12:11:22.985+0800 I  ELECTION [replexec-6] VoteRequester(term 15 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 15, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000f') }, lastCommittedOpTime: Timestamp(1600747881, 3), $clusterTime: { clusterTime: Timestamp(1600747882, 10), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747881, 3) }
2020-09-22T12:11:22.985+0800 I  ELECTION [replexec-6] dry election run succeeded, running for election in term 16
2020-09-22T12:11:22.986+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 313 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 16, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747881, 3), t: 15 } }
2020-09-22T12:11:22.986+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 314 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 16, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747881, 3), t: 15 } }
2020-09-22T12:11:22.988+0800 I  ELECTION [replexec-5] VoteRequester(term 16) received a yes vote from 112.124.21.191:27019; response message: { term: 16, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000e') }, lastCommittedOpTime: Timestamp(1600747881, 3), $clusterTime: { clusterTime: Timestamp(1600747882, 10), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747881, 3) }
2020-09-22T12:11:22.988+0800 I  ELECTION [replexec-5] election succeeded, assuming primary role in term 16
2020-09-22T12:11:22.988+0800 I  REPL     [replexec-5] transition to PRIMARY from SECONDARY
2020-09-22T12:11:22.988+0800 I  REPL     [replexec-5] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:11:22.988+0800 I  REPL     [replexec-5] Entering primary catch-up mode.
2020-09-22T12:11:22.988+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:22.988+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:22.990+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:22.991+0800 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747881, 3), t: 15 }. My Last Applied: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:22.991+0800 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-09-22T12:11:22.991+0800 I  REPL     [replexec-4] Stopping replication producer
2020-09-22T12:11:22.991+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 16
2020-09-22T12:11:22.991+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 16
2020-09-22T12:11:22.991+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:11:22.991+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:22.991+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:22.991+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:22.991+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:11:22.992+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:11:22.993+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:11:22.993+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:11:22.994+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:11:22.994+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:11:23.043+0800 I  NETWORK  [conn56] end connection 47.96.5.198:36874 (30 connections now open)
2020-09-22T12:11:23.102+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:11:23.173+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11791 #152 (31 connections now open)
2020-09-22T12:11:23.173+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22245 #153 (32 connections now open)
2020-09-22T12:11:23.174+0800 I  NETWORK  [conn153] received client metadata from 211.162.81.126:22245 conn153: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:23.174+0800 I  NETWORK  [conn152] received client metadata from 211.162.81.126:11791 conn152: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:23.239+0800 I  NETWORK  [conn153] end connection 211.162.81.126:22245 (31 connections now open)
2020-09-22T12:11:23.239+0800 I  NETWORK  [conn152] end connection 211.162.81.126:11791 (30 connections now open)
2020-09-22T12:11:23.322+0800 I  NETWORK  [conn35] end connection 118.31.43.238:51286 (29 connections now open)
2020-09-22T12:11:23.779+0800 I  NETWORK  [conn31] end connection 47.96.16.32:54578 (28 connections now open)
2020-09-22T12:11:24.079+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:24.084+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:24.084+0800 I  REPL     [replexec-1] can't see a majority of the set, relinquishing primary
2020-09-22T12:11:24.084+0800 I  REPL     [replexec-1] Stepping down from primary in response to heartbeat
2020-09-22T12:11:24.084+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:24.084+0800 W  COMMAND  [conn53] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:24.084+0800 I  COMMAND  [conn53] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.00:27018:1600747823:-5900587996018832451" }, update: { $set: { ping: new Date(1600747883548) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747883, 1), signature: { hash: BinData(0, CEC3D69561D35553ADAA1E607A6A1848FFD01048), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747881, 3), t: 15 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:764 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 531ms
2020-09-22T12:11:24.085+0800 W  COMMAND  [conn54] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:24.085+0800 I  COMMAND  [conn54] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.01:27018:1600747823:-5358775017967458485" }, update: { $set: { ping: new Date(1600747883548) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747882, 20), signature: { hash: BinData(0, 02043B0D42D6BFE388ADBE0BB26A3743ECC1BCA5), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747881, 3), t: 15 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:764 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 535ms
2020-09-22T12:11:24.086+0800 W  COMMAND  [conn55] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:24.086+0800 I  COMMAND  [conn55] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.02:27018:1600747823:-4430192164990820571" }, update: { $set: { ping: new Date(1600747883554) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747882, 20), signature: { hash: BinData(0, 02043B0D42D6BFE388ADBE0BB26A3743ECC1BCA5), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747881, 3), t: 15 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:764 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 530ms
2020-09-22T12:11:24.086+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:24.086+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 3, userOpsRunning: 0 }
2020-09-22T12:11:24.086+0800 I  REPL     [replexec-1] transition to SECONDARY from PRIMARY
2020-09-22T12:11:24.087+0800 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: PrimarySteppedDown: Primary stepped down while waiting for replication
2020-09-22T12:11:24.087+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:11:24.790+0800 I  NETWORK  [conn25] end connection 47.96.5.198:36850 (27 connections now open)
2020-09-22T12:11:25.177+0800 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:11:25.576+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47758 #154 (28 connections now open)
2020-09-22T12:11:25.577+0800 I  NETWORK  [conn154] received client metadata from 120.55.192.104:47758 conn154: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:25.577+0800 I  ELECTION [conn154] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 17, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747881, 3), t: 15 } }
2020-09-22T12:11:25.577+0800 I  ELECTION [conn154] Sending vote response: { term: 17, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747881, 3), t: 15 }, my last applied OpTime: { ts: Timestam..." }
2020-09-22T12:11:25.583+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54370 #155 (29 connections now open)
2020-09-22T12:11:25.584+0800 I  NETWORK  [conn155] received client metadata from 112.124.21.191:54370 conn155: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:25.604+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:25.622+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47760 #156 (30 connections now open)
2020-09-22T12:11:25.622+0800 I  NETWORK  [conn156] received client metadata from 120.55.192.104:47760 conn156: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:25.643+0800 I  ELECTION [conn140] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 16, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747881, 3), t: 15 } }
2020-09-22T12:11:25.643+0800 I  ELECTION [conn140] Sending vote response: { term: 17, voteGranted: false, reason: "candidate's term (16) is lower than mine (17)" }
2020-09-22T12:11:25.643+0800 I  NETWORK  [conn140] end connection 112.124.21.191:54362 (29 connections now open)
2020-09-22T12:11:25.652+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22246 #157 (30 connections now open)
2020-09-22T12:11:25.653+0800 I  NETWORK  [conn157] received client metadata from 211.162.81.126:22246 conn157: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:25.653+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22247 #158 (31 connections now open)
2020-09-22T12:11:25.653+0800 I  NETWORK  [conn158] received client metadata from 211.162.81.126:22247 conn158: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:25.706+0800 I  NETWORK  [conn157] end connection 211.162.81.126:22246 (30 connections now open)
2020-09-22T12:11:25.707+0800 I  NETWORK  [conn158] end connection 211.162.81.126:22247 (29 connections now open)
2020-09-22T12:11:25.716+0800 I  NETWORK  [conn124] end connection 112.124.21.191:54358 (28 connections now open)
2020-09-22T12:11:25.802+0800 I  REPL     [replexec-4] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:26.088+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.089+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:11:26.090+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747883, 3), t: 16 }. source's GTE: { ts: Timestamp(1600747884, 3), t: 17 }
2020-09-22T12:11:26.090+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.090+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:26.090+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:26.090+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:26.090+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:11:26.090+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 155
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 154
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 150
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 139
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:26.090+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:26.091+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:26.091+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:26.091+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:26.091+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:26.091+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:26.091+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:26.094+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.095+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 3
2020-09-22T12:11:26.095+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:26.095+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-26.1.bson
2020-09-22T12:11:26.095+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:26.095+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:26.095+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:26.096+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:26.098+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747881, 3) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:26.099+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11792 #160 (29 connections now open)
2020-09-22T12:11:26.099+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:26.103+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22248 #161 (30 connections now open)
2020-09-22T12:11:26.103+0800 I  NETWORK  [conn160] received client metadata from 211.162.81.126:11792 conn160: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:26.104+0800 I  NETWORK  [conn161] received client metadata from 211.162.81.126:22248 conn161: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:26.107+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:26.107+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 191 records totaling to 42408 bytes
2020-09-22T12:11:26.107+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:26.107+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:26.109+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:26.110+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:26.121+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:26.121+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:26.121+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.121+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 3 update operations and 0 delete operations.
2020-09-22T12:11:26.121+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747882, 12), t: 16 }
2020-09-22T12:11:26.121+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747882, 12) }
2020-09-22T12:11:26.121+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747881, 3) (top of oplog: { ts: Timestamp(1600747881, 3), t: 15 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:26.090+0800
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:26.123+0800
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 3
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747883, 3), t: 16 }
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:23.555+0800
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:22.992+0800
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747882, 12)
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 		update: 3
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 4
2020-09-22T12:11:26.123+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:26.123+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:26.125+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:26.125+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:26.125+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:11:26.128+0800 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747883, 3), t: 16 }, latest oplog optime of sync source: { ts: Timestamp(1600747883, 3), t: 16 } (sync source does not know the primary)
2020-09-22T12:11:26.128+0800 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747883, 3), t: 16 }, its sync source index:-1
2020-09-22T12:11:26.128+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747883, 3), t: 16 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:26.128+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:11:26.128+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.129+0800 I  REPL     [rsBackgroundSync] Changed sync source from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.129+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source changed from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.130+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747883, 3), t: 16 }. source's GTE: { ts: Timestamp(1600747884, 3), t: 17 }
2020-09-22T12:11:26.130+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.130+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:26.130+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:26.130+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:26.130+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 31 }
2020-09-22T12:11:26.130+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 155
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 154
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 150
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 139
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:26.130+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:26.131+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:26.131+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:26.131+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:26.131+0800 I  NETWORK  [conn161] end connection 211.162.81.126:22248 (29 connections now open)
2020-09-22T12:11:26.131+0800 I  NETWORK  [conn160] end connection 211.162.81.126:11792 (28 connections now open)
2020-09-22T12:11:26.131+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:26.131+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:26.131+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:26.134+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.135+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 4
2020-09-22T12:11:26.135+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:26.135+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-26.2.bson
2020-09-22T12:11:26.135+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:26.135+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:26.135+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:26.135+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:26.203+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747881, 3) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:26.204+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:26.210+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:26.210+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 191 records totaling to 42408 bytes
2020-09-22T12:11:26.210+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:26.210+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:26.212+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:26.213+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:26.223+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:26.223+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:26.223+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.223+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 3 update operations and 0 delete operations.
2020-09-22T12:11:26.223+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747882, 12), t: 16 }
2020-09-22T12:11:26.223+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747882, 12) }
2020-09-22T12:11:26.223+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:26.224+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747881, 3) (top of oplog: { ts: Timestamp(1600747881, 3), t: 15 }, appliedThrough: { ts: Timestamp(1600747881, 3), t: 15 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:26.224+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.224+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:26.130+0800
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:26.225+0800
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 4
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747883, 3), t: 16 }
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:23.555+0800
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:22.992+0800
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747882, 12)
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 		update: 3
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 4
2020-09-22T12:11:26.225+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:26.225+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:26.226+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:26.226+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:26.229+0800 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747883, 3), t: 16 }, latest oplog optime of sync source: { ts: Timestamp(1600747883, 3), t: 16 } (sync source does not know the primary)
2020-09-22T12:11:26.229+0800 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747883, 3), t: 16 }, its sync source index:-1
2020-09-22T12:11:26.229+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747883, 3), t: 16 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:26.229+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:11:26.229+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.230+0800 I  REPL     [rsBackgroundSync] Changed sync source from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.230+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source changed from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.231+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747883, 3), t: 16 }. source's GTE: { ts: Timestamp(1600747884, 3), t: 17 }
2020-09-22T12:11:26.231+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.231+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:26.231+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:26.231+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:26.231+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:11:26.231+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 155
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 154
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 150
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 139
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:26.231+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:26.231+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:26.231+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:26.231+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:26.235+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.235+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 5
2020-09-22T12:11:26.235+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:26.235+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-26.3.bson
2020-09-22T12:11:26.237+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:26.237+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:26.237+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:26.237+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:26.309+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747881, 3) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:26.310+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:26.317+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:26.317+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 191 records totaling to 42408 bytes
2020-09-22T12:11:26.317+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:26.317+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:26.319+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:26.320+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:26.331+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:26.331+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:26.331+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.331+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 3 update operations and 0 delete operations.
2020-09-22T12:11:26.331+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747882, 12), t: 16 }
2020-09-22T12:11:26.332+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747882, 12) }
2020-09-22T12:11:26.332+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:26.333+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747881, 3) (top of oplog: { ts: Timestamp(1600747881, 3), t: 15 }, appliedThrough: { ts: Timestamp(1600747881, 3), t: 15 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:26.333+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.333+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:26.333+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:26.333+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:26.231+0800
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:26.333+0800
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 5
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747883, 3), t: 16 }
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:23.555+0800
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:22.992+0800
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747882, 12)
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 		update: 3
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 4
2020-09-22T12:11:26.333+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:26.333+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:26.334+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:26.334+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:26.334+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:26.334+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:26.334+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:26.335+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:26.335+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:26.338+0800 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747883, 3), t: 16 }, latest oplog optime of sync source: { ts: Timestamp(1600747883, 3), t: 16 } (sync source does not know the primary)
2020-09-22T12:11:26.338+0800 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747883, 3), t: 16 }, its sync source index:-1
2020-09-22T12:11:26.338+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747883, 3), t: 16 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:26.338+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:11:26.338+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.339+0800 I  REPL     [rsBackgroundSync] Changed sync source from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.339+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source changed from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.340+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747883, 3), t: 16 }. source's GTE: { ts: Timestamp(1600747884, 3), t: 17 }
2020-09-22T12:11:26.340+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.340+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:26.340+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:26.340+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:26.340+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:11:26.340+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 155
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 154
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 150
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 139
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:26.340+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:26.341+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:26.341+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:26.341+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:26.341+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:26.344+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.345+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 6
2020-09-22T12:11:26.345+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:26.345+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-26.4.bson
2020-09-22T12:11:26.345+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:26.345+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:26.345+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:26.345+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:26.412+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747881, 3) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:26.413+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:26.419+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:26.419+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 191 records totaling to 42408 bytes
2020-09-22T12:11:26.419+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:26.419+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:26.421+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:26.421+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:26.431+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:26.431+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:26.431+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.431+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 3 update operations and 0 delete operations.
2020-09-22T12:11:26.431+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747882, 12), t: 16 }
2020-09-22T12:11:26.431+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747882, 12) }
2020-09-22T12:11:26.431+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:26.432+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747881, 3) (top of oplog: { ts: Timestamp(1600747881, 3), t: 15 }, appliedThrough: { ts: Timestamp(1600747881, 3), t: 15 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:26.432+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:26.340+0800
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:26.433+0800
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 6
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747883, 3), t: 16 }
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:23.555+0800
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:22.992+0800
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747882, 12)
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 		update: 3
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 4
2020-09-22T12:11:26.433+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:26.433+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:26.435+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:26.435+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:26.438+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747883, 3), t: 16 }, latest oplog optime of sync source: { ts: Timestamp(1600747883, 3), t: 16 } (sync source does not know the primary)
2020-09-22T12:11:26.438+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747883, 3), t: 16 }, its sync source index:-1
2020-09-22T12:11:26.438+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747883, 3), t: 16 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:26.438+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:11:26.438+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.439+0800 I  REPL     [rsBackgroundSync] Changed sync source from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.439+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source changed from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.440+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747883, 3), t: 16 }. source's GTE: { ts: Timestamp(1600747884, 3), t: 17 }
2020-09-22T12:11:26.440+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.440+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:26.440+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:26.440+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:26.440+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:11:26.440+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 155
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 154
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 150
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 139
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:26.440+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:26.440+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:26.440+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:26.440+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:26.444+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.444+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11794 #167 (29 connections now open)
2020-09-22T12:11:26.444+0800 I  NETWORK  [conn167] received client metadata from 211.162.81.126:11794 conn167: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:26.445+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 7
2020-09-22T12:11:26.445+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:26.445+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-26.5.bson
2020-09-22T12:11:26.445+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:26.445+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:26.445+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:26.445+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:26.450+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11793 #168 (30 connections now open)
2020-09-22T12:11:26.450+0800 I  NETWORK  [conn168] received client metadata from 211.162.81.126:11793 conn168: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:26.488+0800 I  NETWORK  [conn167] end connection 211.162.81.126:11794 (29 connections now open)
2020-09-22T12:11:26.489+0800 I  NETWORK  [conn168] end connection 211.162.81.126:11793 (28 connections now open)
2020-09-22T12:11:26.517+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747881, 3) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:26.518+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:26.524+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:26.524+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 191 records totaling to 42408 bytes
2020-09-22T12:11:26.524+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:26.524+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:26.526+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:26.527+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:26.536+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:26.536+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:26.536+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.536+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 3 update operations and 0 delete operations.
2020-09-22T12:11:26.536+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747882, 12), t: 16 }
2020-09-22T12:11:26.537+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747882, 12) }
2020-09-22T12:11:26.537+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:26.538+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747881, 3) (top of oplog: { ts: Timestamp(1600747881, 3), t: 15 }, appliedThrough: { ts: Timestamp(1600747881, 3), t: 15 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:26.538+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.538+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:26.538+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:26.538+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:26.440+0800
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:26.538+0800
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 7
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747883, 3), t: 16 }
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:23.555+0800
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:22.992+0800
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747882, 12)
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:26.538+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 		update: 3
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 4
2020-09-22T12:11:26.539+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:26.539+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:26.539+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:26.539+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:26.539+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:26.539+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:26.539+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:26.540+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:26.540+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:26.543+0800 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747883, 3), t: 16 }, latest oplog optime of sync source: { ts: Timestamp(1600747883, 3), t: 16 } (sync source does not know the primary)
2020-09-22T12:11:26.543+0800 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747883, 3), t: 16 }, its sync source index:-1
2020-09-22T12:11:26.543+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747883, 3), t: 16 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:26.543+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:11:26.544+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.544+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:11:26.546+0800 I  REPL     [rsBackgroundSync] Changed sync source from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:26.546+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747883, 3), t: 16 }. source's GTE: { ts: Timestamp(1600747884, 3), t: 17 }
2020-09-22T12:11:26.546+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.547+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:26.547+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:26.547+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:26.547+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:11:26.547+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 155
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 154
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 150
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 139
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:26.547+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 15
2020-09-22T12:11:26.547+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:26.547+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:26.547+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:26.550+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.551+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 8
2020-09-22T12:11:26.551+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:26.551+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-26.6.bson
2020-09-22T12:11:26.551+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:26.551+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:26.551+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:26.551+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:26.625+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747881, 3) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:26.625+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:26.631+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:26.631+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 191 records totaling to 42408 bytes
2020-09-22T12:11:26.631+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:26.631+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:26.639+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:26.639+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:26.656+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:26.656+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:26.656+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.656+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 3 update operations and 0 delete operations.
2020-09-22T12:11:26.656+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747882, 12), t: 16 }
2020-09-22T12:11:26.656+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747882, 12) }
2020-09-22T12:11:26.656+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:26.657+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747881, 3) (top of oplog: { ts: Timestamp(1600747881, 3), t: 15 }, appliedThrough: { ts: Timestamp(1600747881, 3), t: 15 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:26.657+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.657+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:26.657+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:26.547+0800
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:26.658+0800
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 8
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747883, 3), t: 16 }
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747881, 3), t: 15 }
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:23.555+0800
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:22.992+0800
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747882, 12)
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747881, 3)
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 		update: 3
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 4
2020-09-22T12:11:26.658+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:26.658+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:26.659+0800 I  REPL     [replication-1] Blacklisting 120.55.192.104:27019 due to error: 'NotMasterOrSecondary: Oplog collection reads are not allowed while in the rollback or startup state.' for 10s until: 2020-09-22T12:11:36.659+0800
2020-09-22T12:11:26.659+0800 I  REPL     [replication-1] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:26.844+0800 I  NETWORK  [conn150] end connection 120.55.192.104:47750 (27 connections now open)
2020-09-22T12:11:27.524+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:11:28.105+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state ROLLBACK
2020-09-22T12:11:28.302+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:11:28.302+0800 I  ELECTION [replexec-1] Scheduling priority takeover at 2020-09-22T12:11:29.399+0800
2020-09-22T12:11:29.399+0800 I  REPL     [replexec-2] Canceling priority takeover callback
2020-09-22T12:11:29.399+0800 I  ELECTION [replexec-2] Starting an election for a priority takeover
2020-09-22T12:11:29.399+0800 I  ELECTION [replexec-2] conducting a dry run election to see if we could be elected. current term: 18
2020-09-22T12:11:29.399+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 420 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 18, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747887, 5), t: 18 } }
2020-09-22T12:11:29.399+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 421 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 18, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747887, 5), t: 18 } }
2020-09-22T12:11:29.400+0800 I  ELECTION [replexec-6] VoteRequester(term 18 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 18, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000f') }, lastCommittedOpTime: Timestamp(1600747887, 5), $clusterTime: { clusterTime: Timestamp(1600747888, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747887, 5) }
2020-09-22T12:11:29.400+0800 I  ELECTION [replexec-6] dry election run succeeded, running for election in term 19
2020-09-22T12:11:29.401+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 422 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 19, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747887, 5), t: 18 } }
2020-09-22T12:11:29.401+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 423 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 19, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747887, 5), t: 18 } }
2020-09-22T12:11:29.403+0800 I  ELECTION [replexec-2] VoteRequester(term 19) received a yes vote from 120.55.192.104:27019; response message: { term: 19, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000000f') }, lastCommittedOpTime: Timestamp(1600747887, 5), $clusterTime: { clusterTime: Timestamp(1600747888, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747887, 5) }
2020-09-22T12:11:29.403+0800 I  ELECTION [replexec-2] election succeeded, assuming primary role in term 19
2020-09-22T12:11:29.403+0800 I  REPL     [replexec-2] transition to PRIMARY from SECONDARY
2020-09-22T12:11:29.403+0800 I  REPL     [replexec-2] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:29.403+0800 I  REPL     [replexec-2] Entering primary catch-up mode.
2020-09-22T12:11:29.403+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:11:29.403+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:29.403+0800 I  REPL     [replexec-2] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:29.404+0800 I  REPL     [replexec-4] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:29.404+0800 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747887, 5), t: 18 }. My Last Applied: { ts: Timestamp(1600747887, 5), t: 18 }
2020-09-22T12:11:29.404+0800 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-09-22T12:11:29.404+0800 I  REPL     [replexec-4] Stopping replication producer
2020-09-22T12:11:29.405+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 19
2020-09-22T12:11:29.405+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:11:29.405+0800 I  CONNPOOL [RS] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:29.405+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:29.405+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:29.405+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:11:29.406+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:11:29.407+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:11:29.407+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:11:29.407+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:11:29.407+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:11:29.408+0800 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:11:29.410+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:11:29.520+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11796 #171 (28 connections now open)
2020-09-22T12:11:29.521+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11795 #172 (29 connections now open)
2020-09-22T12:11:29.521+0800 I  NETWORK  [conn171] received client metadata from 211.162.81.126:11796 conn171: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:29.521+0800 I  NETWORK  [conn172] received client metadata from 211.162.81.126:11795 conn172: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:29.561+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:11:29.585+0800 I  NETWORK  [conn172] end connection 211.162.81.126:11795 (28 connections now open)
2020-09-22T12:11:29.585+0800 I  NETWORK  [conn171] end connection 211.162.81.126:11796 (27 connections now open)
2020-09-22T12:11:30.404+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:30.520+0800 I  NETWORK  [listener] connection accepted from 47.96.5.198:36992 #173 (28 connections now open)
2020-09-22T12:11:30.520+0800 I  NETWORK  [conn173] received client metadata from 47.96.5.198:36992 conn173: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:30.643+0800 I  REPL     [replexec-3] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:30.643+0800 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-09-22T12:11:30.643+0800 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-09-22T12:11:30.643+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:30.643+0800 W  COMMAND  [conn26] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:30.643+0800 I  COMMAND  [conn26] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747889, 2), signature: { hash: BinData(0, 8AB894D2D1E60D9E94C00B89B2C42CF4CB7817D0), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747887, 5), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1083ms
2020-09-22T12:11:30.644+0800 W  COMMAND  [conn173] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:11:30.644+0800 I  COMMAND  [conn173] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747890, 2), signature: { hash: BinData(0, 292ABAAA8B3832AC42CB09CB50A5D02A90892744), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747887, 5), t: 18 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 121ms
2020-09-22T12:11:30.644+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:30.644+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 2, userOpsRunning: 1 }
2020-09-22T12:11:30.644+0800 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-09-22T12:11:30.645+0800 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: PrimarySteppedDown: Primary stepped down while waiting for replication
2020-09-22T12:11:30.645+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:11:30.650+0800 I  NETWORK  [conn154] end connection 120.55.192.104:47758 (27 connections now open)
2020-09-22T12:11:30.650+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47766 #174 (28 connections now open)
2020-09-22T12:11:30.650+0800 I  NETWORK  [conn174] received client metadata from 120.55.192.104:47766 conn174: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:30.662+0800 I  ELECTION [conn174] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 19, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747889, 2), t: 19 } }
2020-09-22T12:11:30.662+0800 I  ELECTION [conn174] Sending vote response: { term: 19, voteGranted: true, reason: "" }
2020-09-22T12:11:30.664+0800 I  ELECTION [conn174] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 20, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747889, 2), t: 19 } }
2020-09-22T12:11:30.664+0800 I  ELECTION [conn174] Sending vote response: { term: 20, voteGranted: true, reason: "" }
2020-09-22T12:11:30.666+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47772 #175 (29 connections now open)
2020-09-22T12:11:30.666+0800 I  NETWORK  [conn175] received client metadata from 120.55.192.104:47772 conn175: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:30.667+0800 I  NETWORK  [conn174] end connection 120.55.192.104:47766 (28 connections now open)
2020-09-22T12:11:31.216+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54366 #176 (29 connections now open)
2020-09-22T12:11:31.216+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54368 #177 (30 connections now open)
2020-09-22T12:11:31.216+0800 I  NETWORK  [conn177] received client metadata from 112.124.21.191:54368 conn177: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:31.216+0800 I  NETWORK  [conn176] received client metadata from 112.124.21.191:54366 conn176: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:31.403+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:11:31.403+0800 I  ELECTION [replexec-1] Scheduling priority takeover at 2020-09-22T12:11:32.527+0800
2020-09-22T12:11:31.404+0800 I  REPL     [replexec-6] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:31.761+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22249 #178 (31 connections now open)
2020-09-22T12:11:31.761+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11797 #179 (32 connections now open)
2020-09-22T12:11:31.762+0800 I  NETWORK  [conn178] received client metadata from 211.162.81.126:22249 conn178: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:31.762+0800 I  NETWORK  [conn179] received client metadata from 211.162.81.126:11797 conn179: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:31.822+0800 I  NETWORK  [conn178] end connection 211.162.81.126:22249 (31 connections now open)
2020-09-22T12:11:31.822+0800 I  NETWORK  [conn179] end connection 211.162.81.126:11797 (30 connections now open)
2020-09-22T12:11:32.527+0800 I  REPL     [replexec-1] Canceling priority takeover callback
2020-09-22T12:11:32.527+0800 I  ELECTION [replexec-1] Not starting an election for a priority takeover, since we are not electable due to: Not standing for election because member is not caught up enough to the most up-to-date member to call for priority takeover - must be within 2 seconds (mask 0x80)
2020-09-22T12:11:32.614+0800 I  ELECTION [replexec-3] Scheduling priority takeover at 2020-09-22T12:11:33.706+0800
2020-09-22T12:11:32.646+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:32.647+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:11:32.648+0800 I  CONNPOOL [RS] Connecting to 112.124.21.191:27019
2020-09-22T12:11:33.706+0800 I  REPL     [replexec-3] Canceling priority takeover callback
2020-09-22T12:11:33.706+0800 I  ELECTION [replexec-3] Starting an election for a priority takeover
2020-09-22T12:11:33.706+0800 I  ELECTION [replexec-3] conducting a dry run election to see if we could be elected. current term: 20
2020-09-22T12:11:33.706+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 457 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 20, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747892, 1), t: 20 } }
2020-09-22T12:11:33.706+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 458 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 20, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747892, 1), t: 20 } }
2020-09-22T12:11:33.710+0800 I  ELECTION [replexec-5] VoteRequester(term 20 dry run) received a yes vote from 112.124.21.191:27019; response message: { term: 20, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000012') }, lastCommittedOpTime: Timestamp(1600747892, 1), $clusterTime: { clusterTime: Timestamp(1600747892, 22), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747892, 1) }
2020-09-22T12:11:33.710+0800 I  ELECTION [replexec-5] dry election run succeeded, running for election in term 21
2020-09-22T12:11:33.711+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 459 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 21, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747892, 1), t: 20 } }
2020-09-22T12:11:33.711+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 460 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 21, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747892, 1), t: 20 } }
2020-09-22T12:11:33.721+0800 I  ELECTION [replexec-6] VoteRequester(term 21) received a yes vote from 112.124.21.191:27019; response message: { term: 21, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000012') }, lastCommittedOpTime: Timestamp(1600747892, 1), $clusterTime: { clusterTime: Timestamp(1600747892, 22), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747892, 1) }
2020-09-22T12:11:33.721+0800 I  ELECTION [replexec-6] election succeeded, assuming primary role in term 21
2020-09-22T12:11:33.721+0800 I  REPL     [replexec-6] transition to PRIMARY from SECONDARY
2020-09-22T12:11:33.721+0800 I  REPL     [replexec-6] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:33.721+0800 I  REPL     [replexec-6] Entering primary catch-up mode.
2020-09-22T12:11:33.724+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:11:33.724+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747892, 1), t: 20 }. My Last Applied: { ts: Timestamp(1600747892, 1), t: 20 }
2020-09-22T12:11:33.724+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-09-22T12:11:33.724+0800 I  REPL     [replexec-0] Stopping replication producer
2020-09-22T12:11:33.724+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 21
2020-09-22T12:11:33.724+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 21
2020-09-22T12:11:33.724+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:11:33.724+0800 I  CONNPOOL [RS] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:33.725+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:33.725+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:33.725+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:11:33.726+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:11:33.726+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:11:33.727+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:11:33.727+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:11:33.727+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:11:33.921+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:11:34.064+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11798 #181 (31 connections now open)
2020-09-22T12:11:34.064+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22250 #182 (32 connections now open)
2020-09-22T12:11:34.065+0800 I  NETWORK  [conn181] received client metadata from 211.162.81.126:11798 conn181: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:34.065+0800 I  NETWORK  [conn182] received client metadata from 211.162.81.126:22250 conn182: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:34.126+0800 I  NETWORK  [conn181] end connection 211.162.81.126:11798 (31 connections now open)
2020-09-22T12:11:34.126+0800 I  NETWORK  [conn182] end connection 211.162.81.126:22250 (30 connections now open)
2020-09-22T12:11:34.718+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:11:34.718+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:11:34.923+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54374 #183 (31 connections now open)
2020-09-22T12:11:34.923+0800 I  NETWORK  [conn183] received client metadata from 112.124.21.191:54374 conn183: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:35.906+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22251 #184 (32 connections now open)
2020-09-22T12:11:35.907+0800 I  NETWORK  [conn184] received client metadata from 211.162.81.126:22251 conn184: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:35.910+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11799 #185 (33 connections now open)
2020-09-22T12:11:35.911+0800 I  NETWORK  [conn185] received client metadata from 211.162.81.126:11799 conn185: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:35.979+0800 I  NETWORK  [conn184] end connection 211.162.81.126:22251 (32 connections now open)
2020-09-22T12:11:35.981+0800 I  NETWORK  [conn185] end connection 211.162.81.126:11799 (31 connections now open)
2020-09-22T12:11:36.671+0800 I  NETWORK  [conn139] end connection 112.124.21.191:54364 (30 connections now open)
2020-09-22T12:11:39.826+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11801 #186 (31 connections now open)
2020-09-22T12:11:39.827+0800 I  NETWORK  [conn186] received client metadata from 211.162.81.126:11801 conn186: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:39.830+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11800 #187 (32 connections now open)
2020-09-22T12:11:39.831+0800 I  NETWORK  [conn187] received client metadata from 211.162.81.126:11800 conn187: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:39.895+0800 I  NETWORK  [conn186] end connection 211.162.81.126:11801 (31 connections now open)
2020-09-22T12:11:39.895+0800 I  NETWORK  [conn187] end connection 211.162.81.126:11800 (30 connections now open)
2020-09-22T12:11:40.724+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:40.724+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:11:40.724+0800 I  REPL     [replexec-0] can't see a majority of the set, relinquishing primary
2020-09-22T12:11:40.724+0800 I  REPL     [replexec-0] Stepping down from primary in response to heartbeat
2020-09-22T12:11:40.724+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:40.724+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:40.724+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:11:40.724+0800 I  REPL     [replexec-0] transition to SECONDARY from PRIMARY
2020-09-22T12:11:40.725+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:11:41.734+0800 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:11:42.724+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:11:42.724+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:11:42.724+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:42.724+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:11:42.795+0800 I  ELECTION [replexec-6] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:11:43.823+0800 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:11:44.879+0800 I  NETWORK  [conn155] end connection 112.124.21.191:54370 (29 connections now open)
2020-09-22T12:11:44.898+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:11:44.926+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22252 #188 (30 connections now open)
2020-09-22T12:11:44.926+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11802 #189 (31 connections now open)
2020-09-22T12:11:44.926+0800 I  NETWORK  [conn188] received client metadata from 211.162.81.126:22252 conn188: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:44.927+0800 I  NETWORK  [conn189] received client metadata from 211.162.81.126:11802 conn189: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:44.987+0800 I  NETWORK  [conn189] end connection 211.162.81.126:11802 (30 connections now open)
2020-09-22T12:11:44.988+0800 I  NETWORK  [conn188] end connection 211.162.81.126:22252 (29 connections now open)
2020-09-22T12:11:45.074+0800 I  NETWORK  [conn15] end connection 120.55.192.104:47638 (28 connections now open)
2020-09-22T12:11:45.074+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47782 #190 (29 connections now open)
2020-09-22T12:11:45.075+0800 I  NETWORK  [conn190] received client metadata from 120.55.192.104:47782 conn190: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:45.168+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47778 #191 (30 connections now open)
2020-09-22T12:11:45.169+0800 I  NETWORK  [conn191] received client metadata from 120.55.192.104:47778 conn191: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:45.169+0800 I  ELECTION [conn191] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 23, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747897, 14), t: 21 } }
2020-09-22T12:11:45.169+0800 I  ELECTION [conn191] Sending vote response: { term: 23, voteGranted: true, reason: "" }
2020-09-22T12:11:45.171+0800 I  ELECTION [conn191] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 24, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747904, 1), t: 23 } }
2020-09-22T12:11:45.171+0800 I  ELECTION [conn191] Sending vote response: { term: 24, voteGranted: true, reason: "" }
2020-09-22T12:11:45.173+0800 I  NETWORK  [conn191] end connection 120.55.192.104:47778 (29 connections now open)
2020-09-22T12:11:45.362+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47780 #192 (30 connections now open)
2020-09-22T12:11:45.363+0800 I  NETWORK  [conn192] received client metadata from 120.55.192.104:47780 conn192: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:45.751+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11803 #195 (31 connections now open)
2020-09-22T12:11:45.751+0800 I  NETWORK  [conn195] received client metadata from 211.162.81.126:11803 conn195: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:45.752+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22253 #196 (32 connections now open)
2020-09-22T12:11:45.752+0800 I  NETWORK  [conn196] received client metadata from 211.162.81.126:22253 conn196: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:45.811+0800 I  NETWORK  [conn195] end connection 211.162.81.126:11803 (31 connections now open)
2020-09-22T12:11:45.811+0800 I  NETWORK  [conn196] end connection 211.162.81.126:22253 (30 connections now open)
2020-09-22T12:11:46.224+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:11:46.224+0800 I  ELECTION [replexec-0] Scheduling priority takeover at 2020-09-22T12:11:47.345+0800
2020-09-22T12:11:46.225+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:46.244+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54380 #197 (31 connections now open)
2020-09-22T12:11:46.244+0800 I  NETWORK  [conn197] received client metadata from 112.124.21.191:54380 conn197: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:46.349+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22255 #198 (32 connections now open)
2020-09-22T12:11:46.349+0800 I  NETWORK  [conn198] received client metadata from 211.162.81.126:22255 conn198: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:46.350+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22254 #199 (33 connections now open)
2020-09-22T12:11:46.350+0800 I  NETWORK  [conn199] received client metadata from 211.162.81.126:22254 conn199: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:46.404+0800 I  NETWORK  [conn198] end connection 211.162.81.126:22255 (32 connections now open)
2020-09-22T12:11:46.404+0800 I  NETWORK  [conn199] end connection 211.162.81.126:22254 (31 connections now open)
2020-09-22T12:11:46.726+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:46.727+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:11:46.728+0800 I  CONNPOOL [RS] Connecting to 112.124.21.191:27019
2020-09-22T12:11:46.959+0800 I  ELECTION [conn176] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 21, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747897, 14), t: 21 } }
2020-09-22T12:11:46.959+0800 I  ELECTION [conn176] Sending vote response: { term: 24, voteGranted: false, reason: "candidate's term (21) is lower than mine (24)" }
2020-09-22T12:11:46.960+0800 I  ELECTION [conn177] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 22, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747897, 14), t: 21 } }
2020-09-22T12:11:46.960+0800 I  ELECTION [conn177] Sending vote response: { term: 24, voteGranted: false, reason: "candidate's term (22) is lower than mine (24)" }
2020-09-22T12:11:46.960+0800 I  NETWORK  [conn176] end connection 112.124.21.191:54366 (30 connections now open)
2020-09-22T12:11:46.960+0800 I  NETWORK  [conn177] end connection 112.124.21.191:54368 (29 connections now open)
2020-09-22T12:11:47.290+0800 I  ELECTION [conn197] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 24, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747905, 30), t: 24 } }
2020-09-22T12:11:47.290+0800 I  ELECTION [conn197] Sending vote response: { term: 24, voteGranted: true, reason: "" }
2020-09-22T12:11:47.292+0800 I  REPL     [conn197] Canceling priority takeover callback
2020-09-22T12:11:47.292+0800 I  ELECTION [conn197] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 25, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747905, 30), t: 24 } }
2020-09-22T12:11:47.292+0800 I  ELECTION [conn197] Sending vote response: { term: 25, voteGranted: true, reason: "" }
2020-09-22T12:11:47.632+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54378 #201 (30 connections now open)
2020-09-22T12:11:47.632+0800 I  NETWORK  [conn201] received client metadata from 112.124.21.191:54378 conn201: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:47.724+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:11:47.724+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:47.724+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state RS_DOWN - Request 480 timed out, deadline was 2020-09-22T12:11:47.724+0800, op was RemoteCommand 480 -- target:[120.55.192.104:27019] db:admin expDate:2020-09-22T12:11:47.724+0800 cmd:{ replSetHeartbeat: "rs_config", configVersion: 1, hbv: 1, from: "120.55.194.98:27019", fromId: 0, term: 24 }
2020-09-22T12:11:48.416+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22257 #202 (31 connections now open)
2020-09-22T12:11:48.417+0800 I  NETWORK  [conn202] received client metadata from 211.162.81.126:22257 conn202: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:48.425+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22256 #203 (32 connections now open)
2020-09-22T12:11:48.425+0800 I  NETWORK  [conn203] received client metadata from 211.162.81.126:22256 conn203: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:48.481+0800 I  NETWORK  [conn202] end connection 211.162.81.126:22257 (31 connections now open)
2020-09-22T12:11:48.484+0800 I  NETWORK  [conn203] end connection 211.162.81.126:22256 (30 connections now open)
2020-09-22T12:11:49.309+0800 I  ELECTION [conn192] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 25, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747908, 56), t: 25 } }
2020-09-22T12:11:49.309+0800 I  ELECTION [conn192] Sending vote response: { term: 25, voteGranted: true, reason: "" }
2020-09-22T12:11:49.311+0800 I  ELECTION [conn192] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 26, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747908, 56), t: 25 } }
2020-09-22T12:11:49.311+0800 I  ELECTION [conn192] Sending vote response: { term: 26, voteGranted: true, reason: "" }
2020-09-22T12:11:49.724+0800 I  REPL     [replexec-5] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:11:49.725+0800 I  ELECTION [replexec-5] Scheduling priority takeover at 2020-09-22T12:11:50.785+0800
2020-09-22T12:11:49.754+0800 I  NETWORK  [conn190] end connection 120.55.192.104:47782 (29 connections now open)
2020-09-22T12:11:49.773+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22258 #205 (30 connections now open)
2020-09-22T12:11:49.773+0800 I  NETWORK  [conn205] received client metadata from 211.162.81.126:22258 conn205: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:49.776+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11804 #206 (31 connections now open)
2020-09-22T12:11:49.777+0800 I  NETWORK  [conn206] received client metadata from 211.162.81.126:11804 conn206: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:49.832+0800 I  NETWORK  [conn205] end connection 211.162.81.126:22258 (30 connections now open)
2020-09-22T12:11:49.833+0800 I  NETWORK  [conn206] end connection 211.162.81.126:11804 (29 connections now open)
2020-09-22T12:11:50.404+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 112.124.21.191:27019, my last fetched oplog optime: { ts: Timestamp(1600747908, 56), t: 25 }, latest oplog optime of sync source: { ts: Timestamp(1600747908, 56), t: 25 } (sync source does not know the primary)
2020-09-22T12:11:50.404+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 112.124.21.191:27019, OpTime { ts: Timestamp(1600747908, 56), t: 25 }, its sync source index:-1
2020-09-22T12:11:50.404+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 112.124.21.191:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747908, 56), t: 25 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:50.405+0800 I  REPL     [rsBackgroundSync] Clearing sync source 112.124.21.191:27019 to choose a new one.
2020-09-22T12:11:50.405+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:50.422+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:11:50.728+0800 I  ELECTION [conn201] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 26, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747908, 56), t: 25 } }
2020-09-22T12:11:50.728+0800 I  ELECTION [conn201] Sending vote response: { term: 26, voteGranted: true, reason: "" }
2020-09-22T12:11:50.731+0800 I  REPL     [conn201] Canceling priority takeover callback
2020-09-22T12:11:50.731+0800 I  ELECTION [conn201] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 27, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747908, 56), t: 25 } }
2020-09-22T12:11:50.731+0800 I  ELECTION [conn201] Sending vote response: { term: 27, voteGranted: true, reason: "" }
2020-09-22T12:11:51.630+0800 I  REPL     [replexec-3] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:11:51.631+0800 I  ELECTION [replexec-3] Scheduling priority takeover at 2020-09-22T12:11:52.776+0800
2020-09-22T12:11:52.670+0800 I  REPL     [replexec-1] Canceling priority takeover callback
2020-09-22T12:11:52.670+0800 I  ELECTION [replexec-1] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-09-22T12:11:52.670+0800 I  ELECTION [replexec-1] conducting a dry run election to see if we could be elected. current term: 27
2020-09-22T12:11:52.670+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 516 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 27, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747908, 56), t: 25 } }
2020-09-22T12:11:52.670+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 517 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 27, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747908, 56), t: 25 } }
2020-09-22T12:11:52.670+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:52.670+0800 I  ELECTION [replexec-3] VoteRequester(term 27 dry run) received a no vote from 112.124.21.191:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747908, 56), t: 25 }, my last applied OpTime: { ts: Timestamp(1600747911, 15), t: 27 }"; response message: { term: 27, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747908, 56), t: 25 }, my last applied OpTime: { ts: Timesta...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747908, 56), $clusterTime: { clusterTime: Timestamp(1600747911, 15), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747911, 15) }
2020-09-22T12:11:52.672+0800 I  ELECTION [replexec-2] VoteRequester(term 27 dry run) received a no vote from 120.55.192.104:27019 with reason "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747908, 56), t: 25 }, my last applied OpTime: { ts: Timestamp(1600747910, 2), t: 26 }"; response message: { term: 27, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747908, 56), t: 25 }, my last applied OpTime: { ts: Timesta...", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001a') }, lastCommittedOpTime: Timestamp(1600747908, 56), $clusterTime: { clusterTime: Timestamp(1600747911, 14), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747910, 2) }
2020-09-22T12:11:52.672+0800 I  ELECTION [replexec-2] not running for primary, we received insufficient votes
2020-09-22T12:11:52.672+0800 I  ELECTION [replexec-2] Lost dry run election due to internal error
2020-09-22T12:11:52.724+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:52.724+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state RS_DOWN - Request 515 timed out, deadline was 2020-09-22T12:11:52.724+0800, op was RemoteCommand 515 -- target:[120.55.192.104:27019] db:admin expDate:2020-09-22T12:11:52.724+0800 cmd:{ replSetHeartbeat: "rs_config", configVersion: 1, hbv: 1, from: "120.55.194.98:27019", fromId: 0, term: 27 }
2020-09-22T12:11:52.901+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11805 #208 (30 connections now open)
2020-09-22T12:11:52.902+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11806 #209 (31 connections now open)
2020-09-22T12:11:52.902+0800 I  NETWORK  [conn208] received client metadata from 211.162.81.126:11805 conn208: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:52.902+0800 I  NETWORK  [conn209] received client metadata from 211.162.81.126:11806 conn209: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:52.944+0800 I  NETWORK  [conn192] end connection 120.55.192.104:47780 (30 connections now open)
2020-09-22T12:11:52.964+0800 I  NETWORK  [conn209] end connection 211.162.81.126:11806 (29 connections now open)
2020-09-22T12:11:52.965+0800 I  NETWORK  [conn208] end connection 211.162.81.126:11805 (28 connections now open)
2020-09-22T12:11:53.312+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47796 #210 (29 connections now open)
2020-09-22T12:11:53.312+0800 I  NETWORK  [conn210] received client metadata from 120.55.192.104:47796 conn210: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:53.670+0800 I  REPL     [rsBackgroundSync] Changed sync source from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:53.673+0800 I  REPL     [replication-1] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 120.55.192.104:27019, my last fetched oplog optime: { ts: Timestamp(1600747910, 2), t: 26 }, latest oplog optime of sync source: { ts: Timestamp(1600747910, 2), t: 26 } (sync source does not know the primary)
2020-09-22T12:11:53.673+0800 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 120.55.192.104:27019, OpTime { ts: Timestamp(1600747910, 2), t: 26 }, its sync source index:-1
2020-09-22T12:11:53.673+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 120.55.192.104:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747910, 2), t: 26 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:11:53.674+0800 I  REPL     [rsBackgroundSync] Clearing sync source 120.55.192.104:27019 to choose a new one.
2020-09-22T12:11:53.674+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:11:53.674+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:11:53.759+0800 I  ELECTION [conn210] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 27, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747910, 2), t: 26 } }
2020-09-22T12:11:53.759+0800 I  ELECTION [conn210] Sending vote response: { term: 27, voteGranted: true, reason: "" }
2020-09-22T12:11:53.761+0800 I  ELECTION [conn210] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 28, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747910, 2), t: 26 } }
2020-09-22T12:11:53.761+0800 I  ELECTION [conn210] Sending vote response: { term: 28, voteGranted: true, reason: "" }
2020-09-22T12:11:54.235+0800 I  NETWORK  [conn201] end connection 112.124.21.191:54378 (28 connections now open)
2020-09-22T12:11:54.248+0800 I  NETWORK  [conn175] end connection 120.55.192.104:47772 (27 connections now open)
2020-09-22T12:11:54.415+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11807 #211 (28 connections now open)
2020-09-22T12:11:54.415+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22259 #212 (29 connections now open)
2020-09-22T12:11:54.415+0800 I  NETWORK  [conn212] received client metadata from 211.162.81.126:22259 conn212: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:54.415+0800 I  NETWORK  [conn211] received client metadata from 211.162.81.126:11807 conn211: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:54.446+0800 I  REPL     [replexec-2] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:11:54.473+0800 I  NETWORK  [conn212] end connection 211.162.81.126:22259 (28 connections now open)
2020-09-22T12:11:54.474+0800 I  NETWORK  [conn211] end connection 211.162.81.126:11807 (27 connections now open)
2020-09-22T12:11:54.535+0800 I  ELECTION [conn197] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 27, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747911, 15), t: 27 } }
2020-09-22T12:11:54.535+0800 I  ELECTION [conn197] Sending vote response: { term: 28, voteGranted: false, reason: "candidate's term (27) is lower than mine (28)" }
2020-09-22T12:11:54.725+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:11:54.725+0800 I  ELECTION [replexec-0] Scheduling priority takeover at 2020-09-22T12:11:55.784+0800
2020-09-22T12:11:55.246+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22415 #213 (28 connections now open)
2020-09-22T12:11:55.246+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11808 #214 (29 connections now open)
2020-09-22T12:11:55.246+0800 I  NETWORK  [conn214] received client metadata from 211.162.81.126:11808 conn214: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:55.247+0800 I  NETWORK  [conn213] received client metadata from 211.162.81.126:22415 conn213: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:55.301+0800 I  NETWORK  [conn214] end connection 211.162.81.126:11808 (28 connections now open)
2020-09-22T12:11:55.302+0800 I  NETWORK  [conn213] end connection 211.162.81.126:22415 (27 connections now open)
2020-09-22T12:11:55.410+0800 I  REPL     [rsBackgroundSync] Changed sync source from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:11:55.414+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747910, 2), t: 26 }. source's GTE: { ts: Timestamp(1600747911, 1), t: 27 }
2020-09-22T12:11:55.414+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747908, 56), t: 25 }
2020-09-22T12:11:55.415+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:11:55.415+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 112.124.21.191:27019)
2020-09-22T12:11:55.415+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:11:55.415+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 28 }
2020-09-22T12:11:55.415+0800 I  REPL     [rsBackgroundSync] Canceling priority takeover callback
2020-09-22T12:11:55.415+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 210
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 197
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 183
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 173
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 156
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 104
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 101
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:11:55.415+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:11:55.415+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:11:55.415+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:11:55.415+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:11:55.419+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747908, 56), t: 25 }
2020-09-22T12:11:55.420+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 9
2020-09-22T12:11:55.420+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:11:55.420+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-11-55.7.bson
2020-09-22T12:11:55.421+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:11:55.421+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:11:55.421+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:11:55.421+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:11:55.467+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747908, 56) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:11:55.468+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:11:55.473+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:11:55.473+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 221 records totaling to 48001 bytes
2020-09-22T12:11:55.473+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:11:55.474+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:11:55.476+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:11:55.476+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:11:55.485+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:11:55.485+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:11:55.485+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747908, 56)
2020-09-22T12:11:55.485+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 6 update operations and 0 delete operations.
2020-09-22T12:11:55.485+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747909, 10), t: 26 }
2020-09-22T12:11:55.485+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747909, 10) }
2020-09-22T12:11:55.486+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747908, 56) (top of oplog: { ts: Timestamp(1600747908, 56), t: 25 }, appliedThrough: { ts: Timestamp(1600747908, 56), t: 25 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747908, 56)
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:11:55.415+0800
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:11:55.487+0800
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 112.124.21.191:27019
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.lockpings
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 9
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747910, 2), t: 26 }
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747908, 56), t: 25 }
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:11:50.393+0800
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:11:49.674+0800
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 0 second(s)
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747909, 10)
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747908, 56)
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 		update: 6
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 7
2020-09-22T12:11:55.487+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:11:55.487+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:11:55.691+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:11:55.691+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:11:56.219+0800 I  ELECTION [conn197] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 28, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747911, 15), t: 27 } }
2020-09-22T12:11:56.219+0800 I  ELECTION [conn197] Sending vote response: { term: 28, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747911, 15), t: 27 }, my last applied OpTime: { ts: Timesta..." }
2020-09-22T12:11:56.517+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22416 #216 (28 connections now open)
2020-09-22T12:11:56.518+0800 I  NETWORK  [conn216] received client metadata from 211.162.81.126:22416 conn216: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:56.523+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22417 #217 (29 connections now open)
2020-09-22T12:11:56.524+0800 I  NETWORK  [conn217] received client metadata from 211.162.81.126:22417 conn217: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:11:56.579+0800 I  NETWORK  [conn216] end connection 211.162.81.126:22416 (28 connections now open)
2020-09-22T12:11:56.579+0800 I  NETWORK  [conn217] end connection 211.162.81.126:22417 (27 connections now open)
2020-09-22T12:11:56.638+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54402 #218 (28 connections now open)
2020-09-22T12:11:56.639+0800 I  NETWORK  [conn218] received client metadata from 112.124.21.191:54402 conn218: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:56.639+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54404 #219 (29 connections now open)
2020-09-22T12:11:56.639+0800 I  NETWORK  [conn219] received client metadata from 112.124.21.191:54404 conn219: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:56.641+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54406 #220 (30 connections now open)
2020-09-22T12:11:56.641+0800 I  NETWORK  [conn220] received client metadata from 112.124.21.191:54406 conn220: { driver: { name: "MongoDB Internal Client", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:56.689+0800 I  NETWORK  [conn220] end connection 112.124.21.191:54406 (29 connections now open)
2020-09-22T12:11:57.242+0800 I  ELECTION [replexec-5] Starting an election, since we've seen no PRIMARY in the past 1000ms
2020-09-22T12:11:57.242+0800 I  ELECTION [replexec-5] conducting a dry run election to see if we could be elected. current term: 28
2020-09-22T12:11:57.242+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 546 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 28, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747914, 1), t: 28 } }
2020-09-22T12:11:57.242+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 547 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 28, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747914, 1), t: 28 } }
2020-09-22T12:11:57.242+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:11:57.243+0800 I  ELECTION [replexec-3] VoteRequester(term 28 dry run) received a yes vote from 112.124.21.191:27019; response message: { term: 28, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747914, 1), $clusterTime: { clusterTime: Timestamp(1600747915, 21), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747914, 1) }
2020-09-22T12:11:57.243+0800 I  ELECTION [replexec-3] dry election run succeeded, running for election in term 29
2020-09-22T12:11:57.244+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 548 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 29, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747914, 1), t: 28 } }
2020-09-22T12:11:57.244+0800 I  REPL     [replexec-3] Scheduling remote command request for vote request: RemoteCommand 549 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 29, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747914, 1), t: 28 } }
2020-09-22T12:11:57.246+0800 I  ELECTION [replexec-1] VoteRequester(term 29) received a yes vote from 112.124.21.191:27019; response message: { term: 29, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747914, 1), $clusterTime: { clusterTime: Timestamp(1600747915, 21), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747914, 1) }
2020-09-22T12:11:57.246+0800 I  ELECTION [replexec-1] election succeeded, assuming primary role in term 29
2020-09-22T12:11:57.246+0800 I  REPL     [replexec-1] transition to PRIMARY from SECONDARY
2020-09-22T12:11:57.246+0800 I  REPL     [replexec-1] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:11:57.246+0800 I  REPL     [replexec-1] Entering primary catch-up mode.
2020-09-22T12:11:57.246+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 2 connections to that host remain open
2020-09-22T12:11:57.725+0800 I  REPL     [replexec-6] Member 120.55.192.104:27019 is now in state RS_DOWN - Couldn't get a connection within the time limit
2020-09-22T12:11:57.725+0800 I  REPL     [replexec-6] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747914, 1), t: 28 }. My Last Applied: { ts: Timestamp(1600747914, 1), t: 28 }
2020-09-22T12:11:57.725+0800 I  REPL     [replexec-6] Exited primary catch-up mode.
2020-09-22T12:11:57.725+0800 I  REPL     [replexec-6] Stopping replication producer
2020-09-22T12:11:57.725+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:11:57.725+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:11:57.725+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 29
2020-09-22T12:11:57.725+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 29
2020-09-22T12:11:57.725+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:11:57.725+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:11:57.725+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-09-22T12:11:57.726+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:11:57.727+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:11:57.728+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:11:57.728+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:11:57.728+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:11:57.728+0800 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:11:57.732+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:11:57.732+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:11:57.932+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:11:58.763+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47798 #221 (30 connections now open)
2020-09-22T12:11:58.764+0800 I  NETWORK  [conn210] end connection 120.55.192.104:47796 (29 connections now open)
2020-09-22T12:11:58.764+0800 I  NETWORK  [conn221] received client metadata from 120.55.192.104:47798 conn221: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:58.970+0800 I  NETWORK  [conn101] end connection 112.124.21.191:54348 (28 connections now open)
2020-09-22T12:11:59.644+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47800 #222 (29 connections now open)
2020-09-22T12:11:59.644+0800 I  NETWORK  [conn222] received client metadata from 120.55.192.104:47800 conn222: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:11:59.954+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:12:00.247+0800 I  REPL     [replexec-3] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:00.258+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11809 #225 (30 connections now open)
2020-09-22T12:12:00.258+0800 I  NETWORK  [conn225] received client metadata from 211.162.81.126:11809 conn225: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:00.260+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22418 #226 (31 connections now open)
2020-09-22T12:12:00.261+0800 I  NETWORK  [conn226] received client metadata from 211.162.81.126:22418 conn226: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:00.328+0800 I  NETWORK  [conn225] end connection 211.162.81.126:11809 (30 connections now open)
2020-09-22T12:12:00.329+0800 I  NETWORK  [conn226] end connection 211.162.81.126:22418 (29 connections now open)
2020-09-22T12:12:00.851+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22419 #227 (30 connections now open)
2020-09-22T12:12:00.851+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22420 #228 (31 connections now open)
2020-09-22T12:12:00.852+0800 I  NETWORK  [conn227] received client metadata from 211.162.81.126:22419 conn227: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:00.852+0800 I  NETWORK  [conn228] received client metadata from 211.162.81.126:22420 conn228: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:00.911+0800 I  NETWORK  [conn227] end connection 211.162.81.126:22419 (30 connections now open)
2020-09-22T12:12:00.912+0800 I  NETWORK  [conn228] end connection 211.162.81.126:22420 (29 connections now open)
2020-09-22T12:12:01.662+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11810 #229 (30 connections now open)
2020-09-22T12:12:01.663+0800 I  NETWORK  [conn229] received client metadata from 211.162.81.126:11810 conn229: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:01.663+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22421 #230 (31 connections now open)
2020-09-22T12:12:01.664+0800 I  NETWORK  [conn230] received client metadata from 211.162.81.126:22421 conn230: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:01.726+0800 I  NETWORK  [conn230] end connection 211.162.81.126:22421 (30 connections now open)
2020-09-22T12:12:01.728+0800 I  NETWORK  [conn229] end connection 211.162.81.126:11810 (29 connections now open)
2020-09-22T12:12:04.089+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11811 #231 (30 connections now open)
2020-09-22T12:12:04.089+0800 I  NETWORK  [conn231] received client metadata from 211.162.81.126:11811 conn231: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:04.090+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22422 #232 (31 connections now open)
2020-09-22T12:12:04.091+0800 I  NETWORK  [conn232] received client metadata from 211.162.81.126:22422 conn232: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:04.142+0800 I  NETWORK  [conn231] end connection 211.162.81.126:11811 (30 connections now open)
2020-09-22T12:12:04.143+0800 I  NETWORK  [conn232] end connection 211.162.81.126:22422 (29 connections now open)
2020-09-22T12:12:04.901+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22423 #233 (30 connections now open)
2020-09-22T12:12:04.901+0800 I  NETWORK  [conn233] received client metadata from 211.162.81.126:22423 conn233: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:04.903+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11812 #234 (31 connections now open)
2020-09-22T12:12:04.904+0800 I  NETWORK  [conn234] received client metadata from 211.162.81.126:11812 conn234: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:04.954+0800 I  NETWORK  [conn233] end connection 211.162.81.126:22423 (30 connections now open)
2020-09-22T12:12:04.954+0800 I  NETWORK  [conn234] end connection 211.162.81.126:11812 (29 connections now open)
2020-09-22T12:12:06.080+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:06.080+0800 I  REPL     [replexec-1] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:06.080+0800 I  REPL     [replexec-1] can't see a majority of the set, relinquishing primary
2020-09-22T12:12:06.080+0800 I  REPL     [replexec-1] Stepping down from primary in response to heartbeat
2020-09-22T12:12:06.080+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:06.080+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:06.080+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:12:06.080+0800 I  REPL     [replexec-1] transition to SECONDARY from PRIMARY
2020-09-22T12:12:06.081+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:12:06.247+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:12:06.247+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:12:07.128+0800 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:07.247+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:08.261+0800 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:08.424+0800 I  NETWORK  [conn156] end connection 120.55.192.104:47760 (28 connections now open)
2020-09-22T12:12:08.562+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11968 #235 (29 connections now open)
2020-09-22T12:12:08.562+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22424 #236 (30 connections now open)
2020-09-22T12:12:08.562+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:12:08.562+0800 I  ELECTION [replexec-0] Scheduling priority takeover at 2020-09-22T12:12:09.588+0800
2020-09-22T12:12:08.562+0800 I  NETWORK  [conn235] received client metadata from 211.162.81.126:11968 conn235: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:08.563+0800 I  NETWORK  [conn236] received client metadata from 211.162.81.126:22424 conn236: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:08.620+0800 I  NETWORK  [conn236] end connection 211.162.81.126:22424 (29 connections now open)
2020-09-22T12:12:08.620+0800 I  NETWORK  [conn235] end connection 211.162.81.126:11968 (28 connections now open)
2020-09-22T12:12:09.032+0800 I  NETWORK  [conn221] end connection 120.55.192.104:47798 (27 connections now open)
2020-09-22T12:12:09.082+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:12:09.083+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:12:09.083+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:12:09.156+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47802 #238 (28 connections now open)
2020-09-22T12:12:09.157+0800 I  NETWORK  [conn238] received client metadata from 120.55.192.104:47802 conn238: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:09.160+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47804 #239 (29 connections now open)
2020-09-22T12:12:09.160+0800 I  NETWORK  [conn239] received client metadata from 120.55.192.104:47804 conn239: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:09.187+0800 I  NETWORK  [conn104] end connection 112.124.21.191:54344 (28 connections now open)
2020-09-22T12:12:09.588+0800 I  REPL     [replexec-5] Canceling priority takeover callback
2020-09-22T12:12:09.588+0800 I  ELECTION [replexec-5] Starting an election for a priority takeover
2020-09-22T12:12:09.588+0800 I  ELECTION [replexec-5] conducting a dry run election to see if we could be elected. current term: 31
2020-09-22T12:12:09.588+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 590 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 31, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747929, 1), t: 31 } }
2020-09-22T12:12:09.588+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 591 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 31, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747929, 1), t: 31 } }
2020-09-22T12:12:09.589+0800 I  ELECTION [replexec-6] VoteRequester(term 31 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 31, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001f') }, lastCommittedOpTime: Timestamp(1600747929, 1), $clusterTime: { clusterTime: Timestamp(1600747929, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747929, 1) }
2020-09-22T12:12:09.589+0800 I  ELECTION [replexec-6] dry election run succeeded, running for election in term 32
2020-09-22T12:12:09.592+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 592 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 32, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747929, 1), t: 31 } }
2020-09-22T12:12:09.592+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 593 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 32, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747929, 1), t: 31 } }
2020-09-22T12:12:09.594+0800 I  ELECTION [replexec-3] VoteRequester(term 32) received a yes vote from 112.124.21.191:27019; response message: { term: 32, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747929, 1), $clusterTime: { clusterTime: Timestamp(1600747929, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747929, 1) }
2020-09-22T12:12:09.594+0800 I  ELECTION [replexec-3] election succeeded, assuming primary role in term 32
2020-09-22T12:12:09.594+0800 I  REPL     [replexec-3] transition to PRIMARY from SECONDARY
2020-09-22T12:12:09.594+0800 I  REPL     [replexec-3] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:12:09.594+0800 I  REPL     [replexec-3] Entering primary catch-up mode.
2020-09-22T12:12:09.595+0800 I  REPL     [replexec-0] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:12:09.595+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:09.595+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747929, 1), t: 31 }. My Last Applied: { ts: Timestamp(1600747929, 1), t: 31 }
2020-09-22T12:12:09.595+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-09-22T12:12:09.595+0800 I  REPL     [replexec-0] Stopping replication producer
2020-09-22T12:12:09.595+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 32
2020-09-22T12:12:09.595+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 32
2020-09-22T12:12:09.596+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:12:09.596+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:09.596+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:09.596+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:09.596+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-09-22T12:12:09.597+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:12:09.598+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:12:09.599+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:12:09.599+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:12:09.599+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:12:09.600+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:12:09.600+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:12:09.657+0800 I  NETWORK  [listener] connection accepted from 118.31.43.238:51444 #241 (29 connections now open)
2020-09-22T12:12:09.658+0800 I  NETWORK  [conn241] received client metadata from 118.31.43.238:51444 conn241: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:09.667+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:12:09.907+0800 I  NETWORK  [conn197] end connection 112.124.21.191:54380 (28 connections now open)
2020-09-22T12:12:10.068+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22425 #242 (29 connections now open)
2020-09-22T12:12:10.068+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11969 #243 (30 connections now open)
2020-09-22T12:12:10.068+0800 I  NETWORK  [conn242] received client metadata from 211.162.81.126:22425 conn242: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:10.068+0800 I  NETWORK  [conn243] received client metadata from 211.162.81.126:11969 conn243: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:10.129+0800 I  NETWORK  [conn242] end connection 211.162.81.126:22425 (29 connections now open)
2020-09-22T12:12:10.130+0800 I  NETWORK  [conn243] end connection 211.162.81.126:11969 (28 connections now open)
2020-09-22T12:12:10.666+0800 I  ELECTION [conn239] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 32, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 33), t: 32 } }
2020-09-22T12:12:10.666+0800 I  ELECTION [conn239] Sending vote response: { term: 32, voteGranted: true, reason: "" }
2020-09-22T12:12:10.668+0800 I  REPL     [conn239] stepping down from primary, because a new term has begun: 33
2020-09-22T12:12:10.668+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:10.668+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:10.668+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 0, userOpsRunning: 2 }
2020-09-22T12:12:10.668+0800 I  REPL     [replexec-2] transition to SECONDARY from PRIMARY
2020-09-22T12:12:10.669+0800 I  ELECTION [conn239] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 33, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 33), t: 32 } }
2020-09-22T12:12:10.669+0800 I  ELECTION [conn239] Sending vote response: { term: 33, voteGranted: true, reason: "" }
2020-09-22T12:12:10.669+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:12:10.670+0800 I  NETWORK  [conn239] end connection 120.55.192.104:47804 (27 connections now open)
2020-09-22T12:12:11.595+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:12:11.595+0800 I  ELECTION [replexec-4] Scheduling priority takeover at 2020-09-22T12:12:12.628+0800
2020-09-22T12:12:11.602+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22426 #244 (28 connections now open)
2020-09-22T12:12:11.603+0800 I  NETWORK  [conn244] received client metadata from 211.162.81.126:22426 conn244: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:11.604+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11970 #245 (29 connections now open)
2020-09-22T12:12:11.604+0800 I  NETWORK  [conn245] received client metadata from 211.162.81.126:11970 conn245: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:11.668+0800 I  NETWORK  [conn245] end connection 211.162.81.126:11970 (28 connections now open)
2020-09-22T12:12:11.669+0800 I  NETWORK  [conn244] end connection 211.162.81.126:22426 (27 connections now open)
2020-09-22T12:12:11.670+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:12:11.671+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:12:11.672+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:12:12.628+0800 I  REPL     [replexec-6] Canceling priority takeover callback
2020-09-22T12:12:12.628+0800 I  ELECTION [replexec-6] Starting an election for a priority takeover
2020-09-22T12:12:12.628+0800 I  ELECTION [replexec-6] conducting a dry run election to see if we could be elected. current term: 33
2020-09-22T12:12:12.628+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 612 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 33, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 56), t: 33 } }
2020-09-22T12:12:12.628+0800 I  REPL     [replexec-6] Scheduling remote command request for vote request: RemoteCommand 613 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 33, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 56), t: 33 } }
2020-09-22T12:12:12.629+0800 I  ELECTION [replexec-1] VoteRequester(term 33 dry run) received a yes vote from 112.124.21.191:27019; response message: { term: 33, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747930, 33), $clusterTime: { clusterTime: Timestamp(1600747932, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747930, 33) }
2020-09-22T12:12:12.629+0800 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 34
2020-09-22T12:12:12.630+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 615 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 34, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 56), t: 33 } }
2020-09-22T12:12:12.630+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 616 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 34, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 56), t: 33 } }
2020-09-22T12:12:12.632+0800 I  ELECTION [replexec-6] VoteRequester(term 34) received a yes vote from 112.124.21.191:27019; response message: { term: 34, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747930, 33), $clusterTime: { clusterTime: Timestamp(1600747932, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747930, 33) }
2020-09-22T12:12:12.632+0800 I  ELECTION [replexec-6] election succeeded, assuming primary role in term 34
2020-09-22T12:12:12.632+0800 I  REPL     [replexec-6] transition to PRIMARY from SECONDARY
2020-09-22T12:12:12.632+0800 I  REPL     [replexec-6] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:12:12.632+0800 I  REPL     [replexec-6] Entering primary catch-up mode.
2020-09-22T12:12:12.632+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 2 connections to that host remain open
2020-09-22T12:12:12.632+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:12.634+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:12.634+0800 I  REPL     [replexec-0] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747930, 56), t: 33 }. My Last Applied: { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:12.634+0800 I  REPL     [replexec-0] Exited primary catch-up mode.
2020-09-22T12:12:12.634+0800 I  REPL     [replexec-0] Stopping replication producer
2020-09-22T12:12:12.634+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 34
2020-09-22T12:12:12.634+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 34
2020-09-22T12:12:12.634+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:12:12.634+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:12.634+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:12.634+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:12.634+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:12:12.635+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:12:12.635+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:12:12.636+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:12:12.636+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:12:12.636+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:12:12.877+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54416 #249 (28 connections now open)
2020-09-22T12:12:12.877+0800 I  NETWORK  [conn249] received client metadata from 112.124.21.191:54416 conn249: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:13.082+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:12:13.238+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22427 #250 (29 connections now open)
2020-09-22T12:12:13.238+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22428 #251 (30 connections now open)
2020-09-22T12:12:13.239+0800 I  NETWORK  [conn250] received client metadata from 211.162.81.126:22427 conn250: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:13.241+0800 I  NETWORK  [conn251] received client metadata from 211.162.81.126:22428 conn251: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:13.296+0800 I  NETWORK  [conn251] end connection 211.162.81.126:22428 (29 connections now open)
2020-09-22T12:12:13.297+0800 I  NETWORK  [conn250] end connection 211.162.81.126:22427 (28 connections now open)
2020-09-22T12:12:14.174+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:14.378+0800 I  REPL     [replexec-3] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:14.378+0800 I  REPL     [replexec-3] can't see a majority of the set, relinquishing primary
2020-09-22T12:12:14.378+0800 I  REPL     [replexec-3] Stepping down from primary in response to heartbeat
2020-09-22T12:12:14.378+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:14.378+0800 W  COMMAND  [conn39] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:14.378+0800 I  COMMAND  [conn39] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747932, 1), signature: { hash: BinData(0, 7CFC87D9ECABDEB05E64130BA36E42E33F28DFED), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747930, 56), t: 33 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 1688ms
2020-09-22T12:12:14.379+0800 W  COMMAND  [conn29] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:14.379+0800 I  COMMAND  [conn29] command config.$cmd command: update { update: "mongos", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "iZbp157vvbma2xfbxku74xZ:27017" }, u: { $set: { _id: "iZbp157vvbma2xfbxku74xZ:27017", ping: new Date(1600747934077), up: 112, waiting: true, mongoVersion: "4.2.6", advisoryHostFQDNs: [] } }, multi: false, upsert: true } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747933, 1), signature: { hash: BinData(0, B0FE21FAFA92ADF8FF41BDC01CA820EF0454DB45), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747930, 56), t: 33 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 301ms
2020-09-22T12:12:14.380+0800 W  COMMAND  [conn45] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:14.380+0800 I  COMMAND  [conn45] command config.$cmd command: update { update: "shards", bypassDocumentValidation: false, ordered: true, updates: [ { q: { _id: "rs_shard1" }, u: { $set: { host: "rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018" } }, multi: false, upsert: false } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747932, 1), signature: { hash: BinData(0, 7CFC87D9ECABDEB05E64130BA36E42E33F28DFED), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747930, 56), t: 33 } }, $db: "config" } numYields:0 reslen:705 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 1689ms
2020-09-22T12:12:14.381+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:14.381+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 3, userOpsRunning: 0 }
2020-09-22T12:12:14.381+0800 I  REPL     [replexec-3] transition to SECONDARY from PRIMARY
2020-09-22T12:12:14.381+0800 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: PrimarySteppedDown: Primary stepped down while waiting for replication
2020-09-22T12:12:14.381+0800 I  COMMAND  [replSetDistLockPinger] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "ConfigServer" }, update: { $set: { ping: new Date(1600747934265) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:497 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 116ms
2020-09-22T12:12:14.381+0800 W  SHARDING [replSetDistLockPinger] pinging failed for distributed lock pinger :: caused by :: PrimarySteppedDown: Primary stepped down while waiting for replication
2020-09-22T12:12:14.382+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:12:15.446+0800 I  REPL     [replexec-2] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:12:15.450+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:15.507+0800 I  NETWORK  [conn249] end connection 112.124.21.191:54416 (27 connections now open)
2020-09-22T12:12:15.576+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54418 #252 (28 connections now open)
2020-09-22T12:12:15.576+0800 I  NETWORK  [conn252] received client metadata from 112.124.21.191:54418 conn252: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:15.577+0800 I  ELECTION [conn252] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 35, candidateIndex: 2, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747930, 56), t: 33 } }
2020-09-22T12:12:15.577+0800 I  ELECTION [conn252] Sending vote response: { term: 35, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747930, 56), t: 33 }, my last applied OpTime: { ts: Timesta..." }
2020-09-22T12:12:15.601+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11971 #253 (29 connections now open)
2020-09-22T12:12:15.601+0800 I  NETWORK  [conn253] received client metadata from 211.162.81.126:11971 conn253: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:15.604+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22429 #254 (30 connections now open)
2020-09-22T12:12:15.605+0800 I  NETWORK  [conn254] received client metadata from 211.162.81.126:22429 conn254: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:15.668+0800 I  NETWORK  [conn253] end connection 211.162.81.126:11971 (29 connections now open)
2020-09-22T12:12:15.668+0800 I  NETWORK  [conn254] end connection 211.162.81.126:22429 (28 connections now open)
2020-09-22T12:12:16.382+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:12:16.383+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:12:16.384+0800 I  CONNPOOL [RS] Connecting to 120.55.192.104:27019
2020-09-22T12:12:16.384+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747934, 2), t: 34 }. source's GTE: { ts: Timestamp(1600747934, 2), t: 35 }
2020-09-22T12:12:16.384+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:16.384+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:12:16.385+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 120.55.192.104:27019)
2020-09-22T12:12:16.385+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:12:16.385+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:12:16.385+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 252
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 241
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 238
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 222
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 219
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 218
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 183
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 173
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:12:16.385+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:12:16.385+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:12:16.385+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:12:16.385+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:12:16.390+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:16.392+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 10
2020-09-22T12:12:16.392+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:12:16.392+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-12-16.8.bson
2020-09-22T12:12:16.392+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.mongos with uuid 282775af-a43f-4c57-858a-851b59ec70fc to /var/lib/mongodb/rollback/config.mongos/removed.2020-09-22T04-12-16.9.bson
2020-09-22T12:12:16.392+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:12:16.392+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:12:16.392+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:12:16.392+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:12:16.402+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747930, 56) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:12:16.403+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:12:16.408+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:12:16.408+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 242 records totaling to 51807 bytes
2020-09-22T12:12:16.408+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:12:16.408+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:12:16.411+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:12:16.411+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:12:16.420+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:12:16.421+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:12:16.421+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747930, 56)
2020-09-22T12:12:16.421+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 2 update operations and 0 delete operations.
2020-09-22T12:12:16.421+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747932, 5), t: 34 }
2020-09-22T12:12:16.421+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747932, 5) }
2020-09-22T12:12:16.421+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:12:16.424+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747930, 56) (top of oplog: { ts: Timestamp(1600747930, 56), t: 33 }, appliedThrough: { ts: Timestamp(0, 0), t: -1 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:12:16.424+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747930, 56)
2020-09-22T12:12:16.424+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:12:16.424+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:12:16.385+0800
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:12:16.425+0800
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 120.55.192.104:27019
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 10
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747934, 2), t: 34 }
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:12:14.265+0800
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:12:12.635+0800
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 1 second(s)
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747932, 5)
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747930, 56)
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 		config.mongos
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 		update: 2
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 3
2020-09-22T12:12:16.425+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:12:16.425+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:12:16.426+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:12:16.426+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source changed from 120.55.192.104:27019 to 112.124.21.191:27019
2020-09-22T12:12:16.429+0800 I  REPL     [replication-0] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: 112.124.21.191:27019, my last fetched oplog optime: { ts: Timestamp(1600747934, 2), t: 34 }, latest oplog optime of sync source: { ts: Timestamp(1600747934, 2), t: 34 } (sync source does not know the primary)
2020-09-22T12:12:16.429+0800 I  REPL     [replication-0] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 112.124.21.191:27019, OpTime { ts: Timestamp(1600747934, 2), t: 34 }, its sync source index:-1
2020-09-22T12:12:16.429+0800 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 112.124.21.191:27019 (config version: 1; last applied optime: { ts: Timestamp(1600747934, 2), t: 34 }; sync source index: -1; primary index: -1) is no longer valid
2020-09-22T12:12:16.429+0800 I  REPL     [rsBackgroundSync] Clearing sync source 112.124.21.191:27019 to choose a new one.
2020-09-22T12:12:16.429+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:12:16.430+0800 I  REPL     [rsBackgroundSync] Changed sync source from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:12:16.430+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source changed from 112.124.21.191:27019 to 120.55.192.104:27019
2020-09-22T12:12:16.431+0800 I  REPL     [rsBackgroundSync] Starting rollback due to OplogStartMissing: Our last optime fetched: { ts: Timestamp(1600747934, 2), t: 34 }. source's GTE: { ts: Timestamp(1600747934, 2), t: 35 }
2020-09-22T12:12:16.431+0800 I  REPL     [rsBackgroundSync] Replication commit point: { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:16.431+0800 I  REPL     [rsBackgroundSync] Rollback using 'recoverToStableTimestamp' method.
2020-09-22T12:12:16.431+0800 I  REPL     [rsBackgroundSync] Scheduling rollback (sync source: 120.55.192.104:27019)
2020-09-22T12:12:16.431+0800 I  ROLLBACK [rsBackgroundSync] transition to ROLLBACK
2020-09-22T12:12:16.431+0800 I  REPL     [rsBackgroundSync] State transition ops metrics: { lastStateTransition: "rollback", userOpsKilled: 0, userOpsRunning: 29 }
2020-09-22T12:12:16.431+0800 I  REPL     [rsBackgroundSync] transition to ROLLBACK from SECONDARY
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 252
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 241
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 238
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 222
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 219
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 218
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 183
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 173
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 103
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 102
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 55
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 54
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 53
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 52
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 51
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 50
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 45
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 44
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 43
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 39
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 38
2020-09-22T12:12:16.431+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 33
2020-09-22T12:12:16.432+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 32
2020-09-22T12:12:16.432+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 29
2020-09-22T12:12:16.432+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 28
2020-09-22T12:12:16.432+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 27
2020-09-22T12:12:16.432+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 26
2020-09-22T12:12:16.432+0800 I  NETWORK  [rsBackgroundSync] Skip closing connection for connection # 24
2020-09-22T12:12:16.432+0800 I  ROLLBACK [rsBackgroundSync] Waiting for all background operations to complete before starting rollback
2020-09-22T12:12:16.432+0800 I  ROLLBACK [rsBackgroundSync] Finished waiting for background operations to complete before rollback
2020-09-22T12:12:16.432+0800 I  ROLLBACK [rsBackgroundSync] finding common point
2020-09-22T12:12:16.435+0800 I  ROLLBACK [rsBackgroundSync] Rollback common point is { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:16.438+0800 I  REPL     [rsBackgroundSync] Incremented the rollback ID to 11
2020-09-22T12:12:16.438+0800 I  ROLLBACK [rsBackgroundSync] finding record store counts
2020-09-22T12:12:16.438+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.lockpings with uuid 990ea228-4f92-4ac6-83ef-83a3dd549ffd to /var/lib/mongodb/rollback/config.lockpings/removed.2020-09-22T04-12-16.10.bson
2020-09-22T12:12:16.438+0800 I  ROLLBACK [rsBackgroundSync] Preparing to write deleted documents to a rollback file for collection config.mongos with uuid 282775af-a43f-4c57-858a-851b59ec70fc to /var/lib/mongodb/rollback/config.mongos/removed.2020-09-22T04-12-16.11.bson
2020-09-22T12:12:16.438+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing all databases
2020-09-22T12:12:16.438+0800 I  STORAGE  [rsBackgroundSync] closeCatalog: closing storage engine catalog
2020-09-22T12:12:16.438+0800 I  STORAGE  [rsBackgroundSync] Deregistering all the collections
2020-09-22T12:12:16.439+0800 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2020-09-22T12:12:16.450+0800 I  REPL     [replexec-2] Member 120.55.192.104:27019 is now in state PRIMARY
2020-09-22T12:12:16.450+0800 I  ELECTION [replexec-2] Scheduling priority takeover at 2020-09-22T12:12:17.504+0800
2020-09-22T12:12:16.506+0800 I  ROLLBACK [rsBackgroundSync] Rolling back to the stable timestamp. StableTimestamp: Timestamp(1600747930, 56) Initial Data Timestamp: Timestamp(1600747816, 1)
2020-09-22T12:12:16.507+0800 I  STORAGE  [rsBackgroundSync] openCatalog: loading storage engine catalog
2020-09-22T12:12:16.513+0800 I  STORAGE  [rsBackgroundSync] OplogTruncaterThread local.oplog.rs already started
2020-09-22T12:12:16.513+0800 I  STORAGE  [rsBackgroundSync] The size storer reports that the oplog contains 242 records totaling to 51807 bytes
2020-09-22T12:12:16.513+0800 I  STORAGE  [rsBackgroundSync] Scanning the oplog to determine where to place markers for truncation
2020-09-22T12:12:16.513+0800 I  STORAGE  [rsBackgroundSync] WiredTiger record store oplog processing took 0ms
2020-09-22T12:12:16.515+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reconciling catalog and idents
2020-09-22T12:12:16.516+0800 I  STORAGE  [rsBackgroundSync] openCatalog: reopening all databases
2020-09-22T12:12:16.525+0800 I  STORAGE  [rsBackgroundSync] openCatalog: updating cached oplog pointer
2020-09-22T12:12:16.526+0800 I  STORAGE  [rsBackgroundSync] openCatalog: finished reloading collection catalog
2020-09-22T12:12:16.526+0800 I  STORAGE  [rsBackgroundSync] recoverToStableTimestamp successful. Stable Timestamp: Timestamp(1600747930, 56)
2020-09-22T12:12:16.526+0800 I  ROLLBACK [rsBackgroundSync] Rollback reverted 0 insert operations, 2 update operations and 0 delete operations.
2020-09-22T12:12:16.526+0800 I  ROLLBACK [rsBackgroundSync] Marking to truncate all oplog entries with timestamps greater than or equal to { ts: Timestamp(1600747932, 5), t: 34 }
2020-09-22T12:12:16.526+0800 I  REPL     [rsBackgroundSync] Removing unapplied entries starting at: { : Timestamp(1600747932, 5) }
2020-09-22T12:12:16.526+0800 I  REPL     [rsBackgroundSync] Replication recovery oplog truncation finished in: 0ms
2020-09-22T12:12:16.527+0800 I  REPL     [rsBackgroundSync] Recovering from stable timestamp: Timestamp(1600747930, 56) (top of oplog: { ts: Timestamp(1600747930, 56), t: 33 }, appliedThrough: { ts: Timestamp(1600747930, 56), t: 33 }, TruncateAfter: Timestamp(0, 0))
2020-09-22T12:12:16.527+0800 I  REPL     [rsBackgroundSync] Starting recovery oplog application at the stable timestamp: Timestamp(1600747930, 56)
2020-09-22T12:12:16.527+0800 I  REPL     [rsBackgroundSync] No oplog entries to apply for recovery. Start point is at the top of the oplog.
2020-09-22T12:12:16.527+0800 I  REPL     [rsBackgroundSync] Not updating committed snapshot because we are in rollback
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] Triggering the rollback op observer
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] Resetting key manager cache
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] Rollback complete
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] Rollback summary:
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	start time: 2020-09-22T12:12:16.431+0800
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	end time: 2020-09-22T12:12:16.528+0800
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	sync source: 120.55.192.104:27019
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	rollback data file directory: /var/lib/mongodb/rollback/config.
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	rollback id: 11
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	last optime on branch of history rolled back: { ts: Timestamp(1600747934, 2), t: 34 }
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	common point optime: { ts: Timestamp(1600747930, 56), t: 33 }
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	last wall clock time on the branch of history rolled back: 2020-09-22T12:12:14.265+0800
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	wall clock time of the first operation after the common point: 2020-09-22T12:12:12.635+0800
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	difference in wall clock times: 1 second(s)
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	truncate timestamp: Timestamp(1600747932, 5)
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	stable timestamp: Timestamp(1600747930, 56)
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	shard identity document rolled back: false
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	config server config version document rolled back: false
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	affected sessions: none
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	affected namespaces: 
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 		config.lockpings
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 		config.mongos
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	counts of interesting commands rolled back: 
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 		update: 2
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 		insert: 0
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 		delete: 0
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] 	total number of entries rolled back (including no-ops): 3
2020-09-22T12:12:16.528+0800 I  ROLLBACK [rsBackgroundSync] transition to SECONDARY
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] transition to SECONDARY from ROLLBACK
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] Rollback successful.
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] Stopping replication producer
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] Resetting last fetched optimes in bgsync
2020-09-22T12:12:16.528+0800 I  REPL     [rsBackgroundSync] sync source candidate: 120.55.192.104:27019
2020-09-22T12:12:16.529+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 120.55.192.104:27019
2020-09-22T12:12:16.636+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47810 #258 (29 connections now open)
2020-09-22T12:12:16.637+0800 I  NETWORK  [conn258] received client metadata from 120.55.192.104:47810 conn258: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:16.940+0800 I  NETWORK  [conn238] end connection 120.55.192.104:47802 (28 connections now open)
2020-09-22T12:12:17.196+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47814 #259 (29 connections now open)
2020-09-22T12:12:17.196+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47812 #260 (30 connections now open)
2020-09-22T12:12:17.197+0800 I  NETWORK  [conn260] received client metadata from 120.55.192.104:47812 conn260: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:17.197+0800 I  NETWORK  [conn259] received client metadata from 120.55.192.104:47814 conn259: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:17.203+0800 I  COMMAND  [conn54] command admin.system.keys command: find { find: "system.keys", filter: { purpose: "HMAC", expiresAt: { $gt: Timestamp(1616299817, 0) } }, sort: { expiresAt: 1 }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp(1600747930, 56), t: 33 } }, maxTimeMS: 30000, $readPreference: { mode: "nearest" }, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747936, 16), signature: { hash: BinData(0, 242F8BD73A182CA3CCC0FCF6B4A2741ADC84BD00), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747930, 56), t: 33 } }, $db: "admin" } planSummary: COLLSCAN keysExamined:0 docsExamined:2 hasSortStage:1 cursorExhausted:1 numYields:0 nreturned:0 queryHash:6DC32749 planCacheKey:6DC32749 reslen:573 locks:{ ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { r: 1 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } }, Mutex: { acquireCount: { r: 1 } } } storage:{} protocol:op_msg 545ms
2020-09-22T12:12:17.504+0800 I  REPL     [replexec-5] Canceling priority takeover callback
2020-09-22T12:12:17.504+0800 I  ELECTION [replexec-5] Starting an election for a priority takeover
2020-09-22T12:12:17.504+0800 I  ELECTION [replexec-5] conducting a dry run election to see if we could be elected. current term: 36
2020-09-22T12:12:17.504+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 672 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 36, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747937, 2), t: 36 } }
2020-09-22T12:12:17.504+0800 I  REPL     [replexec-5] Scheduling remote command request for vote request: RemoteCommand 673 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 36, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747937, 2), t: 36 } }
2020-09-22T12:12:17.505+0800 I  ELECTION [replexec-1] VoteRequester(term 36 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 36, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000024') }, lastCommittedOpTime: Timestamp(1600747937, 2), $clusterTime: { clusterTime: Timestamp(1600747937, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747937, 2) }
2020-09-22T12:12:17.505+0800 I  ELECTION [replexec-1] dry election run succeeded, running for election in term 37
2020-09-22T12:12:17.506+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 674 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 37, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747937, 2), t: 36 } }
2020-09-22T12:12:17.506+0800 I  REPL     [replexec-1] Scheduling remote command request for vote request: RemoteCommand 675 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 37, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747937, 2), t: 36 } }
2020-09-22T12:12:17.507+0800 I  ELECTION [replexec-5] VoteRequester(term 37) received a yes vote from 112.124.21.191:27019; response message: { term: 37, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff000000000000001b') }, lastCommittedOpTime: Timestamp(1600747937, 2), $clusterTime: { clusterTime: Timestamp(1600747937, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747937, 2) }
2020-09-22T12:12:17.508+0800 I  ELECTION [replexec-5] election succeeded, assuming primary role in term 37
2020-09-22T12:12:17.508+0800 I  REPL     [replexec-5] transition to PRIMARY from SECONDARY
2020-09-22T12:12:17.508+0800 I  REPL     [replexec-5] Resetting sync source to empty, which was 120.55.192.104:27019
2020-09-22T12:12:17.508+0800 I  REPL     [replexec-5] Entering primary catch-up mode.
2020-09-22T12:12:17.508+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:12:17.508+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:17.509+0800 I  REPL     [replexec-4] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:17.509+0800 I  REPL     [replexec-4] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747937, 2), t: 36 }. My Last Applied: { ts: Timestamp(1600747937, 2), t: 36 }
2020-09-22T12:12:17.509+0800 I  REPL     [replexec-4] Exited primary catch-up mode.
2020-09-22T12:12:17.509+0800 I  REPL     [replexec-4] Stopping replication producer
2020-09-22T12:12:17.509+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 37
2020-09-22T12:12:17.509+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 37
2020-09-22T12:12:17.509+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:12:17.510+0800 I  CONNPOOL [RS] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:17.510+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:17.510+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:17.510+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 1 }
2020-09-22T12:12:17.511+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:12:17.511+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:12:17.512+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:12:17.512+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:12:17.512+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:12:17.512+0800 I  NETWORK  [Balancer] Starting new replica set monitor for rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:12:17.514+0800 I  NETWORK  [ReplicaSetMonitor-TaskExecutor] Confirmed replica set for rs_shard1 is rs_shard1/118.31.43.238:27018,47.96.16.32:27018,47.96.5.198:27018
2020-09-22T12:12:17.515+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54422 #262 (31 connections now open)
2020-09-22T12:12:17.515+0800 I  TXN      [TransactionCoordinator] Need to resume coordinating commit for 0 transactions
2020-09-22T12:12:17.516+0800 I  TXN      [TransactionCoordinator] Incoming coordinateCommit requests are now enabled
2020-09-22T12:12:17.517+0800 I  NETWORK  [conn262] received client metadata from 112.124.21.191:54422 conn262: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:17.641+0800 I  NETWORK  [listener] connection accepted from 120.55.192.104:47818 #263 (32 connections now open)
2020-09-22T12:12:17.642+0800 I  NETWORK  [conn263] received client metadata from 120.55.192.104:47818 conn263: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:17.779+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22430 #264 (33 connections now open)
2020-09-22T12:12:17.779+0800 I  NETWORK  [conn264] received client metadata from 211.162.81.126:22430 conn264: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:17.784+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22431 #265 (34 connections now open)
2020-09-22T12:12:17.784+0800 I  NETWORK  [conn265] received client metadata from 211.162.81.126:22431 conn265: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:17.843+0800 I  NETWORK  [conn264] end connection 211.162.81.126:22430 (33 connections now open)
2020-09-22T12:12:17.846+0800 I  NETWORK  [conn265] end connection 211.162.81.126:22431 (32 connections now open)
2020-09-22T12:12:17.895+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 120.55.192.104:27019: InvalidSyncSource: Sync source was cleared. Was 120.55.192.104:27019
2020-09-22T12:12:18.369+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22432 #266 (33 connections now open)
2020-09-22T12:12:18.370+0800 I  NETWORK  [conn266] received client metadata from 211.162.81.126:22432 conn266: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:18.371+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11972 #267 (34 connections now open)
2020-09-22T12:12:18.371+0800 I  NETWORK  [conn267] received client metadata from 211.162.81.126:11972 conn267: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:18.427+0800 I  NETWORK  [conn266] end connection 211.162.81.126:22432 (33 connections now open)
2020-09-22T12:12:18.428+0800 I  NETWORK  [conn267] end connection 211.162.81.126:11972 (32 connections now open)
2020-09-22T12:12:18.515+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:18.619+0800 I  ELECTION [conn260] Received vote request: { replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 37, candidateIndex: 1, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747937, 2), t: 36 } }
2020-09-22T12:12:18.619+0800 I  ELECTION [conn260] Sending vote response: { term: 37, voteGranted: false, reason: "candidate's data is staler than mine. candidate's last applied OpTime: { ts: Timestamp(1600747937, 2), t: 36 }, my last applied OpTime: { ts: Timestam..." }
2020-09-22T12:12:19.509+0800 I  REPL     [replexec-1] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:20.488+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22433 #268 (33 connections now open)
2020-09-22T12:12:20.489+0800 I  NETWORK  [conn268] received client metadata from 211.162.81.126:22433 conn268: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:20.493+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22434 #269 (34 connections now open)
2020-09-22T12:12:20.494+0800 I  NETWORK  [conn269] received client metadata from 211.162.81.126:22434 conn269: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:20.554+0800 I  NETWORK  [conn268] end connection 211.162.81.126:22433 (33 connections now open)
2020-09-22T12:12:20.554+0800 I  NETWORK  [conn269] end connection 211.162.81.126:22434 (32 connections now open)
2020-09-22T12:12:21.070+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11973 #270 (33 connections now open)
2020-09-22T12:12:21.071+0800 I  NETWORK  [conn270] received client metadata from 211.162.81.126:11973 conn270: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:21.072+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11974 #271 (34 connections now open)
2020-09-22T12:12:21.073+0800 I  NETWORK  [conn271] received client metadata from 211.162.81.126:11974 conn271: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:21.135+0800 I  NETWORK  [conn270] end connection 211.162.81.126:11973 (33 connections now open)
2020-09-22T12:12:21.135+0800 I  NETWORK  [conn271] end connection 211.162.81.126:11974 (32 connections now open)
2020-09-22T12:12:21.765+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22435 #272 (33 connections now open)
2020-09-22T12:12:21.765+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11975 #273 (34 connections now open)
2020-09-22T12:12:21.766+0800 I  NETWORK  [conn273] received client metadata from 211.162.81.126:11975 conn273: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:21.766+0800 I  NETWORK  [conn272] received client metadata from 211.162.81.126:22435 conn272: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:21.818+0800 I  NETWORK  [conn272] end connection 211.162.81.126:22435 (33 connections now open)
2020-09-22T12:12:21.818+0800 I  NETWORK  [conn273] end connection 211.162.81.126:11975 (32 connections now open)
2020-09-22T12:12:22.716+0800 I  REPL     [replexec-2] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:22.732+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54424 #274 (33 connections now open)
2020-09-22T12:12:22.732+0800 I  NETWORK  [conn274] received client metadata from 112.124.21.191:54424 conn274: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:22.733+0800 I  NETWORK  [conn252] end connection 112.124.21.191:54418 (32 connections now open)
2020-09-22T12:12:22.733+0800 I  NETWORK  [listener] connection accepted from 112.124.21.191:54426 #275 (33 connections now open)
2020-09-22T12:12:22.733+0800 I  NETWORK  [conn275] received client metadata from 112.124.21.191:54426 conn275: { driver: { name: "NetworkInterfaceTL", version: "4.2.6" }, os: { type: "Linux", name: "Ubuntu", architecture: "x86_64", version: "16.04" } }
2020-09-22T12:12:22.947+0800 I  REPL     [conn183] stepping down from primary, because a new term has begun: 38
2020-09-22T12:12:22.947+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:22.947+0800 W  COMMAND  [conn26] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:22.947+0800 I  COMMAND  [conn26] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.02:27017:1600747818:7156575011715358840" }, update: { $set: { ping: new Date(1600747942639) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747942, 9), signature: { hash: BinData(0, C47830FCF4C2B1F076C5431F9575D8D4A6B157A3), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747940, 12), t: 37 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:763 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 306ms
2020-09-22T12:12:22.948+0800 W  COMMAND  [conn241] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:22.948+0800 I  COMMAND  [conn241] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.01:27017:1600747818:3466407884530610701" }, update: { $set: { ping: new Date(1600747942638) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747942, 13), signature: { hash: BinData(0, C47830FCF4C2B1F076C5431F9575D8D4A6B157A3), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747940, 12), t: 37 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:763 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } protocol:op_msg 308ms
2020-09-22T12:12:22.949+0800 W  COMMAND  [conn45] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:22.949+0800 I  COMMAND  [conn45] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "Disalg.Hangzhou.I.00:27017:1600747818:2541039578456057690" }, update: { $set: { ping: new Date(1600747942640) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747942, 12), signature: { hash: BinData(0, C47830FCF4C2B1F076C5431F9575D8D4A6B157A3), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747940, 12), t: 37 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:763 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1 } protocol:op_msg 307ms
2020-09-22T12:12:22.949+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:22.949+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 3, userOpsRunning: 1 }
2020-09-22T12:12:22.950+0800 I  REPL     [replexec-5] transition to SECONDARY from PRIMARY
2020-09-22T12:12:22.951+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:12:23.715+0800 I  REPL     [replexec-4] Member 112.124.21.191:27019 is now in state PRIMARY
2020-09-22T12:12:23.716+0800 I  ELECTION [replexec-4] Scheduling priority takeover at 2020-09-22T12:12:24.756+0800
2020-09-22T12:12:23.716+0800 I  REPL     [replexec-6] Member 120.55.192.104:27019 is now in state SECONDARY
2020-09-22T12:12:23.759+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22436 #276 (34 connections now open)
2020-09-22T12:12:23.760+0800 I  NETWORK  [conn276] received client metadata from 211.162.81.126:22436 conn276: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:23.761+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22437 #277 (35 connections now open)
2020-09-22T12:12:23.761+0800 I  NETWORK  [conn277] received client metadata from 211.162.81.126:22437 conn277: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:23.814+0800 I  NETWORK  [conn276] end connection 211.162.81.126:22436 (34 connections now open)
2020-09-22T12:12:23.815+0800 I  NETWORK  [conn277] end connection 211.162.81.126:22437 (33 connections now open)
2020-09-22T12:12:23.951+0800 I  REPL     [rsBackgroundSync] sync source candidate: 112.124.21.191:27019
2020-09-22T12:12:23.952+0800 I  REPL     [rsBackgroundSync] Changed sync source from empty to 112.124.21.191:27019
2020-09-22T12:12:24.756+0800 I  REPL     [replexec-4] Canceling priority takeover callback
2020-09-22T12:12:24.756+0800 I  ELECTION [replexec-4] Starting an election for a priority takeover
2020-09-22T12:12:24.756+0800 I  ELECTION [replexec-4] conducting a dry run election to see if we could be elected. current term: 38
2020-09-22T12:12:24.756+0800 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 706 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 38, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747942, 17), t: 38 } }
2020-09-22T12:12:24.756+0800 I  REPL     [replexec-4] Scheduling remote command request for vote request: RemoteCommand 707 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: true, term: 38, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747942, 17), t: 38 } }
2020-09-22T12:12:24.757+0800 I  ELECTION [replexec-2] VoteRequester(term 38 dry run) received a yes vote from 120.55.192.104:27019; response message: { term: 38, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000024') }, lastCommittedOpTime: Timestamp(1600747942, 17), $clusterTime: { clusterTime: Timestamp(1600747944, 18), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747942, 17) }
2020-09-22T12:12:24.757+0800 I  ELECTION [replexec-2] dry election run succeeded, running for election in term 39
2020-09-22T12:12:24.757+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:12:24.757+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:12:24.758+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 708 -- target:120.55.192.104:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 39, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747942, 17), t: 38 } }
2020-09-22T12:12:24.758+0800 I  REPL     [replexec-2] Scheduling remote command request for vote request: RemoteCommand 709 -- target:112.124.21.191:27019 db:admin cmd:{ replSetRequestVotes: 1, setName: "rs_config", dryRun: false, term: 39, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1600747942, 17), t: 38 } }
2020-09-22T12:12:24.759+0800 I  ELECTION [replexec-4] VoteRequester(term 39) received a yes vote from 120.55.192.104:27019; response message: { term: 39, voteGranted: true, reason: "", ok: 1.0, $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000024') }, lastCommittedOpTime: Timestamp(1600747942, 17), $clusterTime: { clusterTime: Timestamp(1600747944, 18), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, operationTime: Timestamp(1600747942, 17) }
2020-09-22T12:12:24.759+0800 I  ELECTION [replexec-4] election succeeded, assuming primary role in term 39
2020-09-22T12:12:24.759+0800 I  REPL     [replexec-4] transition to PRIMARY from SECONDARY
2020-09-22T12:12:24.759+0800 I  REPL     [replexec-4] Resetting sync source to empty, which was 112.124.21.191:27019
2020-09-22T12:12:24.759+0800 I  REPL     [replexec-4] Entering primary catch-up mode.
2020-09-22T12:12:24.760+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:24.761+0800 I  REPL     [replexec-5] Member 112.124.21.191:27019 is now in state SECONDARY
2020-09-22T12:12:24.761+0800 I  REPL     [replexec-5] Caught up to the latest optime known via heartbeats after becoming primary. Target optime: { ts: Timestamp(1600747942, 17), t: 38 }. My Last Applied: { ts: Timestamp(1600747942, 17), t: 38 }
2020-09-22T12:12:24.761+0800 I  REPL     [replexec-5] Exited primary catch-up mode.
2020-09-22T12:12:24.761+0800 I  REPL     [replexec-5] Stopping replication producer
2020-09-22T12:12:24.762+0800 I  CONNPOOL [RS] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2020-09-22T12:12:24.762+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 39
2020-09-22T12:12:24.762+0800 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 39
2020-09-22T12:12:24.762+0800 I  REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2020-09-22T12:12:24.762+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:24.762+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:24.762+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
2020-09-22T12:12:24.763+0800 I  TXN      [rsSync-0] Waiting for coordinator tasks from previous term to complete
2020-09-22T12:12:24.764+0800 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
2020-09-22T12:12:24.764+0800 I  SHARDING [Balancer] CSRS balancer is starting
2020-09-22T12:12:24.764+0800 I  SHARDING [Balancer] CSRS balancer thread is recovering
2020-09-22T12:12:24.764+0800 I  SHARDING [Balancer] CSRS balancer thread is recovered
2020-09-22T12:12:24.766+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22438 #280 (34 connections now open)
2020-09-22T12:12:24.767+0800 I  NETWORK  [conn280] received client metadata from 211.162.81.126:22438 conn280: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:24.767+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22439 #281 (35 connections now open)
2020-09-22T12:12:24.767+0800 I  NETWORK  [conn281] received client metadata from 211.162.81.126:22439 conn281: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:24.824+0800 I  NETWORK  [conn281] end connection 211.162.81.126:22439 (34 connections now open)
2020-09-22T12:12:24.830+0800 I  NETWORK  [conn280] end connection 211.162.81.126:22438 (33 connections now open)
2020-09-22T12:12:24.955+0800 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 112.124.21.191:27019: InvalidSyncSource: Sync source was cleared. Was 112.124.21.191:27019
2020-09-22T12:12:25.362+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:22440 #282 (34 connections now open)
2020-09-22T12:12:25.363+0800 I  NETWORK  [listener] connection accepted from 211.162.81.126:11976 #283 (35 connections now open)
2020-09-22T12:12:25.363+0800 I  NETWORK  [conn283] received client metadata from 211.162.81.126:11976 conn283: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:25.363+0800 I  NETWORK  [conn282] received client metadata from 211.162.81.126:22440 conn282: { driver: { name: "mongo-java-driver|sync", version: "4.0.2" }, os: { type: "Linux", name: "Linux", architecture: "amd64", version: "5.4.0-47-generic" }, platform: "Java/Ubuntu/11.0.8+10-post-Ubuntu-0ubuntu120.04" }
2020-09-22T12:12:25.418+0800 I  NETWORK  [conn283] end connection 211.162.81.126:11976 (34 connections now open)
2020-09-22T12:12:25.419+0800 I  NETWORK  [conn282] end connection 211.162.81.126:22440 (33 connections now open)
2020-09-22T12:12:26.161+0800 I  REPL     [replexec-0] Member 120.55.192.104:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:26.260+0800 I  REPL     [replexec-6] Member 112.124.21.191:27019 is now in state RS_DOWN - no response within election timeout period
2020-09-22T12:12:26.260+0800 I  REPL     [replexec-6] can't see a majority of the set, relinquishing primary
2020-09-22T12:12:26.260+0800 I  REPL     [replexec-6] Stepping down from primary in response to heartbeat
2020-09-22T12:12:26.260+0800 I  REPL     [RstlKillOpThread] Starting to kill user operations
2020-09-22T12:12:26.260+0800 W  COMMAND  [conn29] Unable to gather storage statistics for a slow operation due to lock aquire timeout
2020-09-22T12:12:26.260+0800 I  COMMAND  [conn29] command config.lockpings command: findAndModify { findAndModify: "lockpings", query: { _id: "iZbp157vvbma2xfbxku74xZ:27017:1600747818:7332321692021962316" }, update: { $set: { ping: new Date(1600747945700) } }, upsert: true, writeConcern: { w: "majority", wtimeout: 15000 }, maxTimeMS: 30000, $replData: 1, $clusterTime: { clusterTime: Timestamp(1600747944, 25), signature: { hash: BinData(0, 609BAABEBB68342B1BE49C23D885437B7D807656), keyId: 6875159523158392863 } }, $configServerState: { opTime: { ts: Timestamp(1600747942, 17), t: 38 } }, $db: "config" } planSummary: IDHACK keysExamined:1 docsExamined:1 nMatched:1 nModified:1 keysInserted:1 keysDeleted:1 numYields:0 reslen:766 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } }, Database: { acquireCount: { w: 1 } }, Collection: { acquireCount: { w: 1 } }, Mutex: { acquireCount: { r: 2 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 1 } protocol:op_msg 559ms
2020-09-22T12:12:26.261+0800 I  REPL     [RstlKillOpThread] Stopped killing user operations
2020-09-22T12:12:26.261+0800 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepDown", userOpsKilled: 1, userOpsRunning: 0 }
2020-09-22T12:12:26.261+0800 I  REPL     [replexec-6] transition to SECONDARY from PRIMARY
2020-09-22T12:12:26.261+0800 W  TXN      [TransactionCoordinator] Coordinator recovery failed and coordinateCommit requests will not be allowed :: caused by :: TransactionCoordinatorSteppingDown: operation was interrupted
2020-09-22T12:12:26.262+0800 I  SHARDING [Balancer] CSRS balancer is now stopped
2020-09-22T12:12:27.281+0800 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:27.760+0800 I  CONNPOOL [Replication] Ending connection to host 120.55.192.104:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:12:27.760+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:12:27.761+0800 I  CONNPOOL [Replication] Ending connection to host 112.124.21.191:27019 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2020-09-22T12:12:27.761+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:12:28.302+0800 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:29.321+0800 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:30.345+0800 I  ELECTION [replexec-6] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:31.396+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:32.429+0800 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:33.529+0800 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:34.643+0800 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:35.724+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:36.854+0800 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:37.929+0800 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:39.059+0800 I  ELECTION [replexec-1] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:40.172+0800 I  ELECTION [replexec-6] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:41.200+0800 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:42.224+0800 I  ELECTION [replexec-4] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:43.312+0800 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:44.313+0800 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:45.409+0800 I  ELECTION [replexec-0] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:46.538+0800 I  ELECTION [replexec-3] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:47.601+0800 I  ELECTION [replexec-2] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:47.760+0800 I  CONNPOOL [Replication] Connecting to 120.55.192.104:27019
2020-09-22T12:12:47.761+0800 I  CONNPOOL [Replication] Connecting to 112.124.21.191:27019
2020-09-22T12:12:48.693+0800 I  ELECTION [replexec-5] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2020-09-22T12:12:49.737+0800 I  ELECTION [replexec-6] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
